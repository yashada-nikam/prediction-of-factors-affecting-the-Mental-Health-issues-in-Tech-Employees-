 1/1: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
 2/1: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
 3/1: runfile('C:/Users/YASHADA/.spyder-py3/untitled0.py', wdir='C:/Users/YASHADA/.spyder-py3')
 4/1: runfile('C:/Users/YASHADA/.spyder-py3/untitled1.py', wdir='C:/Users/YASHADA/.spyder-py3')
 5/1: runfile('C:/Users/YASHADA/.spyder-py3/untitled1.py', wdir='C:/Users/YASHADA/.spyder-py3')
 6/1: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
 7/1: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
 8/1: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
 8/2: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
 9/1: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
 9/2: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
10/1: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
10/2: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
11/1: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
11/2: 4
11/3: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
11/4: 60
11/5: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
12/1: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
12/2: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
12/3: 1
12/4: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
12/5: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
13/1: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
13/2: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
14/1: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
14/2: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
14/3: runfile('C:/Users/YASHADA/.spyder-py3/temp.py', wdir='C:/Users/YASHADA/.spyder-py3')
15/1: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/1: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/2: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/3: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/4: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/5: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/6: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/7: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/8: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/9: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/10: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/11: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/12: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/13: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/14: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/15: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/16: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/17: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/18: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/19: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/20: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/21: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/22: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/23: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/24: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/25: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/26: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/27: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/28: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/29: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/30: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/31: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/32: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/33: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/34: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/35: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/36: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/37: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/38: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
16/39: runfile('C:/Users/YASHADA/Desktop/TES.py', wdir='C:/Users/YASHADA/Desktop')
16/40: runfile('C:/Users/YASHADA/Desktop/TES.py', wdir='C:/Users/YASHADA/Desktop')
16/41: runfile('C:/Users/YASHADA/Desktop/TES.py', wdir='C:/Users/YASHADA/Desktop')
16/42: runfile('C:/Users/YASHADA/Desktop/untitled2.py', wdir='C:/Users/YASHADA/Desktop')
16/43: runfile('C:/Users/YASHADA/Desktop/untitled2.py', wdir='C:/Users/YASHADA/Desktop')
16/44: runfile('C:/Users/YASHADA/Desktop/untitled2.py', wdir='C:/Users/YASHADA/Desktop')
16/45: runfile('C:/Users/YASHADA/Desktop/untitled2.py', wdir='C:/Users/YASHADA/Desktop')
16/46: runfile('C:/Users/YASHADA/Desktop/untitled2.py', wdir='C:/Users/YASHADA/Desktop')
16/47: runfile('C:/Users/YASHADA/Desktop/untitled2.py', wdir='C:/Users/YASHADA/Desktop')
17/1: runfile('C:/Users/YASHADA/Desktop/untitled2.py', wdir='C:/Users/YASHADA/Desktop')
17/2: runfile('C:/Users/YASHADA/Desktop/untitled2.py', wdir='C:/Users/YASHADA/Desktop')
17/3: runfile('C:/Users/YASHADA/Desktop/untitled2.py', wdir='C:/Users/YASHADA/Desktop')
17/4: runfile('C:/Users/YASHADA/Desktop/untitled2.py', wdir='C:/Users/YASHADA/Desktop')
17/5: runfile('C:/Users/YASHADA/Desktop/untitled3.py', wdir='C:/Users/YASHADA/Desktop')
17/6: runfile('C:/Users/YASHADA/Desktop/untitled4.py', wdir='C:/Users/YASHADA/Desktop')
17/7: runfile('C:/Users/YASHADA/Desktop/untitled4.py', wdir='C:/Users/YASHADA/Desktop')
17/8: runfile('C:/Users/YASHADA/Desktop/untitled4.py', wdir='C:/Users/YASHADA/Desktop')
17/9: runfile('C:/Users/YASHADA/Desktop/untitled5.py', wdir='C:/Users/YASHADA/Desktop')
17/10: runfile('C:/Users/YASHADA/Desktop/untitled5.py', wdir='C:/Users/YASHADA/Desktop')
17/11: runfile('C:/Users/YASHADA/Desktop/untitled5.py', wdir='C:/Users/YASHADA/Desktop')
17/12: runfile('C:/Users/YASHADA/Desktop/untitled6.py', wdir='C:/Users/YASHADA/Desktop')
17/13: runfile('C:/Users/YASHADA/Downloads/5_6113808675099377928.py', wdir='C:/Users/YASHADA/Downloads')
17/14: runfile('C:/Users/YASHADA/Desktop/untitled7.py', wdir='C:/Users/YASHADA/Desktop')
18/1: runfile('C:/Users/YASHADA/Desktop/untitled7.py', wdir='C:/Users/YASHADA/Desktop')
18/2: runfile('C:/Users/YASHADA/Downloads/5_6113808675099377928.py', wdir='C:/Users/YASHADA/Downloads')
18/3: runfile('C:/Users/YASHADA/Desktop/untitled9.py', wdir='C:/Users/YASHADA/Desktop')
19/1: runfile('C:/Users/YASHADA/Downloads/5_6113808675099377928.py', wdir='C:/Users/YASHADA/Downloads')
19/2: runfile('C:/Users/YASHADA/Desktop/5_6113808675099377928.py', wdir='C:/Users/YASHADA/Desktop')
19/3: runfile('C:/Users/YASHADA/Desktop/5_6113808675099377929.py', wdir='C:/Users/YASHADA/Desktop')
19/4: runfile('C:/Users/YASHADA/Desktop/5_6113808675099377930.py', wdir='C:/Users/YASHADA/Desktop')
19/5: runfile('C:/Users/YASHADA/Desktop/geodata/geodump.py', wdir='C:/Users/YASHADA/Desktop/geodata')
19/6: runfile('C:/Users/YASHADA/Desktop/geodata/geoload.py', wdir='C:/Users/YASHADA/Desktop/geodata')
20/1: runfile('C:/Users/YASHADA/Desktop/geodata/geoload.py', wdir='C:/Users/YASHADA/Desktop/geodata')
20/2: runfile('C:/Users/YASHADA/Desktop/geodata/geodump.py', wdir='C:/Users/YASHADA/Desktop/geodata')
21/1: runfile('C:/Users/YASHADA/Desktop/geodata/geoload.py', wdir='C:/Users/YASHADA/Desktop/geodata')
21/2: runfile('C:/Users/YASHADA/Desktop/geodata/geodump.py', wdir='C:/Users/YASHADA/Desktop/geodata')
22/1: runfile('C:/Users/YASHADA/Desktop/geodata/geoload.py', wdir='C:/Users/YASHADA/Desktop/geodata')
22/2: runfile('C:/Users/YASHADA/Desktop/geodata/geodump.py', wdir='C:/Users/YASHADA/Desktop/geodata')
22/3: runfile('C:/Users/YASHADA/Desktop/geodata/geoload.py', wdir='C:/Users/YASHADA/Desktop/geodata')
22/4: runfile('C:/Users/YASHADA/Desktop/geodata/geoload.py', wdir='C:/Users/YASHADA/Desktop/geodata')
23/1: runfile('C:/Users/YASHADA/Desktop/geodata/geoload.py', wdir='C:/Users/YASHADA/Desktop/geodata')
24/1: runfile('C:/Users/YASHADA/Desktop/geodata/geoload.py', wdir='C:/Users/YASHADA/Desktop/geodata')
24/2: runfile('C:/Users/YASHADA/Desktop/geodata/geoload.py', wdir='C:/Users/YASHADA/Desktop/geodata')
24/3: runfile('C:/Users/YASHADA/Desktop/geodata/geodump.py', wdir='C:/Users/YASHADA/Desktop/geodata')
25/1: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
25/2: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
26/1: runfile('C:/Users/YASHADA/untitled1.py', wdir='C:/Users/YASHADA')
26/2: runfile('C:/Users/YASHADA/untitled1.py', wdir='C:/Users/YASHADA')
27/1: runfile('C:/Users/YASHADA/untitled2.py', wdir='C:/Users/YASHADA')
27/2: runfile('C:/Users/YASHADA/untitled2.py', wdir='C:/Users/YASHADA')
28/1: runfile('C:/Users/YASHADA/untitled3.py', wdir='C:/Users/YASHADA')
28/2: runfile('C:/Users/YASHADA/untitled3.py', wdir='C:/Users/YASHADA')
29/1: runfile('C:/Users/YASHADA/untitled3.py', wdir='C:/Users/YASHADA')
29/2: runfile('C:/Users/YASHADA/untitled3.py', wdir='C:/Users/YASHADA')
30/1: runfile('C:/Users/YASHADA/untitled4.py', wdir='C:/Users/YASHADA')
30/2: runfile('C:/Users/YASHADA/untitled4.py', wdir='C:/Users/YASHADA')
31/1: runfile('C:/Users/YASHADA/untitled5.py', wdir='C:/Users/YASHADA')
32/1: runfile('C:/Users/YASHADA/untitled6.py', wdir='C:/Users/YASHADA')
33/1: runfile('C:/Users/YASHADA/Desktop/untitled7.py', wdir='C:/Users/YASHADA/Desktop')
34/1: runfile('C:/Users/YASHADA/Desktop/untitled8.py', wdir='C:/Users/YASHADA/Desktop')
35/1: runfile('C:/Users/YASHADA/Desktop/untitled9.py', wdir='C:/Users/YASHADA/Desktop')
36/1: runfile('C:/Users/YASHADA/Desktop/untitled10.py', wdir='C:/Users/YASHADA/Desktop')
37/1: runfile('C:/Users/YASHADA/Desktop/untitled10.py', wdir='C:/Users/YASHADA/Desktop')
38/1: runfile('C:/Users/YASHADA/Desktop/ppl3.py', wdir='C:/Users/YASHADA/Desktop')
39/1: runfile('C:/Users/YASHADA/Desktop/untitled11.py', wdir='C:/Users/YASHADA/Desktop')
41/1: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
41/2: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
41/3: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
41/4: runfile('C:/Users/YASHADA/untitled1.py', wdir='C:/Users/YASHADA')
42/1: runfile('C:/Users/YASHADA/untitled1.py', wdir='C:/Users/YASHADA')
43/1: runfile('C:/Users/YASHADA/untitled4.py', wdir='C:/Users/YASHADA')
43/2: runfile('C:/Users/YASHADA/untitled4.py', wdir='C:/Users/YASHADA')
43/3: runcell(0, 'C:/Users/YASHADA/untitled4.py')
44/1: runfile('C:/Users/YASHADA/untitled5.py', wdir='C:/Users/YASHADA')
44/2: runfile('C:/Users/YASHADA/untitled5.py', wdir='C:/Users/YASHADA')
44/3: runfile('C:/Users/YASHADA/untitled6.py', wdir='C:/Users/YASHADA')
44/4: runfile('C:/Users/YASHADA/untitled7.py', wdir='C:/Users/YASHADA')
44/5: runfile('C:/Users/YASHADA/untitled8.py', wdir='C:/Users/YASHADA')
45/1: runfile('C:/Users/YASHADA/untitled7.py', wdir='C:/Users/YASHADA')
46/1: runfile('C:/Users/YASHADA/Desktop/publisher.py', wdir='C:/Users/YASHADA/Desktop')
46/2: runfile('C:/Users/YASHADA/Desktop/subscriber.py', wdir='C:/Users/YASHADA/Desktop')
46/3: runfile('C:/Users/YASHADA/Desktop/publisher.py', wdir='C:/Users/YASHADA/Desktop')
46/4: runfile('C:/Users/YASHADA/Desktop/subscriber.py', wdir='C:/Users/YASHADA/Desktop')
46/5: runfile('C:/Users/YASHADA/Desktop/publisher.py', wdir='C:/Users/YASHADA/Desktop')
46/6: runfile('C:/Users/YASHADA/Desktop/subscriber.py', wdir='C:/Users/YASHADA/Desktop')
50/1:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
50/2:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
50/3: dataset = pd.read_csv('diabetes.csv')
50/4: dataset
50/5: dataset = pd.read_csv('diabetes.csv')
50/6: dataset
50/7: dataset = pd.read_csv('diabetes.csv')
50/8: dataset
50/9: dataset = pd.read_csv('diabetes.csv', header ='None')
50/10: dataset = pd.read_csv('diabetes.csv')
50/11: dataset.shape
50/12: pd.show_versions()
50/13:
import pandas as pd
pd.show_versions()
50/14: print(pd._version_)
50/15:
df=pd.DataFrame()
df
51/1:
months=['Jan','Feb','Mar','Apr','May']
births=[968,155,77,578,973]
51/2:
data = list(zip(months,births))
data
51/3:
import pandas as pd
df = pd.DataFrame(data = data, columns=['Month','Birth'])
51/4:
import pandas as pd
df = pd.DataFrame(data = data, columns=['Month','Birth'])
51/5: df
51/6:
#Saving a dataframe to csv file
df.to_csv("Data.csv")
51/7: dataseries = pd.Series([27,33,13,19])
51/8: dataseries
51/9: dataseries.replace(13,42)
51/10: dataseries
51/11: dataseries.replace(13,42, inplace=True)
51/12: dataseries
51/13: dataseries.replace(13,42, inplace=False)
51/14: dataseries
51/15: dataseries.replace(42,52, inplace=True)
51/16: dataseries
51/17: dataseries.replace(42,52, inplace=False)
51/18: dataseries
51/19: dataseries.replace(52,60, inplace=False)
51/20: dataseries
51/21: days = [1,2,3,4,5,6,7,8,9,10]
51/22: days
51/23: days[2:5]
51/24: days[2:]
51/25: days[:5]
51/26: y = [n*2 for n in days]
51/27: y
51/28: days2 = [...days]
51/29: days2 = [...days, ...y]
51/30: days3 = days + y
51/31: days3 = days + days
51/32: days3
52/1:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
52/2: dataset = pd.read_csv('stud_Day1.csv')
52/3: df
52/4: dataset
52/5: data = dataset.loc[10:20,['Roll', 'Name']]
52/6: data = dataset.iloc[4:,:]
52/7: data
52/8: data = dataset.iloc[:,1:5]
52/9: data
52/10: data = dataset.iloc[:,2:5]
52/11: data
52/12: data = dataset.iloc[:6,5:5]
52/13: data
52/14: data = dataset.iloc[:6,2:5]
52/15: data
52/16:
data = dataset.iloc[:,2:3]
data
52/17:
data = dataset.iloc[4:10,2:3]
data
52/18:
data = dataset.iloc[4:10,3:4]
data
52/19:
data = dataset.iloc[4:10,3]
data
52/20:
data = dataset.iloc[4:4]
data
52/21:
data = dataset.iloc[4:4, 3:3]
data
52/22:
data = dataset.iloc[4:4, 3:3]
data
52/23:
data = dataset.iloc[4,3]
data
52/24:
data2 = dataset.head(5)
data2
52/25:
data2 = dataset.head()
data2
52/26:
data2 = dataset.tail(5)
data2
52/27:
data2 = dataset.head(3)
data2
52/28:
data2 = dataset.loc[:, 'Age']
data2
52/29: marks = def.loc[:, 'Marks']
52/30: marks = dataset.loc[:, 'Marks']
52/31: dataset.Marks.mean()
52/32: dataset['Marks'].mean()
52/33: dataset['Marks'].max()
52/34: dataset['Marks'].std()
52/35: dataset.mean()
52/36: dataset.max()
53/1: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
53/2: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
53/3: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
53/4: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
53/5: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
53/6: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
53/7: runfile('C:/Users/YASHADA/untitled0.py', wdir='C:/Users/YASHADA')
55/1: import pandas as pd
55/2:
ids = [11,22,33,44,55,66,77]
countries = ['Spain', 'France', 'Spain', 'Germany', 'France']
55/3:
df = pd.DataFrame(list(zip(ids, countries)),
                  columns=['Ids', 'Countries'])
55/4: df
55/5: df_sorted = df.sort_values( by ='Countries')
55/6: df_sorted
57/1:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
57/2: dataset = pd.read_csv('breast-cancer-data.csv')
57/3: dataset
57/4: dataset = pd.read_csv('WorldCupMatches.csv')
57/5: dataset
57/6: df_sorted = df.sort_values( by ='MatchID')
57/7: df = pd.read_csv('WorldCupMatches.csv')
57/8: df
57/9: df_sorted = df.sort_values( by ='MatchID')
57/10: df_sorted
55/7:
df = pd.DataFrame({"Name :":["Atharva","Hritika","Parth","Siddhi","Rahul","Atharva"]},index=[1,2,3,4,5,6])

print(df.drop_duplicates(subset='Name',keep='last'))
55/8:
df = pd.DataFrame({"Name :":["Atharva","Hritika","Parth","Siddhi","Rahul","Atharva"]},index=[1,2,3,4,5,6])

print(df.drop_duplicates(subset='Name',keep='first'))
55/9:
df = pd.DataFrame({"Name :":["Atharva","Hritika","Parth","Siddhi","Rahul","Atharva"]},index=[1,2,3,4,5,6])

print(df.drop_duplicates(subset='Name',keep='first'))
57/11: print(df.head,5)
57/12: print(df.shape)
57/13: df.duplicated(subset=None,keep='first')
57/14: df.duplicated(subset=None,keep='first').value_counts()
57/15:
ats = df.duplicated(subset=None,keep='first')

df_unique = df[~ats]
57/16: df_unique
57/17: df.duplicated(subset=None,keep='false').value_counts()
57/18: df.duplicated(subset=None,keep=False).value_counts()
57/19:
ats = df.duplicated(subset=None,keep=False)

df_unique = df[~ats]
57/20: df_unique
57/21:
print(df.shape)
ats = df.duplicated(subset=None,keep=False)

df_unique = df[~ats]
print(df_unique.shape)
57/22:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
import seaborn as sns
57/23: df = sns.load_dataset('tips')
57/24: df
57/25: df.info()
57/26: sns.heatmap(df.corr)
57/27: sns.heatmap(df.corr())
57/28: sns.heatmap(df.corr(),annot=True)
60/1: y = pd.get_dummies(df.Countries,prefix='Country')
60/2: import pandas as pd
60/3: y = pd.get_dummies(df.Countries,prefix='Country')
60/4:
ids = [11,22,33,44,55,66,77]
countries = ['Spain', 'France', 'Spain', 'Germany', 'France']
60/5:
df = pd.DataFrame(list(zip(ids, countries)),
                  columns=['Ids', 'Countries'])
60/6: df
60/7: df_sorted = df.sort_values( by ='Countries')
60/8: df_sorted
60/9: y = pd.get_dummies(df.Countries,prefix='Country')
60/10: y
61/1: tips_df.info
61/2:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
import seaborn as sns
61/3: df = pd.read_csv('WorldCupMatches.csv')
61/4: df
61/5: df_sorted = df.sort_values( by ='MatchID')
61/6: df_sorted
61/7: print(df.head,5)
61/8: print(df.shape)
61/9: df.duplicated(subset=None,keep='first')
61/10: df.duplicated(subset=None,keep=False).value_counts()
61/11:
print(df.shape)
ats = df.duplicated(subset=None,keep=False)  ##tuple duplication

df_unique = df[~ats]
print(df_unique.shape)
61/12: df_unique
61/13: df = sns.load_dataset('tips')
61/14: df
61/15: df.info()
61/16: sns.heatmap(df.corr())
61/17: tips_df.info
61/18: cat_list = list_df.select_dtypes(include="category").columns.to_list()
61/19: cat_list
61/20: cat_list = tips_df.select_dtypes(include="category").columns.to_list()
61/21:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
import seaborn as sns
61/22: df = pd.read_csv('WorldCupMatches.csv')
61/23: df_sorted = df.sort_values( by ='MatchID')
61/24: print(df.head,5)
61/25: print(df.shape)
61/26: df.duplicated(subset=None,keep='first')
61/27: df.duplicated(subset=None,keep=False).value_counts()
61/28:
print(df.shape)
ats = df.duplicated(subset=None,keep=False)  ##tuple duplication

df_unique = df[~ats]
print(df_unique.shape)
61/29: df = sns.load_dataset('tips')
61/30: df.info()
61/31: sns.heatmap(df.corr())
61/32: sns.heatmap(df.corr(),annot=True)
61/33: cat_list = tips_df.select_dtypes(include="category").columns.to_list()
61/34: cat_list
61/35: cat_list = tips_df.select_dtypes(include="category").columns.to_list()
61/36: cat_list
61/37: cat_list = df.select_dtypes(include="category").columns.to_list()
61/38: cat_list
61/39:
new_list = cat_list.pop(2)
new_list
61/40: cat_list
61/41:
new_list = cat_list.pop(2,3)
new_list
61/42:
new_list = cat_list.pop(2)
new_list
61/43: cat_list
61/44: cat_list.append(time)
61/45: cat_list.append('time')
61/46: cat_list
61/47: tips_df
61/48: df
61/49: df.sample(5)
61/50:
for x in df:
    print(x)
61/51:
for x in df.values:
    print(x[2])
61/52:
for x in df.values:
    if x[2] == "Male" :
        x[2] == 1
    else:
        x[2] == 0
    print(x[2])
61/53:
for x in df.values:
    if x[2] == "Male" :
        x[2] == 1
    else:
        x[2] == 0
    print(x[2])
61/54:
for x in df.values:
    if x[2] == "Male":
        x[2] == 1
    else:
        x[2] == 0
    print(x[2])
61/55:
for x in df.values:
    if x[2] == "Male":
    x[2] == 1
    else:
        x[2] == 0
    print(x[2])
61/56:
for x in df.values:
    if x[2] == "Male":
        x[2] == 1
    else:
        x[2] == 0
    print(x[2])
61/57: df
61/58:
count = 0
for x in df.values:
    if x[2] == "Male":
        x[2] == 1
        df.values[count][2] == 1
    else:
        x[2] == 0
    print(x[2])
61/59:
count = 0
for x in df.values:
    if x[2] == "Male":
        x[2] == 1
        df.values[count][2] == 1
    else:
        x[2] == 0
                df.values[count][2] == 0
    print(x[2])
61/60:
count = 0
for x in df.values:
    if x[2] == "Male":
        x[2] == 1
        df.values[count][2] == 1
    else:
        x[2] == 0
        df.values[count][2] == 0
    print(x[2])
61/61:
day_df = pd.get_dummies(df["day"])
print(day_df)
61/62:
day_df = pd.get_dummies(tips_df["day"])
print(day_df)
61/63:
day_df = pd.get_dummies(["day"])
print(day_df)
61/64:
day_df = pd.get_dummies(["day"])
print(day_df)
62/1:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
62/2: df=pd.read_csv('adult.csv')
62/3: df
62/4: df.head()
62/5: df.shape()
62/6: df.shape
62/7: from sklearn.datasets import load_iris
62/8: iris_data = load_iris()
62/9: iris_data
62/10: iris_data = load_iris().data
62/11: iris_data
62/12: names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width']
62/13: names
62/14: data = pd.Fataframe(data = iris_data, columns = names)
62/15: data = pd.DataFrame(data = iris_data, columns = names)
62/16: data
62/17: data.head()
62/18: data[0]
62/19: sns.histplot(data = iris_data)
62/20:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import seaborn as sns
62/21: sns.histplot(data = iris_data)
62/22:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
62/23: sns.histplot(data = iris_data)
62/24: plt.hist(data = iris_data)
68/1:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
import seaborn as sns
68/2:
df = pd.read_csv('titanic.csv')
df
68/3: df_sorted = df.sort_values( by ='Ticket')
68/4: df_sorted
68/5: df_sorted = df.sort_values( by ='PassengerId')
68/6: df_sorted
68/7: df_sorted = df.sort_values( by ='Name')
68/8: df_sorted
68/9: print(df.head,5)
68/10: print(df.shape)
68/11: df.duplicated(subset=None,keep='first')
68/12:
print(df.shape)
ats = df.duplicated(subset=None,keep=False)  ##tuple duplication

df_unique = df[~ats]
print(df_unique.shape)
68/13: df = sns.load_dataset('Age')
68/14: sns.heatmap(df.corr())
68/15: sns.heatmap(df.corr(),annot=True)
68/16: df = sns.load_dataset('tips')
68/17: df = sns.load_dataset('tips')
68/18: df = sns.load_dataset('tips')
68/19: df = sns.load_dataset('titanic')
68/20: df = sns.load_dataset('tips')
68/21: df.duplicated(subset=None,keep=False).value_counts()
68/22:
print(df.shape)
ats = df.duplicated(subset=None,keep=False)  ##tuple duplication

df_unique = df[~ats]
print(df_unique.shape)
68/23: sns.heatmap(df.corr(),annot=True)
68/24: df_unique
70/1:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
import seaborn as sns
70/2:
df = pd.read_csv('titanic.csv')
df
70/3:
df_sorted = df.sort_values( by ='Name')
df_sorted
70/4: print(df.head,5)
70/5: df.duplicated(subset=None,keep='first')
70/6: df.duplicated(subset=None,keep=False).value_counts()
70/7:
print(df.shape)
ats = df.duplicated(subset=None,keep=False)  ##tuple duplication

df_unique = df[~ats]
print(df_unique.shape)
70/8: df_unique
71/1:
df = pd.read_csv('Batsman_Data.csv')
df
71/2:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
import seaborn as sns
71/3:
df = pd.read_csv('Batsman_Data.csv')
df
71/4: df.head()
71/5: dataset.shape
71/6: df.shape
71/7: sns.heatmap(df.corr())
71/8: sns.heatmap(df.corr())
71/9: sns.heatmap(df.corr(),annot=True
71/10: sns.heatmap(df.corr(),annot=True)
71/11:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
68/25:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
68/26:
df = pd.read_csv('iris.csv')
df
68/27: print(df.shape)
68/28: df.duplicated(subset=None,keep=False).value_counts()
68/29:
print(df.shape)
ats = df.duplicated(subset=None,keep=False)  ##tuple duplication

df_unique = df[~ats]
print(df_unique.shape)
68/30: sns.heatmap(df.corr())
68/31: sns.heatmap(df.corr(),annot=True)
68/32: sns.heatmap(df.corr(), linewidths = 0.9, annot=True, fmt='d')
68/33: sns.heatmap(df.corr(), linewidths = 0.9, annot=True)
68/34: sns.heatmap(df.corr(), linewidths = 1.0, annot=True)
75/1:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
75/2:
df = pd.read_csv('titanic.csv')
df
75/3: df.duplicated(subset=None,keep=False).value_counts()
75/4:
print(df.shape)
ats = df.duplicated(subset=None,keep=False)  ##tuple duplication

df_unique = df[~ats]
print(df_unique.shape)
75/5: sns.heatmap(df.corr())
75/6: sns.heatmap(df.corr(), linewidths = 1.0, annot=True)
75/7:
df = df.pivot('PassengerId','Age','Survived')
df
75/8:
df = df.pivot('PassengerId','Age','Survived')
sns.heatmap('df')
75/9:
df = df.pivot('PassengerId','Age','Survived')
sns.heatmap(df)
75/10:
df = df.pivot('PassengerId','Age','Survived')
sns.heatmap(df)
75/11:
df = df.pivot('Name','Age','Survived')
sns.heatmap(df)
75/12:
df = df.pivot('Name','Age','Survived')
sns.heatmap(df.corr())
75/13:
data = np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]], size=2000)
data = pd.DataFrame(data, columns=['x', 'y'])

for col in 'xy':
    plt.hist(data[col], normed=True, alpha=0.5)
75/14:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
import matplotlib.pyplot as plt
plt.style.use('classic')
%matplotlib inline
import seaborn as sns
75/15:
data = np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]], size=2000)
data = pd.DataFrame(data, columns=['x', 'y'])

for col in 'xy':
    plt.hist(data[col], normed=True, alpha=0.5)
75/16:
plt.plot(x, y)
plt.legend('ABCDEF', ncol=2, loc='upper left');
75/17:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
75/18: gapminder['Survived'].hist(bins=100)
75/19: df['Survived'].hist(bins=100)
75/20: sns.heatmap(df.corr(), linewidths = 0.9, annot=True)
75/21: sns.heatmap(df.corr(),annot=True)
75/22: sns.heatmap(df.corr(),annot=True)
75/23: sns.heatmap(df.corr())
78/1:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
78/2:
df = pd.read_csv('titanic.csv')
df
78/3: df.duplicated(subset=None,keep=False).value_counts()
78/4: sns.heatmap(df.corr())
78/5:
print(df.shape)
ats = df.duplicated(subset=None,keep=False)  ##tuple duplication

df_unique = df[~ats]
print(df_unique.shape)
78/6: sns.heatmap(df.corr(),annot=True)
78/7: sns.heatmap(df.corr(), linewidths = 0.9, annot=True)
68/35:
plt.subplots(figsize=(7,6), dpi=100)
sns.distplot( df.loc[df.species=='setosa', "sepal_length"] , color="dodgerblue", label="Setosa")
sns.distplot( df.loc[df.species=='virginica', "sepal_length"] , color="orange", label="virginica")
sns.distplot( df.loc[df.species=='versicolor', "sepal_length"] , color="deeppink", label="versicolor")

plt.title('Iris Histogram')
plt.legend();
68/36:
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
68/37:
plt.subplots(figsize=(7,6), dpi=100)
sns.distplot( df.loc[df.species=='setosa', "sepal_length"] , color="dodgerblue", label="Setosa")
sns.distplot( df.loc[df.species=='virginica', "sepal_length"] , color="orange", label="virginica")
sns.distplot( df.loc[df.species=='versicolor', "sepal_length"] , color="deeppink", label="versicolor")

plt.title('Iris Histogram')
plt.legend();
68/38:
plt.subplots(figsize=(7,6), dpi=100)
sns.distplot( df.loc[df.corr=='setosa', "sepal_length"] , color="dodgerblue", label="Setosa")
sns.distplot( df.loc[df.corr=='virginica', "sepal_length"] , color="orange", label="virginica")
sns.distplot( df.loc[df.corr=='versicolor', "sepal_length"] , color="deeppink", label="versicolor")

plt.title('Iris Histogram')
plt.legend();
80/1:
#import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
80/2:
#Using Pearson Correlation
import seaborn as sns
plt.figure(figsize=(12,10))
cor = df.corr()
sns.heatmap(cor, annot=True, linewidths = 0.9, cmap=plt.cm.Reds)
plt.show()
80/3:
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
80/4:
X=df.iloc[:,4:10]
X
80/5:
# Feature extraction

test = SelectKBest(score_func=chi2, k=2)
fit = test.fit_transform(X,Y)
#X_clf_new=SelectKBest(score_func=chi2,k=2).fit_transform(X_clf,y_clf)
#print(fit[:5])
print(fit[0:5,:])
98/1: runfile('C:/Users/YASHADA/Desktop/PYTHON TUTORIALS/TUTORIAL 01.py', wdir='C:/Users/YASHADA/Desktop/PYTHON TUTORIALS')
98/2: runfile('C:/Users/YASHADA/Desktop/PYTHON TUTORIALS/TUTORIAL 01.py', wdir='C:/Users/YASHADA/Desktop/PYTHON TUTORIALS')
98/3:
age = 20
print(age)
98/4: print(age)
98/5: runfile('C:/Users/YASHADA/Desktop/PYTHON TUTORIALS/TUTORIAL 01.py', wdir='C:/Users/YASHADA/Desktop/PYTHON TUTORIALS')
98/6: runfile('C:/Users/YASHADA/Desktop/PYTHON TUTORIALS/TUTORIAL 01.py', wdir='C:/Users/YASHADA/Desktop/PYTHON TUTORIALS')
98/7: runfile('C:/Users/YASHADA/Desktop/PYTHON TUTORIALS/TUTORIAL 01.py', wdir='C:/Users/YASHADA/Desktop/PYTHON TUTORIALS')
99/1: runfile('C:/Users/YASHADA/Desktop/PYTHON TUTORIALS/TUTORIAL 02.py', wdir='C:/Users/YASHADA/Desktop/PYTHON TUTORIALS')
99/2: runfile('C:/Users/YASHADA/Desktop/PYTHON TUTORIALS/TUTORIAL 02.py', wdir='C:/Users/YASHADA/Desktop/PYTHON TUTORIALS')
99/3: print('Hello','',name)
99/4: runfile('C:/Users/YASHADA/Desktop/PYTHON TUTORIALS/TUTORIAL 02.py', wdir='C:/Users/YASHADA/Desktop/PYTHON TUTORIALS')
99/5:
num1 = 45
num2 = 3
print(num1 + num2)
99/6:
num1 = 45
num2 = 3
print(num1 * num2)
99/7:
num1 = 45
num2 = 3
print(num1 - num2)
99/8:
num1 = 45
num2 = 3
print(num1 / num2)
99/9:
num1 = 45
num2 = 3
num3 = num1**num2
print(num3)
99/10:
num1 = 45
num2 = 3
num3 = num1//num2
print(num3)
99/11:
num1 = 45
num2 = 3
num3 = num1%num2
print(num3)
99/12:
print('Pick a number: ')
num4 = input()
print('Pick another number: ')
num5 = input()

SUM= num1 + num2
99/13:
print('Pick a number: ')
num4 = input()
print('Pick another number: ')
num5 = input()

SUM = num1 + num2
print(SUM)
99/14:
print('Pick a number: ')
num4 = input()
print('Pick another number: ')
num5 = input()

SUM = num4 + num5
print(SUM)
99/15:
print('Pick a number: ')
num4 = input()
print('Pick another number: ')
num5 = input()

SUM = int(num4) + int(num5)
print(SUM)
109/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
109/2: data=pd.read_csv(("olist_order_payments_dataset.csv"))
109/3: data.head()
109/4: data.shape()
109/5: data.shape
109/6: data.info()
109/7: data.dtypes
109/8: data.drop_duplicates(keep='first', inplace=True)
109/9:
bool_series = data.duplicated(subset=None, keep=False)
df = data[~bool_series]
109/10: data.shape
109/11: df.shape
109/12: df.head()
109/13: correlations = data.corr()
109/14: sns.heatmap(correlations, annot = True)
109/15: sns.heatmap(correlations, annot = True, cmap=plt.cm.Reds)
109/16: df["SIZE_SET"]=((df["SIZE_SET"]-df["SIZE_SET"].min())/(df["SIZE_SET"].max()-df["SIZE_SET"].min()))*1
109/17:
inputs = df.drop('payment_value',axis='columns')
x = df['payment_value']
x
109/18:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
109/19: plt.hist(x)
109/20:
inputs = df.drop('payment_installments',axis='columns')
y = df['payment_intallments']
y
109/21:
inputs = df.drop('payment_installments',axis='columns')
y = df['payment_installments']
y
109/22: plt.hist(x)
109/23:
plt.hist(y,20,
         density=True,
         histtype='bar',
         facecolor='b',
         alpha=0.5)
109/24: plt.hist(x, edgecolor='k')
109/25:
plt.hist(y,20,
         density=True,
         histtype='bar',
         facecolor='b',
         alpha=0.5,
        edgecolor='k')
109/26: df_categorical = df.select_dtypes(exclude=[np.number])
109/27: df_categorical
109/28: from sklearn.preprocessing import LabelEncoder
109/29: le = LabelEncoder()
109/30: df['order_id'] = le.fit_transform(df['order_id'])
109/31: df['order_id']
109/32:
inputs = df.drop('order_id',axis='columns')
z = df['order_id']
z
109/33:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='pink',
         alpha=0.5,
        edgecolor='k')
109/34:
# normalize the exponential data with boxcox
normalized_data = stats.boxcox(original_data)

# plot both together to compare
fig, ax=plt.subplots(1,2)
sns.distplot(original_data, ax=ax[0])
ax[0].set_title("Original Data")
sns.distplot(normalized_data[0], ax=ax[1])
ax[1].set_title("Normalized data")
109/35: df.head()
109/36: df["payment_value"]=((df["payment_value"]-df["payment_value"].min())/(df["payment_value"].max()-df["payment_value"].min()))*1
109/37: df.head()
109/38: correlations = df.corr()
109/39: sns.heatmap(correlations, annot = True, cmap=plt.cm.Reds)
109/40: df['payment_type'] = le.fit_transform(df['payment_type'])
109/41: df['payment_type']
109/42:
inputs = df.drop('payment_type',axis='columns')
z = df['payment_type']
z
109/43:
inputs = df.drop('payment_type',axis='columns')
h = df['payment_type']
h
109/44:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='orange',
         alpha=0.5,
        edgecolor='k')
109/45:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='orange',
         alpha=0.5)
109/46:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='orange',
         alpha=0.5,
        edgecolor='k')
109/47: df.head()
109/48: df.head()
109/49: correlations = df.corr()
109/50: sns.heatmap(correlations, annot = True, cmap=plt.cm.Reds)
109/51: sns.heatmap(correlations, annot = True, cmap='coolwarm')
109/52:
matrix = np.triu(df.corr())
sns.heatmap(correlations, annot = True, cmap='coolwarm', mask=matrix)
109/53: sns.heatmap(correlations, annot = True)
109/54: sns.heatmap(correlations, annot = True, cmap=plt.cm.Reds)
109/55: sns.heatmap(correlations, annot = True, cmap=plt.cm.Orange)
109/56: sns.heatmap(correlations, annot = True, cmap=plt.cm.Oranges)
109/57: df.duplicated()
109/58: df.isnull().sum()
115/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
115/2: data=pd.read_csv(("olist_order_payments_dataset.csv"))
115/3: df=pd.read_csv(("olist_order_payments_dataset.csv"))
115/4: df.head()
115/5:
from scipy import stats
df['zscore']=stats.zscore(df['payment_value'])
df.sort_values(by='payment_value')
115/6: from sklearn import preprocessing
115/7:
min_max_scaler = preprocessing.MinMaxScaler()

x_scaled = min_max_scaler.fit_transform(df[['payment_value']].values.astype(float))

df_normalized = pd.DataFrame(x_scaled)
df['normalized2']=x_scaled
df.sort_values(by='payment_value')
109/59:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='orange',
         alpha=0.9,
        edgecolor='k')
109/60:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='pink',
         alpha=0.9,
        edgecolor='k')
109/61:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='red',
         alpha=0.9,
        edgecolor='k')
109/62:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='yellow',
         alpha=0.9,
        edgecolor='k')
109/63:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='black',
         alpha=0.9,
        edgecolor='k')
109/64:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='brown',
         alpha=0.9,
        edgecolor='k')
109/65:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='green',
         alpha=0.9,
        edgecolor='k')
109/66:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='green',
         alpha=0.5,
        edgecolor='k')
109/67:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='green',
         alpha=0.9,
        edgecolor='k')
109/68:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='violet',
         alpha=0.9,
        edgecolor='k')
109/69:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='yellow',
         alpha=0.9,
        edgecolor='k')
109/70:
plt.hist(y,20,
         density=True,
         histtype='bar',
         facecolor='b',
         alpha=0.9,
        edgecolor='k')
109/71:
plt.hist(y,20,
         density=True,
         histtype='bar',
         facecolor='b',
         alpha=0.5,
        edgecolor='k')
109/72: sns.heatmap(correlations, annot = True, cmap=plt.cm.Reds, linewidths = 1.0)
109/73: sns.heatmap(correlations, annot = True,cmap=plt.cm.Reds )
117/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
117/2: data=pd.read_csv(("olist_order_payments_dataset.csv"))
117/3: data.head()
117/4: data.shape
117/5: data.info()
117/6: data.dtypes
117/7: data.drop_duplicates(keep='first', inplace=True)
117/8:
bool_series = data.duplicated(subset=None, keep=False)
df = data[~bool_series]
117/9: df.duplicated()
117/10: df.isnull().sum()
117/11: correlations = data.corr()
109/74: sns.heatmap(correlations, annot = True,cmap=plt.cm.Reds, linewidths = 1.0 )
117/12: sns.heatmap(correlations, annot = True,cmap=plt.cm.Reds, linewidths = 1.0 )
117/13:
inputs = df.drop('payment_value',axis='columns')
x = df['payment_value']
x
117/14: plt.hist(x, edgecolor='k')
117/15:
inputs = df.drop('payment_installments',axis='columns')
y = df['payment_installments']
y
117/16:
plt.hist(y,20,
         density=True,
         histtype='bar',
         facecolor='b',
         alpha=0.5,
        edgecolor='k')
117/17: df_categorical = df.select_dtypes(exclude=[np.number])
117/18: df_categorical
117/19:
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
117/20:
df['order_id'] = le.fit_transform(df['order_id'])
df['order_id']
117/21:
df['payment_type'] = le.fit_transform(df['payment_type'])
df['payment_type']
117/22:
inputs = df.drop('order_id',axis='columns')
z = df['order_id']
z
117/23:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='pink',
         alpha=0.9,
        edgecolor='k')
117/24:
inputs = df.drop('payment_type',axis='columns')
h = df['payment_type']
h
117/25:
plt.hist(y,20,
         density=True,
         histtype='bar',
         color='yellow',
         alpha=0.9,
        edgecolor='k')
117/26: df.head()
117/27: df["payment_value"]=((df["payment_value"]-df["payment_value"].min())/(df["payment_value"].max()-df["payment_value"].min()))*1
117/28: df.head()
117/29: correlations = df.corr()
117/30: sns.heatmap(correlations, annot = True, cmap=plt.cm.Oranges, linewidths = 1.0)
136/1:
pip install hiplot
import hiplot as hip
136/2:
pip install hiplot
import hiplot as hip
136/3:
pip install hiplot
import hiplot as hip
136/4:
import hiplot as hip
data = [{'dropout':0.1, 'lr': 0.001, 'loss': 10.0, 'optimizer': 'SGD'},
        {'dropout':0.15, 'lr': 0.01, 'loss': 3.5, 'optimizer': 'Adam'},
        {'dropout':0.3, 'lr': 0.1, 'loss': 4.5, 'optimizer': 'Adam'}]
hip.Experiment.from_iterable(data).display()
136/5: hiplot
136/6:
import site
site.getsitepackages()
136/7: which virtualenv
136/8:

import hiplot as hip
136/9:
import time
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as se
136/10: which virtualenv
136/11: data = pd.read_csv('threeAxisFft')
136/12: data = pd.read_csv('threeAxisFft.csv')
136/13:
plt.figure(figsize = (12,8))
plt.plot(range(len(temp1.values)),[292]*len(temp1.values), alpha = 0.5, linewidth = 1.8, color = 'green')
plt.scatter(range(len(temp1.values)),temp1.values,alpha = 0.8,color = 'red')
136/14:
temp1 = data.isnull().sum()
temp1
136/15:
plt.figure(figsize = (12,8))
plt.plot(range(len(temp1.values)),[292]*len(temp1.values), alpha = 0.5, linewidth = 1.8, color = 'green')
plt.scatter(range(len(temp1.values)),temp1.values,alpha = 0.8,color = 'red')
138/1: #### Loading data into Pandas
138/2: #### Loading data into Pandas
138/3:
import pandas as pd

df = pd.read_csv('pokemon_data.csv')
138/4:
import pandas as pd

df = pd.read_csv('pokemon_data.csv')

print df
138/5:
import pandas as pd

df = pd.read_csv('pokemon_data.csv')

print (df)
138/6: print(df.head)
138/7: print(df.head (3))
138/8: print(df.tail (3))
138/9: print(df.tail ())
138/10: print(df.head ())
138/11: print(df.columns)
138/12:
#Read headers
print(df.columns)

#Read each column
print(df['Name'])
138/13:
#Read headers
print(df.columns)

#Read each column
print(df['Name'] [0:5])
138/14:
#Read headers
print(df.columns)

#Read each column
print(df['Name'] [0:5])

print(df['Name', 'Type 1', 'HP'])

#Read each row
138/15:
#Read headers
print(df.columns)

#Read each column
print(df['Name'] [0:5])

print(df [['Name', 'Type 1', 'HP']])

#Read each row
138/16:
#Read headers
print(df.columns)

#Read each column
print(df['Name'] [0:5])

print(df [['Name', 'Type 1', 'HP']])

#Read each row
print (df.head(4))
138/17:
#Read headers
print(df.columns)

#Read each column
print(df['Name'] [0:5])

print(df [['Name', 'Type 1', 'HP']])

#Read each row
print (df.head(4))

print (df.iloc(4))
138/18:
#Read headers
print(df.columns)

#Read each column
print(df['Name'] [0:5])

print(df [['Name', 'Type 1', 'HP']])

#Read each row
print (df.head(4))

print (df.iloc(1:4))
138/19:
#Read headers
print(df.columns)

#Read each column
print(df['Name'] [0:5])

print(df [['Name', 'Type 1', 'HP']])

#Read each row
print (df.head(4))

print (df.iloc[1:4])
138/20:
#Read headers
print(df.columns)

#Read each column
print(df['Name'] [0:5])

print(df [['Name', 'Type 1', 'HP']])

#Read each row
print (df.head(4))

print (df.iloc[1:4])

# Read specific location (R,C)
print (df.iloc[2,1])
138/21:
# Read headers
#print(df.columns)

#Read each column
#print(df['Name'] [0:5])

#print(df [['Name', 'Type 1', 'HP']])

#Read each row
#print (df.head(4))

#print (df.iloc[1:4])

# Read specific location (R,C)
print (df.iloc[2,1])
138/22:
# Read headers
#print(df.columns)

#Read each column
#print(df['Name'] [0:5])

#print(df [['Name', 'Type 1', 'HP']])

#Read each row
#print (df.head(4))

#print (df.iloc[1:4])

#itterate through rows
for index, row in df.itertows():
    print(index, row['Name'])

# Read specific location (R,C)
print (df.iloc[2,1])
138/23:
# Read headers
#print(df.columns)

#Read each column
#print(df['Name'] [0:5])

#print(df [['Name', 'Type 1', 'HP']])

#Read each row
#print (df.head(4))

#print (df.iloc[1:4])

#itterate through rows
for index, row in df.iterrows():
    print(index, row['Name'])

# Read specific location (R,C)
print (df.iloc[2,1])
138/24:
# Read headers
#print(df.columns)

#Read each column
#print(df['Name'] [0:5])

#print(df [['Name', 'Type 1', 'HP']])

#Read each row
#print (df.head(4))

#print (df.iloc[1:4])

#itterate through rows
for index, row in df.iterrows():
    print(index, row['Name'])

#for fining data based on txt/no based info
df.loc[df['Type 1'] =="Fire"]

# Read specific location (R,C)
print (df.iloc[2,1])
138/25:
# Read headers
#print(df.columns)

#Read each column
#print(df['Name'] [0:5])

#print(df [['Name', 'Type 1', 'HP']])

#Read each row
#print (df.head(4))

#print (df.iloc[1:4])

#itterate through rows
#for index, row in df.iterrows():
#    print(index, row['Name'])

#for fining data based on txt/no based info
df.loc[df['Type 1'] =="Fire"]

# Read specific location (R,C)
print (df.iloc[2,1])
138/26:
# Read headers
#print(df.columns)

#Read each column
#print(df['Name'] [0:5])

#print(df [['Name', 'Type 1', 'HP']])

#Read each row
#print (df.head(4))

#print (df.iloc[1:4])

#itterate through rows
#for index, row in df.iterrows():
#    print(index, row['Name'])

#for fining data based on txt/no based info
df.loc[df['Type 1'] =="Fire"]
df
# Read specific location (R,C)
#print (df.iloc[2,1])
138/27: df.describe()
138/28: df
138/29: df.sort_values('Name')
138/30: df.sort_values('Name', ascending=False)
138/31:
#df.sort_values('Name', ascending=False)
#ascending is default sort

df.sort_values(['Type 1', 'HP'], ascending=True)
138/32:
#df.sort_values('Name', ascending=False)
#ascending is default sort

df.sort_values(['Type 1', 'HP'], ascending=[1,0])
138/33: df.head95
138/34: df.head()
138/35: df['Total'] = df['HP'] + df['Attack'] + df['Defense'] + df['Sp. Atk'] + df['Sp. Def'] + df['Speed']
138/36:
df['Total'] = df['HP'] + df['Attack'] + df['Defense'] + df['Sp. Atk'] + df['Sp. Def'] + df['Speed']
df.head()
138/37:
df['Total'] = df['HP'] + df['Attack'] + df['Defense'] + df['Sp. Atk'] + df['Sp. Def'] + df['Speed']
df.head()

df = df.drop(columns=['Total'])
138/38: df.head()
138/39:
df['Total'] = df.iloc[:, 4:10].sum(axis=1)
# axis=1 -> horizontal, if not, vertical
138/40:
df['Total'] = df.iloc[:, 4:10].sum(axis=1)
# axis=1 -> horizontal, if not, vertical
df.head()
138/41:
cols = list(df.columns)
df = df[cols[0:4] + [cols[-1]]+cols[4:12]]

df.head(5)
138/42:
df.to_csv('modified_pokemon.csv', index=False)

#df.to_excel('modified_pokemon.xlsx', index=False)

#df.to_csv('modified_pokemon', index=False, sep='\t')
140/1: df
140/2: df.head
140/3:
new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

new_df.reset_index(drop=True, inplace=True)

new_df
140/4: df = pd.read_csv('modified_pokemon.csv')
140/5:
import pandas as pd

df = pd.read_csv('pokemon_data.csv')

#df_xlsx = pd.read_excel('file name.xlsx')

#df = pd.read_csv('file name.txt', delimiter = '\t') \->delimiter
#delimiter can be changed into anything that was separating the columns

print (df)
140/6:
import pandas as pd

df = pd.read_csv('pokemon_data.csv')

#df_xlsx = pd.read_excel('file name.xlsx')

#df = pd.read_csv('file name.txt', delimiter = '\t') \->delimiter
#delimiter can be changed into anything that was separating the columns

print (df)
140/7: print(df.head ())
140/8: print(df.tail ())
140/9:
# Read headers
print(df.columns)

#Read each column
print(df['Name'] [0:5])

print(df [['Name', 'Type 1', 'HP']])

#Read each row
print (df.head(4))

print (df.iloc[1:4])

#itterate through rows
for index, row in df.iterrows():
    print(index, row['Name'])

#for finding data based on txt/no based info
df.loc[df['Type 1'] =="Fire"]
df

# Read specific location (R,C)
print (df.iloc[2,1])
140/10: df.describe()
140/11:
df.sort_values('Name', ascending=False)
#ascending is default sort

df.sort_values(['Type 1', 'HP'], ascending=[1,0])
140/12: df.head()
140/13:
#df['Total'] = df['HP'] + df['Attack'] + df['Defense'] + df['Sp. Atk'] + df['Sp. Def'] + df['Speed']
#df.head()

#df = df.drop(columns=['Total'])
140/14: df.head()
140/15:
df['Total'] = df.iloc[:, 4:10].sum(axis=1)
# axis=1 -> horizontal, if not, vertical
df.head()
140/16:
cols = list(df.columns)
df = df[cols[0:4] + [cols[-1]]+cols[4:12]]

df.head(5)
140/17:
df.to_csv('modified_pokemon.csv', index=False)

#index=false -> indexes default by pandas are removed
#df.to_excel('modified_pokemon.xlsx', index=False)

#df.to_csv('modified_pokemon', index=False, sep='\t')
140/18: df
140/19: df.loc[['Type 1'] == 'Grass']
140/20: df.loc['Type 1'] == 'Grass']
140/21: df.loc[df['Type 1'] == 'Grass']
140/22:
new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

# &-> for filtering in pandas
# |-> or
140/23:
new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

# &-> for filtering in pandas
# |-> or

new_df
140/24:
new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

# &-> for filtering in pandas
# |-> or

new_df.reset_index(drop=True, inplace=True)

new_df
140/25:
new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

# &-> for filtering in pandas
# |-> or

new_df.reset_index(drop=True, inplace=True)

new_df

new_df.to_csv('filtered.csv')
140/26:
new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

# &-> for filtering in pandas
# |-> or

new_df.reset_index(drop=True, inplace=True)

new_df

new_df.to_csv('filtered_pokemon.csv')
140/27:
new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

# &-> for filtering in pandas
# |-> or

new_df.reset_index(drop=True, inplace=True)

new_df

new_df.to_csv('filtered_pokemon.csv')
140/28:
new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

# &-> for filtering in pandas
# |-> or

new_df.reset_index(drop=True, inplace=True)

new_df
140/29:
#new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

# &-> for filtering in pandas
# |-> or

#new_df.reset_index(drop=True, inplace=True)

#new_df

df.loc[~df['Name'].str.contains('Mega')]
140/30:
#new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

# &-> for filtering in pandas
# |-> or

#new_df.reset_index(drop=True, inplace=True)

#new_df

#new_df.to_csv('filtered_pokemon.csv')
import re

df.loc[~df['Name'].str.contains('Mega')]
140/31:
#new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

# &-> for filtering in pandas
# |-> or

#new_df.reset_index(drop=True, inplace=True)

#new_df

#new_df.to_csv('filtered_pokemon.csv')
import re

#df.loc[~df['Name'].str.contains('Mega')]
df.loc[df['Type 1'].str.contains('Fire|Grass', regex=True)]
140/32:
 df.loc[df['Total'] > 500, ['Generation','Legendary']] = ['Test 1', 'Test 2']

 df

#df = pd.read_csv('modified.csv')

#df
140/33:
df = pd.read_csv('modified_pokemon.csv')

#df['count'] = 1

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/34:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/35:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

df.groupby(['Type 1', 'Type 2']).count()['count']
140/36:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

df.groupby(['Type 1']).mean()

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/37:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

df.groupby(['Type 1']).mean().sort_values('Attack', ascending=False)

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/38:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

df.groupby(['Type 1']).mean().sort_values('HP', ascending=False)

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/39:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

#df.groupby(['Type 1']).mean().sort_values('HP', ascending=False)

df.groupby(['Type 1']).sum()

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/40:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

#df.groupby(['Type 1']).mean().sort_values('HP', ascending=False)

#df.groupby(['Type 1']).sum()

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/41:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

#df.groupby(['Type 1']).mean().sort_values('HP', ascending=False)

#df.groupby(['Type 1']).sum()

df.groupby(['Type 1']).count()

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/42:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

#df.groupby(['Type 1']).mean().sort_values('HP', ascending=False)

#df.groupby(['Type 1']).sum()

df.groupby(['Type 1']).count()

df.groupby(['Type 1', 'Type 2']).count()['count']
140/43:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

#df.groupby(['Type 1']).mean().sort_values('HP', ascending=False)

#df.groupby(['Type 1']).sum()

df.groupby(['Type 1']).count()

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/44:
df = pd.read_csv('modified_pokemon.csv')



#df.groupby(['Type 1']).mean().sort_values('HP', ascending=False)

#df.groupby(['Type 1']).sum()

df.groupby(['Type 1']).count()

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/45:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

#df.groupby(['Type 1']).mean().sort_values('HP', ascending=False)

#df.groupby(['Type 1']).sum()

df.groupby(['Type 1']).count()

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/46:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

df

#df.groupby(['Type 1']).mean().sort_values('HP', ascending=False)

#df.groupby(['Type 1']).sum()

#df.groupby(['Type 1']).count()

#df.groupby(['Type 1', 'Type 2']).count()['count']
140/47:
df = pd.read_csv('modified_pokemon.csv')

df['count'] = 1

df

#df.groupby(['Type 1']).mean().sort_values('HP', ascending=False)

#df.groupby(['Type 1']).sum()

#df.groupby(['Type 1']).count()

df.groupby(['Type 1', 'Type 2']).count()['count']
140/48:
new_df = pd.DataFrame(columns=df.columns)

for df in pd.read_csv('modified.csv', chunksize=5):
    results = df.groupby(['Type 1']).count()
    
    new_df = pd.concat([new_df, results])
140/49:
new_df = pd.DataFrame(columns=df.columns)

for df in pd.read_csv('modified_pokemon.csv', chunksize=5):
    results = df.groupby(['Type 1']).count()
    
    new_df = pd.concat([new_df, results])
140/50:    new_df = pd.concat([new_df, results])
140/51: print(new_df = pd.concat([new_df, results]))
140/52:
new_df = pd.DataFrame(columns=df.columns)

for df in pd.read_csv('modified_pokemon.csv', chunksize=5):
    results = df.groupby(['Type 1']).count()
140/53:
new_df = pd.DataFrame(columns=df.columns)

for df in pd.read_csv('modified_pokemon.csv', chunksize=5):
    results = df.groupby(['Type 1']).count()
    print(df)
144/1: pip install hiplot
144/2: import hiplot as hip
144/3:
import hiplot as hip
data = [
       {'dropout':0.1, 
        'learning_rate': 0.001, 
        'optimizer': 'SGD', 
        'loss': 10.0
       },
       {'dropout':0.15, 
        'learning_rate': 0.01, 
        'optimizer': 'Adam', 
        'loss': 3.5
       },
       {'dropout':0.3, 
        'learning_rate': 0.1, 
        'optimizer': 'Adam', 
        'loss': 4.5
       }]
hip.Experiment.from_iterable(data).display(force_full_width=True)
144/4:
exp = hip.Experiment()
    exp.display_data(hip.Displays.XY).update({
        'axis_x': 'generation',
        'axis_y': 'loss',
    })
    
for i in range(200):
    dp = hip.Datapoint(
        uid=str(i),
        values={
            'generation': i,
            'param': 10 ** random.uniform(-1, 1),
            'loss': random.uniform(-5, 5)
        })
    if i > 10:
        from_parent = random.choice(exp.datapoints[-10:]) 
        # Connecting the parent to the child
        dp.from_uid = from_parent.uid
        dp.values['loss'] += from_parent.values['loss']
        dp.values['param'] *= from_parent.values['param']
    exp.datapoints.append(dp)
144/5:
exp = hip.Experiment()
    exp.display_data(hip.Displays.XY).update({
        'axis_x': 'generation',
        'axis_y': 'loss',
    })
    
for i in range(200):
    dp = hip.Datapoint(
        uid=str(i),
        values={
            'generation': i,
            'param': 10 ** random.uniform(-1, 1),
            'loss': random.uniform(-5, 5)
        })
    if i > 10:
        from_parent = random.choice(exp.datapoints[-10:]) 
        # Connecting the parent to the child
        dp.from_uid = from_parent.uid
        dp.values['loss'] += from_parent.values['loss']
        dp.values['param'] *= from_parent.values['param']
exp.datapoints.append(dp)
144/6:
exp = hip.Experiment()
exp.display_data(hip.Displays.XY).update({
        'axis_x': 'generation',
        'axis_y': 'loss',
    })
    
for i in range(200):
    dp = hip.Datapoint(
        uid=str(i),
        values={
            'generation': i,
            'param': 10 ** random.uniform(-1, 1),
            'loss': random.uniform(-5, 5)
        })
    if i > 10:
        from_parent = random.choice(exp.datapoints[-10:]) 
        # Connecting the parent to the child
        dp.from_uid = from_parent.uid
        dp.values['loss'] += from_parent.values['loss']
        dp.values['param'] *= from_parent.values['param']
exp.datapoints.append(dp)
144/7:
import hiplot as hip

def fetch_local_csv_experiment(uri):
    # Only apply this fetcher if the URI starts with webxp://
    PREFIX="localcsvxp://"

    if not uri.startswith(PREFIX):
        # Let other fetchers handle this one
        raise hip.ExperimentFetcherDoesntApply()
    
    # Parse out the local file path from the uri
    local_path = uri[len(PREFIX):]  # Remove the prefix

    # Return the hiplot experiment to render
    return hip.Experiment.from_csv(local_path)
144/8:
import hiplot as hip

def fetch_local_csv_experiment(uri):
    # Only apply this fetcher if the URI starts with webxp://
    PREFIX="localcsvxp://"

    if not uri.startswith(PREFIX):
        # Let other fetchers handle this one
        raise hip.ExperimentFetcherDoesntApply()
    
    # Parse out the local file path from the uri
    local_path = uri[len(PREFIX):]  # Remove the prefix

    # Return the hiplot experiment to render
    return hip.Experiment.from_csv(local_path)
144/9:
import hiplot as hip

def fetch_local_csv_experiment(uri):
    # Only apply this fetcher if the URI starts with webxp://
    PREFIX="localcsvxp://threeAxisFft.csv"

    if not uri.startswith(PREFIX):
        # Let other fetchers handle this one
        raise hip.ExperimentFetcherDoesntApply()
    
    # Parse out the local file path from the uri
    local_path = uri[len(PREFIX):]  # Remove the prefix

    # Return the hiplot experiment to render
    return hip.Experiment.from_csv(local_path)
145/1:
# my_fetcher.py
from pathlib import Path
import hiplot as hip
def fetch_my_experiment(uri):
    # Only apply this fetcher if the URI starts with myxp://
    PREFIX="myxp://"
    if not uri.startswith(PREFIX):
        # Let other fetchers handle this one
        raise hip.ExperimentFetcherDoesntApply()
    uri = uri[len(PREFIX):]  # Remove the prefix

    return hip.Experiment.from_csv(uri + '/data.csv')
145/2:
# my_fetcher.py
from pathlib import Path
import hiplot as hip
def fetch_my_experiment(uri):
    # Only apply this fetcher if the URI starts with myxp://
    PREFIX="myxp://"
    if not uri.startswith(PREFIX):
        # Let other fetchers handle this one
        raise hip.ExperimentFetcherDoesntApply()
    uri = uri[len(PREFIX):]  # Remove the prefix

    return hip.Experiment.from_csv(uri + '/data.csv')
145/3:
# Some dummy data for the demo
mkdir xp_folder
echo -e "col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder/data.csv
145/4:
# Some dummy data for the demo
xp_folder
echo -e "col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder/data.csv
145/5:
# Some dummy data for the demo
xp_folder
echo "col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder/data.csv
145/6:
# Some dummy data for the demo
xp_folder
print "col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder/data.csv
145/7:
# Some dummy data for the demo
xp_folder
print ("col1, col2, col3\n1,2,3\n2,2,3\n4,4,2") > xp_folder/data.csv
145/8:
# Some dummy data for the demo
mkdir xp_folder
print ("col1, col2, col3\n1,2,3\n2,2,3\n4,4,2") > xp_folder/data.csv
145/9:
# Some dummy data for the demo
os.mkdir (xp_folder)
print ("col1, col2, col3\n1,2,3\n2,2,3\n4,4,2") > xp_folder/data.csv
145/10:
# Some dummy data for the demo
mkdir (xp_folder)
print ("col1, col2, col3\n1,2,3\n2,2,3\n4,4,2") > xp_folder/data.csv
145/11:
# Some dummy data for the demo
mkdir xp_folder
!echo{"col1, col2, col3\n1,2,3\n2,2,3\n4,4,2"} > xp_folder/data.csv
145/12:
#!/usr/bin/python

import os, sys

# Path to be created
path = "/tmp/home/monthly/daily/hourly"

os.mkdir( path, 0755 );

print "Path is created"
145/13:
# Some dummy data for the demo
%mkdir xp_folder
!echo{"col1, col2, col3\n1,2,3\n2,2,3\n4,4,2"} > xp_folder/data.csv
146/1:
# my_fetcher.py
from pathlib import Path
import hiplot as hip
def fetch_my_experiment(uri):
    # Only apply this fetcher if the URI starts with myxp://
    PREFIX="myxp://"
    if not uri.startswith(PREFIX):
        # Let other fetchers handle this one
        raise hip.ExperimentFetcherDoesntApply()
    uri = uri[len(PREFIX):]  # Remove the prefix

    return hip.Experiment.from_csv(uri + '/data.csv')
146/2:
# Some dummy data for the demo
mkdir xp_folder
echo -e "col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder/data.csv
146/3:
# Some dummy data for the demo
mkdir xpfolder
echo -e "col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder/data.csv
146/4:
# Some dummy data for the demo
mkdir 
echo -e "col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder/data.csv
146/5:
# Some dummy data for the demo
mkdir 
echo -e <"col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder/data.csv
146/6:
# Some dummy data for the demo
mkdir $xp_folder
echo -e <"col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder/data.csv
146/7:
# Some dummy data for the demo
mkdir $ xp_folder
echo -e <"col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder/data.csv
146/8:
# Some dummy data for the demo
mkdir xp
echo -e <"col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp/data.csv
146/9:
# Some dummy data for the demo
mkdir xp
echo -e <"col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp /data.csv
146/10: hiplot my_fetcher.fetch_my_experiment
146/11:
# Some dummy data for the demo
osx:projects $ mkdir xp_folder
echo -e "col1, col2, col3\n1,2,3\n2,2,3\n4,4,2" > xp_folder /data.csv
146/12:
import hiplot as hip

def fetch_local_csv_experiment(uri):
    # Only apply this fetcher if the URI starts with webxp://
    PREFIX="localcsvxp://"

    if not uri.startswith(PREFIX):
        # Let other fetchers handle this one
        raise hip.ExperimentFetcherDoesntApply()
    
    # Parse out the local file path from the uri
    local_path = uri[len(PREFIX):]  # Remove the prefix

    # Return the hiplot experiment to render
    return hip.Experiment.from_csv(local_path)
147/1:
import pandas as pd  # (version 1.0.0)
import plotly.express as px  # (version 4.7.0)
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
147/2: data = px.read('threeAxisdata.csv')
147/3: data = pd.read('threeAxisdata.csv')
147/4: data = pd.read_csv('threeAxisdata.csv')
147/5: data = px.read_csv('threeAxisdata.csv')
147/6: data = pd.read_csv('threeAxisdata.csv')
147/7:
fig = px.scatter_3d(df, x='X axis', y='Y axis', color='Z axis')
fig.show()
147/8: conda install plotly
151/1:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
147/9:
fig = px.scatter_3d(df, x='X axis', y='Y axis', color='Z axis')
fig.show()
151/2:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
151/3: data = pd.read_csv('threeAxisdata.csv')
151/4: data.head(5)
151/5:
fig = px.scatter_3d(df, x='X axis', y='Y axis', color='Z axis')
fig.show()
151/6:
fig = px.scatter_3d(data, x='X axis', y='Y axis', color='Z axis')
fig.show()
151/7:
fig = px.scatter_3d(data, x='X axis', y='Y axis', z='Z axis')
fig.show()
151/8:
fig = px.scatter_3d(data, x='X axis', y='Y axis', z='Z axis', color='red')
fig.show()
151/9: data.min()
151/10: data.avg()
151/11:
fig = px.scatter_3d(data, x='X axis', y='Y axis', z='Z axis', color='5')
fig.show()
151/12:
fig = px.scatter_3d(data, x='X axis', y='Y axis', z='Z axis', color='X axis')
fig.show()
151/13: average= df.mean(axis=1)
151/14: average= data.mean(axis=1)
151/15:
average= data.mean(axis=1)
average
151/16: avg = pd.Series(np.random.randn(data.mean(axis=1)))
151/17: avg = pd.data(np.random.randn(data.mean(axis=1)))
151/18: data2 = data.assign(Avg = data.mean(axis=1))
151/19:
data2 = data.assign(Avg = data.mean(axis=1))
data2
151/20:
fig = px.scatter_3d(data, x='X axis', y='Y axis', z='Z axis', color='Avg')
fig.show()
151/21:
fig = px.scatter_3d(data2, x='X axis', y='Y axis', z='Z axis', color='Avg')
fig.show()
151/22:
fig = px.scatter_3d(data2, x='X axis', y='Y axis', z='Z axis', color='Avg')
fig.show()
152/1:
import pandas as pd
%matplotlib inline

df=pd.read_csv("threeAxisdata.csv")

df.head()
152/2:
import pandas as pd
%matplotlib inline

df=pd.read_csv("threeAxisdata.csv")

df.head(5)
152/3:
import pandas as pd
%matplotlib inline
152/4:
df=pd.read_csv("threeAxisdata.csv")

df.head(5)
152/5:
df=pd.read_csv("threeAxisdata.csv")

df.head()
152/6: df.plot.scatter(x='X axis', y='Y axis', z='Z axis')
152/7: df.plot.scatter(x='X axis', y='Y axis')
152/8: df.plot.scatter(x='X axis[:1000]', y='Y axis[:1000]')
152/9:
x=X axis[:1000]
y=Y axis[:1000]
152/10:
x='X axis[:1000]'
y='Y axis[:1000]'
152/11: df.plot.scatter(x,y)
152/12: df.plot.scatter(x='X axis',y='Y axis')
152/13: Avg = data.mean(axis=1)
152/14: Avg = df.mean(axis=1)
152/15:
Avg = df.mean(axis=1)
Avg
152/16:
import seaborn as sns

sns.set_style('white')
sns.set_style('ticks')

sns.regplot(x='X axis', y='Y axis', data=df)
152/17:
import seaborn as sns

sns.set_style('white')
sns.set_style('ticks')

sns.regplot(x='X axis', y='Y axis', data=df)
152/18:
import seaborn as sns

sns.set_style('white')
sns.set_style('ticks')

sns.regplot(x='X axis', y='Y axis', data=df)
152/19: df.plot.scatter(x='X axis',y='Y axis', hue=vs)
152/20:
df=pd.read_csv("threeAxisdata.csv")

df.head()
152/21:
import pandas as pd
%matplotlib inline
152/22: df.plot.scatter(x='X axis',y='Y axis', hue='vs')
152/23: df.plot.scatter(x='X axis',y='Y axis', hue='Z axis')
152/24: df.plot.scatter(x='X axis',y='Y axis')
152/25:
import seaborn as sns

sns.set_style('white')
sns.set_style('ticks')

sns.regplot(x='X axis', y='Y axis', data=df)
152/26:
import seaborn as sns

sns.set_style('white')
sns.set_style('ticks')

sns.regplot(x='X axis', y='Y axis', data=df)
168/1:
import pandas as pd  
import plotly.express as px  
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
168/2:
from subprocess import check_output
print(check_output(["ls", "../input"]).decode("utf8"))
168/3: df= pd.read_csv('threeAxisdata.csv')
168/4: df.shape
168/5: df.isnull()
168/6: df.isnull().sum()
168/7: df['X axis'].isnull().sum()
168/8: df=df.fillna(" ")
168/9: df.to_csv('newdata.csv', index=False)
168/10: data= pd.read_csv('newdata.csv')
168/11: data.shape
168/12: plt.scatter(X axis,Y axis)
168/13: plt.scatter('X axis','Y axis')
168/14: px.scatter('X axis','Y axis')
168/15: pd.scatter('X axis','Y axis')
171/1:
import pandas as pd  
import plotly.express as px  
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
171/2: df= pd.read_csv('threeAxisdata.csv')
172/1:
import pandas as pd  
import plotly.express as px  
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
172/2: df= pd.read_csv('threeAxisdata.csv')
172/3: df.shape
172/4: df.isnull()
172/5: df.isnull().sum()
172/6: df['X axis'].isnull().sum()
172/7:
average= df.mean(axis=1)
average
172/8:
df2 = df.assign(Avg = df.mean(axis=1))
df2
172/9: dataframe = df2[df2.Avg<70]
172/10:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X axis,
    y=dataframe.Y axis,
    z=dataframe.Z axis,
    mode='markers',
    marker=dict(
        size=10,
        color='rgb(255,0,0)',                # set color to an array/list of desired values      
    )
)


data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
iplot(fig)
173/1:
import pandas as pd  
import plotly.express as px  
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
173/2: df= pd.read_csv('threeAxisdata.csv')
173/3: df.shape
173/4: df.isnull()
173/5: df.isnull().sum()
173/6: df.columns
173/7: df.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
173/8: df.columns
173/9:
average= df.mean(axis=1)
average
173/10:
df2 = df.assign(Avg = df.mean(axis=1))
df2
173/11: dataframe = df2[df2['Avg'] < 70]
173/12:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color='rgb(255,0,0)',                # set color to an array/list of desired values      
    )
)


data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
iplot(fig)
173/13:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color='rgb(255,0,0)',                # set color to an array/list of desired values      
    )
)


data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
180/1:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
180/2: df = pd.read_csv('threeAxisdata.csv')
180/3: df.shape()
180/4: df.shape
180/5: df.isnull().sum()
180/6:
df2 = df.assign(Avg = df.mean(axis=1))
df2
180/7:
df2 = df.assign(Average = df.mean(axis=1))
df2
180/8: dataframe = df2[df2['Average'] < 65]
180/9:
d2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
df2
180/10:
d2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
df2.columns
180/11: d2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
180/12:
df2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
df2.columns
180/13:
df2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
df2
180/14: dataframe = df2[df2['Average'] < 65]
180/15: dataframe = df2[df2['Average'] < 65]
180/16:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color_discrete_sequence=['magenta', 'green','blue'],
    )
     title='Machine Vibration Data',
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
180/17:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color_discrete_sequence=['magenta', 'green','blue'],
        title='Machine Vibration Data',
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
180/18:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=['black', 'aqua','crimson'],
        title='Machine Vibration Data',
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
180/19:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=['black', 'aqua','crimson'],
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
181/1:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
181/2: df = pd.read_csv('threeAxisdata.csv')
181/3: df.shape
181/4: df.isnull().sum()
181/5:
df2 = df.assign(Average = df.mean(axis=1))
df2
181/6:
df2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
df2
181/7: dataframe = df2[df2['Average'] < 65]
181/8:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=['black', 'aqua','crimson'],
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
181/9:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color_discrete_sequence=['black', 'aqua','crimson'],
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
181/10:
# create trace 1 that is 3d scatter
trace1 = px.scatter(df2,
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    color="smoker",
    title="Machine Vibration data"                
    )
    

trace1.show()
181/11:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color="smoker"
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
181/12:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=['black'], ['aqua'],['crimson'],
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
181/13:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        colorscale ='Viridis',
        opacity = 0.8
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
181/14:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
181/15:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.5
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
182/1:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
182/2: df = pd.read_csv('threeAxisdata.csv')
182/3: df.shape
182/4: df.isnull().sum()
182/5:
df2 = df.assign(Average = df.mean(axis=0))
df2
182/6:
df2 = df.assign(Average = df.mean(axis=1))
df2
182/7:
df2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
df2
182/8: dataframe = df2[df2['Average'] < 65]
182/9:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
183/1:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
183/2: df = pd.read_csv('threeAxisdata.csv')
183/3: df.shape
183/4: df.isnull().sum()
183/5:
df2 = df.assign(Average = df.mean(axis=0))
df2
183/6:
df2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
df2
183/7: dataframe = df2[df2['Average'] < 65]
183/8:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
183/9:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
183/10: df = pd.read_csv('threeAxisdata.csv')
183/11: df.shape
183/12: df.isnull().sum()
183/13:
df2 = df.assign(Average = df.mean(axis=0))
df2
183/14:
df2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
df2
183/15: dataframe = df2[df2['Average'] < 65]
183/16:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
184/1:
import pandas as pd  
import plotly.express as px  
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
184/2: df= pd.read_csv('threeAxisdata.csv')
184/3: df.shape
184/4: df.isnull()
184/5: df.isnull().sum()
184/6: df.columns
184/7: df.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
184/8: df.columns
184/9:
average= df.mean(axis=1)
average
184/10:
df2 = df.assign(Avg = df.mean(axis=1))
df2
184/11: dataframe = df2[df2['Avg'] < 65
184/12: dataframe = df2[df2['Avg'< 65]
184/13: dataframe = df2[df2['Avg']< 65]
184/14:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
  marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.5              # set color to an array/list of desired values      
    )
)


data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
184/15:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
  marker=dict(
        size=10,
        color=df2['Avg'],
        colorscale ='Viridis',
        opacity = 0.5              # set color to an array/list of desired values      
    )
)


data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
183/17:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
183/18: df = pd.read_csv('threeAxisdata.csv')
183/19: df.shape
185/1:
import pandas as pd  
import plotly.express as px  
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
185/2: df= pd.read_csv('threeAxisdata.csv')
185/3: df.shape
185/4: df.isnull()
185/5: df.isnull().sum()
185/6: df.columns
185/7: df.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
185/8: df.columns
185/9:
average= df.mean(axis=1)
average
185/10:
df2 = df.assign(Avg = df.mean(axis=1))
df2
185/11: dataframe = df2[df2['Avg']< 65]
185/12:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
  marker=dict(
        size=10,
        color=df2['Avg'],
        colorscale ='Viridis',
        opacity = 0.5              # set color to an array/list of desired values      
    )
)


data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
186/1:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
186/2: df = pd.read_csv('threeAxisdata.csv')
186/3: df.shape
186/4: df.isnull().sum()
186/5:
df2 = df.assign(Average = df.mean(axis=0))
df2
186/6:
df2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
df2
186/7: dataframe = df2[df2['Average'] < 65]
186/8:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
185/13:
import pandas as pd  
import plotly.express as px  
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
185/14: df= pd.read_csv('threeAxisdata.csv')
185/15: df.shape
185/16: df.isnull()
185/17: df.isnull().sum()
185/18: df.columns
185/19: df.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
185/20: df.columns
185/21:
average= df.mean(axis=1)
average
185/22:
df2 = df.assign(Avg = df.mean(axis=1))
df2
185/23: dataframe = df2[df2['Avg']< 65]
185/24:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
  marker=dict(
        size=10,
        color=df2['Avg'],
        colorscale ='Viridis',
        opacity = 0.5              # set color to an array/list of desired values      
    )
)


data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
187/1:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
187/2: df = pd.read_csv('threeAxisdata.csv')
187/3:
df.shape
df.isnull().sum()
187/4:
df2 = df.assign(Average = df.mean(axis=0))
df2
187/5:
df2.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
df2
187/6: dataframe = df2[df2['Average'] < 65]
187/7:
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
187/8:
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
187/9:
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
188/1:
import pandas as pd  
import plotly.express as px  
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
188/2: df= pd.read_csv('threeAxisdata.csv')
188/3: df.shape
188/4: df.isnull()
188/5: df.isnull().sum()
188/6: df.columns
188/7: df.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
188/8: df.columns
188/9:
average= df.mean(axis=1)
average
188/10:
df2 = df.assign(Avg = df.mean(axis=1))
df2
188/11: dataframe = df2[df2['Avg']< 65]
188/12:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
  marker=dict(
        size=10,
        color=df2['Avg'],
        colorscale ='Viridis',
        opacity = 0.5              # set color to an array/list of desired values      
    )
)


data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
189/1:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
189/2: timesData = pd.read_csv("../input/timesData.csv")
189/3: timesData = pd.read_csv("cwurData.csv")
189/4:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.world_rank,
    y=dataframe.research,
    z=dataframe.citations,
     mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8              # set color to an array/list of desired values      
    )
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
iplot(fig)
189/5: dataframe = timesData[timesData.year == 2015]
189/6: dataframe = timesData[timesData.year == 2015]
189/7: dataframe = timesData[timesData.year == 2015]
189/8: dataframe = timesData[timesData.year == 2015]
189/9:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.world_rank,
    y=dataframe.research,
    z=dataframe.citations,
     mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8              # set color to an array/list of desired values      
    )
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
iplot(fig)
189/10:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
import plotly.figure_factory as ff
189/11:
data2015 = dataframe.loc[:,["research","international", "total_score"]]
data2015["index"] = np.arange(1,len(data2015)+1)
189/12: timesData = pd.read_csv("timesData.csv")
189/13: dataframe = timesData[timesData.year == 2015]
189/14:
data2015 = dataframe.loc[:,["research","international", "total_score"]]
data2015["index"] = np.arange(1,len(data2015)+1)
189/15:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.world_rank,
    y=dataframe.research,
    z=dataframe.citations,
  mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8            # set color to an array/list of desired values      
    )
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
iplot(fig)
189/16:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.world_rank,
    y=dataframe.research,
    z=dataframe.citations,
  mode='markers',
    marker=dict(
        size=10,
        color=dataframe['timesData.year == 2015'],
        colorscale ='Viridis',
        opacity = 0.8            # set color to an array/list of desired values      
    )
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
iplot(fig)
189/17:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.world_rank,
    y=dataframe.research,
    z=dataframe.citations,
  mode='markers',
    marker=dict(
        size=10,
        colorscale ='Viridis',
        opacity = 0.8            # set color to an array/list of desired values      
    )
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
iplot(fig)
189/18:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.world_rank,
    y=dataframe.research,
    z=dataframe.citations,
  mode='markers',
    marker=dict(
        size=10,
        colorscale ='Viridis',
        opacity = 0.8            # set color to an array/list of desired values      
    )
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
189/19:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.world_rank,
    y=dataframe.research,
    z=dataframe.citations,
  mode='markers',
    marker=dict(
        size=10,
        color=[dataframe['year']==2015]
        colorscale ='Viridis',
        opacity = 0.8            # set color to an array/list of desired values      
    )
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
189/20:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.world_rank,
    y=dataframe.research,
    z=dataframe.citations,
  mode='markers',
    marker=dict(
        size=10,
        color=[dataframe['year']==2015],
        colorscale ='Viridis',
        opacity = 0.8            # set color to an array/list of desired values      
    )
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
189/21:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.world_rank,
    y=dataframe.research,
    z=dataframe.citations,
  mode='markers',
    marker=dict(
        size=10,
        color=[dataframe['year']==2015],
        colorscale ='Cividis_r',
        opacity = 0.8            # set color to an array/list of desired values      
    )
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
190/1:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
    mode='markers',
    marker=dict(
        size=10,
        color=df2['Average'],
        colorscale ='Viridis',
        opacity = 0.8
    )
    
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
191/1:
import pandas as pd  
import plotly.express as px  
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
191/2: df= pd.read_csv('threeAxisdata.csv')
191/3: df.shape
191/4: df.isnull()
191/5: df.isnull().sum()
191/6: df.columns
191/7: df.rename(columns={'X axis':'X_axis','Y axis':'Y_axis','Z axis':'Z_axis'}, inplace = True)
191/8: df.columns
191/9:
average= df.mean(axis=1)
average
191/10:
df2 = df.assign(Avg = df.mean(axis=1))
df2
191/11: dataframe = df2[df2['Avg']< 65]
191/12:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.X_axis,
    y=dataframe.Y_axis,
    z=dataframe.Z_axis,
  marker=dict(
        size=10,
        color=df2['Avg'],
        colorscale ='Viridis',
        opacity = 0.5              # set color to an array/list of desired values      
    )
)


data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
196/1:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
import plotly.figure_factory as ff
196/2: timesData = pd.read_csv("timesData.csv")
196/3: dataframe = timesData[timesData.year == 2015]
196/4:
data2015 = dataframe.loc[:,["research","international", "total_score"]]
data2015["index"] = np.arange(1,len(data2015)+1)
196/5:
# create trace 1 that is 3d scatter
trace1 = go.Scatter3d(
    x=dataframe.world_rank,
    y=dataframe.research,
    z=dataframe.citations,
  mode='markers',
    marker=dict(
        size=10,
        color=[dataframe['year']==2015],
        colorscale ='Cividis_r',
        opacity = 0.8            # set color to an array/list of desired values      
    )
)

data = [trace1]
layout = go.Layout(
    margin=dict(
        l=0,
        r=0,
        b=0,
        t=0  
    )
    
)
fig = go.Figure(data=data, layout=layout)
fig.show()
197/1: df.info()
197/2:
import pandas as pd  
import plotly.express as px 
import plotly.io as pio
import numpy as np
import plotly.graph_objects as go
197/3: df = pd.read_csv('threeAxisdata.csv')
197/4: df.info()
199/1:
import pandas as pd
import numpy as np
import seaborn as sns
199/2: data = pd.read_csv('train.csv')
199/3: data.head
199/4: data.head()
199/5: data
199/6: data.head()
199/7: data.tail()
199/8: data.columns()
199/9: data.columns
199/10: data.shape
199/11: data.describe
199/12: data.describe()
199/13: data.dtype()
199/14: data.dtype
199/15: data.dtypes
199/16:
new_cols = ['ID', 'Date', 'Temperature', 'Humidity', 'Operator', 'Measure1',
       'Measure2', 'Measure3', 'Measure4', 'Measure5', 'Measure6', 'Measure7',
       'Measure8', 'Measure9', 'Measure10', 'Measure11', 'Measure12',
       'Measure13', 'Measure14', 'Measure15', 'Hours Since Previous Failure',
       'Failure', 'Year', 'Month', 'day-of-month',
       'day-of-week', 'Hours', 'Minutes', 'Seconds']
df.columns = new_cols

df.head()
199/17:
new_cols = ['ID', 'Date', 'Temperature', 'Humidity', 'Operator', 'Measure1',
       'Measure2', 'Measure3', 'Measure4', 'Measure5', 'Measure6', 'Measure7',
       'Measure8', 'Measure9', 'Measure10', 'Measure11', 'Measure12',
       'Measure13', 'Measure14', 'Measure15', 'Hours Since Previous Failure',
       'Failure', 'Year', 'Month', 'day-of-month',
       'day-of-week', 'Hours', 'Minutes', 'Seconds']
data.columns = new_cols

data.head()
199/18: data = pd.read_csv('train.csv',index_col=False)
199/19: data.head()
199/20: data = pd.read_csv('train.csv')
199/21: data.head()
199/22: data.tail()
199/23: data = pd.read_csv('train.csv',index_col=False)
199/24: data.head()
199/25: data = pd.read_csv('train.csv')
199/26: data.head()
199/27: data.tail()
199/28: data.columns
199/29: data.shape
199/30: data.describe()
199/31: data.dtypes
199/32:
new_cols = ['ID', 'Date', 'Temperature', 'Humidity', 'Operator', 'Measure1',
       'Measure2', 'Measure3', 'Measure4', 'Measure5', 'Measure6', 'Measure7',
       'Measure8', 'Measure9', 'Measure10', 'Measure11', 'Measure12',
       'Measure13', 'Measure14', 'Measure15', 'Hours Since Previous Failure',
       'Failure', 'Year', 'Month', 'day-of-month',
       'day-of-week', 'Hours', 'Minutes', 'Seconds']
data.columns = new_cols

data.head()
199/33:
plt.figure(figsize = (50,25))
sns.heatmap(data.isnull())
199/34:
import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
199/35:
plt.figure(figsize = (50,25))
sns.heatmap(data.isnull())
199/36:
temp1 = data.isnull().sum()
temp1
199/37: sns.replot(x='Temperature' , y='Failure' , data=data)
199/38: sns.relplot(x='Temperature' , y='Failure' , data=data)
199/39: sns.relplot(x='Humidity' , y='Failure' , data=data)
199/40: sns.relplot(x='Measure1' , y='Failure' , data=data)
199/41: sns.relplot(x='Measure2' , y='Failure' , data=data)
199/42: sns.relplot(x='Measure3' , y='Failure' , data=data)
199/43: sns.relplot(x='Measure15' , y='Failure' , data=data)
199/44: sns.relplot(x='Measure15' , y='Failure' , hue'Temperature', =data=data)
199/45: sns.relplot(x='Measure15' , y='Failure' , hue='Temperature', =data=data)
199/46: sns.relplot(x='Measure15' , y='Failure' , hue='Temperature', data=data)
199/47: sns.relplot(x='Measure15' , y='Failure' , hue='Humidity', data=data)
199/48: sns.relplot(x='Measure1' , y='Failure' ,hue='Humidity', data=data)
199/49: sns.relplot(x='Measure1' , y='Failure' ,hue='Temperature', data=data)
199/50: df['Failure'] = df['Failure'].astype('int')
199/51: data['Failure'] = data['Failure'].astype('int')
199/52:
%timeit
data['Failure'] = data['Failure'].apply(lambda x: 0 if x=='no' else 1)
199/53:
%timeit
data['Failure'] = data['Failure'].apply(lambda x: 0 if x=='no' else 1)
data
199/54:
%timeit
data['Failure'] = data['Failure'].apply(lambda x: 0 if x=='no' else 1)
data.head()
199/55:
%timeit
data['Failure'] = data['Failure'].apply(lambda x: 0 if x=='yes' else 1)
data.head()
199/56:
%timeit
data['Failure'] = data['Failure'].apply(lambda x: 0 if x=='yes' else 0)
data.head()
199/57:
%timeit
data['Failure'] = data['Failure'].apply(lambda x: 1 if x=='yes' else 0)
data.head()
199/58: data.tail()
199/59: data.dtypes
199/60: sns.relplot(x='Measure1' , y='Failure' ,hue='Temperature', data=data)
199/61: sns.relplot(x='Measure1' , y='Failure' ,hue='Humidity', data=data)
199/62:
%timeit
data['Failure'] = data['Failure'].apply(lambda x: 0 if x=='yes' else 0)
data.head()
199/63: sns.relplot(x='Measure1' , y='Failure' ,hue='Humidity', data=data)
199/64: sns.relplot(x='Measure15' , y='Failure' , hue='Temperature', data=data)
199/65: sns.relplot(x='Measure1' , y='Failure' ,hue='Temperature', data=data)
203/1:
import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
203/2: data = pd.read_csv('train.csv')
204/1:
import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
204/2: data = pd.read_csv('train.csv')
204/3: data.head()
204/4: data.tail()
204/5: data.columns
204/6: data.shape
204/7: data.describe()
204/8: data.dtypes
204/9:
new_cols = ['ID', 'Date', 'Temperature', 'Humidity', 'Operator', 'Measure1',
       'Measure2', 'Measure3', 'Measure4', 'Measure5', 'Measure6', 'Measure7',
       'Measure8', 'Measure9', 'Measure10', 'Measure11', 'Measure12',
       'Measure13', 'Measure14', 'Measure15', 'Hours Since Previous Failure',
       'Failure', 'Year', 'Month', 'day-of-month',
       'day-of-week', 'Hours', 'Minutes', 'Seconds']
data.columns = new_cols

data.head()
204/10:
plt.figure(figsize = (50,25))
sns.heatmap(data.isnull())
204/11:
temp1 = data.isnull().sum()
temp1
204/12: sns.relplot(x='Measure1' , y='Failure' ,hue='Temperature', data=data)
204/13: sns.relplot(x='Measure1' , y='Failure' ,hue='Humidity', data=data)
204/14: sns.relplot(x='Measure15' , y='Failure' , hue='Temperature', data=data)
204/15: sns.relplot(x='Measure15' , y='Failure' , hue='Humidity', data=data)
213/1:
import os
import numpy as np
import pandas as pd

import seaborn as sns
import plotly.express as px 
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import euclidean_distances
from scipy.spatial.distance import cdist

import warnings
warnings.filterwarnings("ignore")
213/2:
data = pd.read_csv("../input/spotify-dataset/data/data.csv")
genre_data = pd.read_csv('../input/spotify-dataset/data/data_by_genres.csv')
year_data = pd.read_csv('../input/spotify-dataset/data/data_by_year.csv')
213/3:
data = pd.read_csv("data.csv")
genre_data = pd.read_csv('data_by_genres.csv')
year_data = pd.read_csv('data_by_year.csv')
213/4: print(data.info())
213/5: print(genre_data.info())
213/6: print(year_data.info())
213/7:
from yellowbrick.target import FeatureCorrelation

feature_names = ['acousticness', 'danceability', 'energy', 'instrumentalness',
       'liveness', 'loudness', 'speechiness', 'tempo', 'valence','duration_ms','explicit','key','mode','year']

X, y = data[feature_names], data['popularity']

# Create a list of the feature names
features = np.array(feature_names)

# Instantiate the visualizer
visualizer = FeatureCorrelation(labels=features)

plt.rcParams['figure.figsize']=(20,20)
visualizer.fit(X, y)     # Fit the data to the visualizer
visualizer.show()
213/8:
from yellowbrick.target import FeatureCorrelation

feature_names = ['acousticness', 'danceability', 'energy', 'instrumentalness',
       'liveness', 'loudness', 'speechiness', 'tempo', 'valence','duration_ms','explicit','key','mode','year']

X, y = data[feature_names], data['popularity']

# Create a list of the feature names
features = np.array(feature_names)

# Instantiate the visualizer
visualizer = FeatureCorrelation(labels=features)

plt.rcParams['figure.figsize']=(20,20)
visualizer.fit(X, y)     # Fit the data to the visualizer
visualizer.show()
213/9:
def get_decade(year):
    period_start = int(year/10) * 10
    decade = '{}s'.format(period_start)
    return decade

data['decade'] = data['year'].apply(get_decade)

sns.set(rc={'figure.figsize':(11 ,6)})
sns.countplot(data['decade'])
213/10:
sound_features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'valence']
fig = px.line(year_data, x='year', y=sound_features)
fig.show()
213/11:
top10_genres = genre_data.nlargest(10, 'popularity')

fig = px.bar(top10_genres, x='genres', y=['valence', 'energy', 'danceability', 'acousticness'], barmode='group')
fig.show()
213/12:
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

cluster_pipeline = Pipeline([('scaler', StandardScaler()), ('kmeans', KMeans(n_clusters=10, n_jobs=-1))])
X = genre_data.select_dtypes(np.number)
cluster_pipeline.fit(X)
genre_data['cluster'] = cluster_pipeline.predict(X)
213/13:
# Visualizing the Clusters with t-SNE

from sklearn.manifold import TSNE

tsne_pipeline = Pipeline([('scaler', StandardScaler()), ('tsne', TSNE(n_components=2, verbose=1))])
genre_embedding = tsne_pipeline.fit_transform(X)
projection = pd.DataFrame(columns=['x', 'y'], data=genre_embedding)
projection['genres'] = genre_data['genres']
projection['cluster'] = genre_data['cluster']

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'genres'])
fig.show()
213/14:
song_cluster_pipeline = Pipeline([('scaler', StandardScaler()), 
                                  ('kmeans', KMeans(n_clusters=20, 
                                   verbose=False, n_jobs=4))
                                 ], verbose=False)

X = data.select_dtypes(np.number)
number_cols = list(X.columns)
song_cluster_pipeline.fit(X)
song_cluster_labels = song_cluster_pipeline.predict(X)
data['cluster_label'] = song_cluster_labels
213/15:
# Visualizing the Clusters with PCA

from sklearn.decomposition import PCA

pca_pipeline = Pipeline([('scaler', StandardScaler()), ('PCA', PCA(n_components=2))])
song_embedding = pca_pipeline.fit_transform(X)
projection = pd.DataFrame(columns=['x', 'y'], data=song_embedding)
projection['title'] = data['name']
projection['cluster'] = data['cluster_label']

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'title'])
fig.show()
213/16: !pip install spotipy
213/17:
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
from collections import defaultdict

sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=os.environ["SPOTIFY_CLIENT_ID"],
                                                           client_secret=os.environ["SPOTIFY_CLIENT_SECRET"]))

def find_song(name, year):
    song_data = defaultdict()
    results = sp.search(q= 'track: {} year: {}'.format(name,year), limit=1)
    if results['tracks']['items'] == []:
        return None

    results = results['tracks']['items'][0]
    track_id = results['id']
    audio_features = sp.audio_features(track_id)[0]

    song_data['name'] = [name]
    song_data['year'] = [year]
    song_data['explicit'] = [int(results['explicit'])]
    song_data['duration_ms'] = [results['duration_ms']]
    song_data['popularity'] = [results['popularity']]

    for key, value in audio_features.items():
        song_data[key] = value

    return pd.DataFrame(song_data)
213/18: !pip install spotipy
213/19:
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
from collections import defaultdict

sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=os.environ["8ea63ef143cd4feea3d652e085df744a"],
                                                           client_secret=os.environ["b9925c9054ac49928a17ca3d1bff8037"]))

def find_song(name, year):
    song_data = defaultdict()
    results = sp.search(q= 'track: {} year: {}'.format(name,year), limit=1)
    if results['tracks']['items'] == []:
        return None

    results = results['tracks']['items'][0]
    track_id = results['id']
    audio_features = sp.audio_features(track_id)[0]

    song_data['name'] = [name]
    song_data['year'] = [year]
    song_data['explicit'] = [int(results['explicit'])]
    song_data['duration_ms'] = [results['duration_ms']]
    song_data['popularity'] = [results['popularity']]

    for key, value in audio_features.items():
        song_data[key] = value

    return pd.DataFrame(song_data)
213/20:
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
from collections import defaultdict

sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=os.environ["8ea63ef143cd4feea3d652e085df744a"],
                                                           client_secret=os.environ["b9925c9054ac49928a17ca3d1bff8037"]))

def find_song(name, year):
    song_data = defaultdict()
    results = sp.search(q= 'track: {} year: {}'.format(name,year), limit=1)
    if results['tracks']['items'] == []:
        return None

    results = results['tracks']['items'][0]
    track_id = results['id']
    audio_features = sp.audio_features(track_id)[0]

    song_data['name'] = [name]
    song_data['year'] = [year]
    song_data['explicit'] = [int(results['explicit'])]
    song_data['duration_ms'] = [results['duration_ms']]
    song_data['popularity'] = [results['popularity']]

    for key, value in audio_features.items():
        song_data[key] = value

    return pd.DataFrame(song_data)
213/21:
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
cid = '8ea63ef143cd4feea3d652e085df744a'
secret = 'b9925c9054ac49928a17ca3d1bff8037'
client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)
sp = spotipy.Spotify(client_credentials_manager
=
client_credentials_manager)
213/22:
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
from collections import defaultdict

sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=os.environ["8ea63ef143cd4feea3d652e085df744a"],
                                                           client_secret=os.environ["b9925c9054ac49928a17ca3d1bff8037"]))

def find_song(name, year):
    song_data = defaultdict()
    results = sp.search(q= 'track: {} year: {}'.format(name,year), limit=1)
    if results['tracks']['items'] == []:
        return None

    results = results['tracks']['items'][0]
    track_id = results['id']
    audio_features = sp.audio_features(track_id)[0]

    song_data['name'] = [name]
    song_data['year'] = [year]
    song_data['explicit'] = [int(results['explicit'])]
    song_data['duration_ms'] = [results['duration_ms']]
    song_data['popularity'] = [results['popularity']]

    for key, value in audio_features.items():
        song_data[key] = value

    return pd.DataFrame(song_data)
213/23:
from collections import defaultdict
from sklearn.metrics import euclidean_distances
from scipy.spatial.distance import cdist
import difflib

number_cols = ['valence', 'year', 'acousticness', 'danceability', 'duration_ms', 'energy', 'explicit',
 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'popularity', 'speechiness', 'tempo']


def get_song_data(song, spotify_data):
    
    try:
        song_data = spotify_data[(spotify_data['name'] == song['name']) 
                                & (spotify_data['year'] == song['year'])].iloc[0]
        return song_data
    
    except IndexError:
        return find_song(song['name'], song['year'])
        

def get_mean_vector(song_list, spotify_data):
    
    song_vectors = []
    
    for song in song_list:
        song_data = get_song_data(song, spotify_data)
        if song_data is None:
            print('Warning: {} does not exist in Spotify or in database'.format(song['name']))
            continue
        song_vector = song_data[number_cols].values
        song_vectors.append(song_vector)  
    
    song_matrix = np.array(list(song_vectors))
    return np.mean(song_matrix, axis=0)


def flatten_dict_list(dict_list):
    
    flattened_dict = defaultdict()
    for key in dict_list[0].keys():
        flattened_dict[key] = []
    
    for dictionary in dict_list:
        for key, value in dictionary.items():
            flattened_dict[key].append(value)
            
    return flattened_dict


def recommend_songs( song_list, spotify_data, n_songs=10):
    
    metadata_cols = ['name', 'year', 'artists']
    song_dict = flatten_dict_list(song_list)
    
    song_center = get_mean_vector(song_list, spotify_data)
    scaler = song_cluster_pipeline.steps[0][1]
    scaled_data = scaler.transform(spotify_data[number_cols])
    scaled_song_center = scaler.transform(song_center.reshape(1, -1))
    distances = cdist(scaled_song_center, scaled_data, 'cosine')
    index = list(np.argsort(distances)[:, :n_songs][0])
    
    rec_songs = spotify_data.iloc[index]
    rec_songs = rec_songs[~rec_songs['name'].isin(song_dict['name'])]
    return rec_songs[metadata_cols].to_dict(orient='records')
213/24:
recommend_songs([{'name': 'Come As You Are', 'year':1991},
                {'name': 'Smells Like Teen Spirit', 'year': 1991},
                {'name': 'Lithium', 'year': 1992},
                {'name': 'All Apologies', 'year': 1993},
                {'name': 'Stay Away', 'year': 1993}],  data)
213/25:
spotify = spotipy.Spotify(auth_manager = SpotifyOAuth(client_id = '8ea63ef143cd4feea3d652e085df744a',
                                                      client_secret = 'b9925c9054ac49928a17ca3d1bff8037',
                                                      redirect_uri = 'http://localhost:8888/callback',
                                                     ))
213/26:
from collections import defaultdict
from scipy.spatial.distance import cdist
import difflib

number_cols = ['valence', 'year', 'acousticness', 'danceability', 'duration_ms', 'energy', 'explicit',
 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'popularity', 'speechiness', 'tempo']

def get_song_data(song, spotify_data):
    
    """
    Gets the song data for a specific song. The song argument takes the form of a dictionary with 
    key-value pairs for the name and release year of the song.
    """
    
    try:
        song_data = spotify_data[(spotify_data['name'] == song['name']) 
                                & (spotify_data['year'] == song['year'])].iloc[0]
        return song_data
    
    except IndexError:
        return find_song(song['name'], song['year'])
        

def get_mean_vector(song_list, spotify_data):
  
    """
    Gets the mean vector for a list of songs.
    """
    
    song_vectors = []
    
    for song in song_list:
        song_data = get_song_data(song, spotify_data)
        if song_data is None:
            print('Warning: {} does not exist in Spotify or in database'.format(song['name']))
            continue
        song_vector = song_data[number_cols].values
        song_vectors.append(song_vector)  
    
    song_matrix = np.array(list(song_vectors))
    return np.mean(song_matrix, axis=0)

def flatten_dict_list(dict_list):
   
    """
    Utility function for flattening a list of dictionaries.
    """
    
    flattened_dict = defaultdict()
    for key in dict_list[0].keys():
        flattened_dict[key] = []
    
    for dictionary in dict_list:
        for key, value in dictionary.items():
            flattened_dict[key].append(value)
            
    return flattened_dict
        

def recommend_songs(song_list, spotify_data, n_songs=10):
  
    """
    Recommends songs based on a list of previous songs that a user has listened to.
    """
    
    metadata_cols = ['name', 'year', 'artists']
    song_dict = flatten_dict_list(song_list)
    
    song_center = get_mean_vector(song_list, spotify_data)
    scaler = song_cluster_pipeline.steps[0][1]
    scaled_data = scaler.transform(spotify_data[number_cols])
    scaled_song_center = scaler.transform(song_center.reshape(1, -1))
    distances = cdist(scaled_song_center, scaled_data, 'cosine')
    index = list(np.argsort(distances)[:, :n_songs][0])
    
    rec_songs = spotify_data.iloc[index]
    rec_songs = rec_songs[~rec_songs['name'].isin(song_dict['name'])]
    return rec_songs[metadata_cols].to_dict(orient='records')
213/27:
recommend_songs([{'name': 'Come As You Are', 'year':1991},
                {'name': 'Smells Like Teen Spirit', 'year': 1991},
                {'name': 'Lithium', 'year': 1992},
                {'name': 'All Apologies', 'year': 1993},
                {'name': 'Stay Away', 'year': 1993}],  spotify_data)
214/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import spotipy
import os
%matplotlib inline
214/2:
spotify_data = pd.read_csv('data.csv')
genre_data = pd.read_csv('data_by_genres.csv')
data_by_year = pd.read_csv('data_by_year.csv')
215/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import spotipy
import os
%matplotlib inline
215/2:
spotify_data = pd.read_csv('data.csv')
genre_data = pd.read_csv('data_by_genres.csv')
data_by_year = pd.read_csv('data_by_year.csv')
215/3: spotify_data.head(10)
215/4: spotify_data.info()
215/5: genre_data.info()
215/6: data_by_year.info()
215/7: sns.distplot(spotify_data['popularity'])
215/8:
def get_decade(year):
    
    period_start = int(year/10) * 10
    decade = '{}s'.format(period_start)
    
    return decade

spotify_data['decade'] = spotify_data['year'].apply(get_decade)
215/9:
sns.set(rc={'figure.figsize':(11 ,6)})
sns.countplot(spotify_data['decade'])
215/10:
import plotly.express as px 

sound_features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'valence']
fig = px.line(data_by_year, x='year', y=sound_features)
fig.show()

#report = dp.Report(dp.Plot(fig) ) #Create a report
#report.publish(name='music_over_time', open=True, visibility='PUBLIC') #Publish the report
215/11:
fig = px.line(data_by_year, x='year', y='tempo')
fig.show()

#report = dp.Report(dp.Plot(fig) ) #Create a report
#report.publish(name='music_tempo_over_time', open=True, visibility='PUBLIC') #Publish the report
215/12:
top10_genres = genre_data.nlargest(10, 'popularity')
fig = px.bar(top10_genres, x='genres', y=['valence', 'energy', 'danceability', 'acousticness'], barmode='group')
fig.show()


#report = dp.Report(dp.Plot(fig) ) #Create a report
#report.publish(name='sound_of_different_genres', open=True, visibility='PUBLIC') #Publish the report
215/13:
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
cluster_pipeline = Pipeline([('scaler', StandardScaler()), ('kmeans', KMeans(n_clusters=10, n_jobs=-1))])
X = genre_data.select_dtypes(np.number)
cluster_pipeline.fit(X)
genre_data['cluster'] = cluster_pipeline.predict(X)
215/14:
from sklearn.manifold import TSNE

tsne_pipeline = Pipeline([('scaler', StandardScaler()), ('tsne', TSNE(n_components=2, verbose=2))])
genre_embedding = tsne_pipeline.fit_transform(X)

projection = pd.DataFrame(columns=['x', 'y'], data=genre_embedding)
projection['genres'] = genre_data['genres']
projection['cluster'] = genre_data['cluster']
215/15:
import plotly.express as px
fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'genres'])
fig.show()
215/16:
song_cluster_pipeline = Pipeline([('scaler', StandardScaler()), 
                                  ('kmeans', KMeans(n_clusters=20, 
                                   verbose=2, n_jobs=4))],verbose=True)
X = spotify_data.select_dtypes(np.number)
number_cols = list(X.columns)
song_cluster_pipeline.fit(X)
song_cluster_labels = song_cluster_pipeline.predict(X)
spotify_data['cluster_label'] = song_cluster_labels
215/17:
from sklearn.decomposition import PCA
pca_pipeline = Pipeline([('scaler', StandardScaler()), ('PCA', PCA(n_components=2))])
song_embedding = pca_pipeline.fit_transform(X)
projection = pd.DataFrame(columns=['x', 'y'], data=song_embedding)
projection['title'] = spotify_data['name']
projection['cluster'] = spotify_data['cluster_label']
215/18:
import plotly.express as px
fig = px.scatter(projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'title'])
fig.show()
215/19:
from collections import defaultdict
from scipy.spatial.distance import cdist
import difflib

number_cols = ['valence', 'year', 'acousticness', 'danceability', 'duration_ms', 'energy', 'explicit',
 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'popularity', 'speechiness', 'tempo']

def get_song_data(song, spotify_data):
    
    """
    Gets the song data for a specific song. The song argument takes the form of a dictionary with 
    key-value pairs for the name and release year of the song.
    """
    
    try:
        song_data = spotify_data[(spotify_data['name'] == song['name']) 
                                & (spotify_data['year'] == song['year'])].iloc[0]
        return song_data
    
    except IndexError:
        return find_song(song['name'], song['year'])
        

def get_mean_vector(song_list, spotify_data):
  
    """
    Gets the mean vector for a list of songs.
    """
    
    song_vectors = []
    
    for song in song_list:
        song_data = get_song_data(song, spotify_data)
        if song_data is None:
            print('Warning: {} does not exist in Spotify or in database'.format(song['name']))
            continue
        song_vector = song_data[number_cols].values
        song_vectors.append(song_vector)  
    
    song_matrix = np.array(list(song_vectors))
    return np.mean(song_matrix, axis=0)

def flatten_dict_list(dict_list):
   
    """
    Utility function for flattening a list of dictionaries.
    """
    
    flattened_dict = defaultdict()
    for key in dict_list[0].keys():
        flattened_dict[key] = []
    
    for dictionary in dict_list:
        for key, value in dictionary.items():
            flattened_dict[key].append(value)
            
    return flattened_dict
        

def recommend_songs(song_list, spotify_data, n_songs=10):
  
    """
    Recommends songs based on a list of previous songs that a user has listened to.
    """
    
    metadata_cols = ['name', 'year', 'artists']
    song_dict = flatten_dict_list(song_list)
    
    song_center = get_mean_vector(song_list, spotify_data)
    scaler = song_cluster_pipeline.steps[0][1]
    scaled_data = scaler.transform(spotify_data[number_cols])
    scaled_song_center = scaler.transform(song_center.reshape(1, -1))
    distances = cdist(scaled_song_center, scaled_data, 'cosine')
    index = list(np.argsort(distances)[:, :n_songs][0])
    
    rec_songs = spotify_data.iloc[index]
    rec_songs = rec_songs[~rec_songs['name'].isin(song_dict['name'])]
    return rec_songs[metadata_cols].to_dict(orient='records')
215/20:
recommend_songs([{'name': 'Come As You Are', 'year':1991},
                {'name': 'Smells Like Teen Spirit', 'year': 1991},
                {'name': 'Lithium', 'year': 1992},
                {'name': 'All Apologies', 'year': 1993},
                {'name': 'Stay Away', 'year': 1993}],  spotify_data)
215/21:
from spotipy.oauth2 import SpotifyClientCredentials
from collections import defaultdict

sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=os.environ["SPOTIFY_CLIENT_ID"],
                                                           client_secret=os.environ["SPOTIFY_CLIENT_SECRET"]))


def find_song(name, year):
    
    song_data = defaultdict()
    results = sp.search(q= 'track: {} year: {}'.format(name,
                                                       year), limit=1)
    if results['tracks']['items'] == []:
        return None
    
    results = results['tracks']['items'][0]

    track_id = results['id']
    audio_features = sp.audio_features(track_id)[0]
    
    song_data['name'] = [name]
    song_data['year'] = [year]
    song_data['explicit'] = [int(results['explicit'])]
    song_data['duration_ms'] = [results['duration_ms']]
    song_data['popularity'] = [results['popularity']]
    
    for key, value in audio_features.items():
        song_data[key] = value
    
    return pd.DataFrame(song_data)
216/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import spotipy
import datapane as dp
import chart_studio.plotly as py
import chart_studio
import os
username = os.environ['PLOTLY_USERNAME']
api_key = os.environ['PLOTLY_API_KEY']
chart_studio.tools.set_credentials_file(username=username, api_key=api_key)
plt.rcParams.update({'font.size': 22})
%matplotlib inline
216/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import spotipy
import datapane as dp
import chart_studio.plotly as py
import chart_studio
import os
%matplotlib inline
216/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import spotipy
import chart_studio.plotly as py
import chart_studio
import os
%matplotlib inline
216/4:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import spotipy
import os
%matplotlib inline
216/5:
spotify_data = pd.read_csv('data.csv')
genre_data = pd.read_csv('data_by_genres.csv')
data_by_year = pd.read_csv('data_by_year.csv')
spotify_data.head(10)
216/6:
spotify_data = pd.read_csv('data.csv')
genre_data = pd.read_csv('data_by_genres.csv')
data_by_year = pd.read_csv('data_by_year.csv')
spotify_data.head(10)
216/7:
spotify_data = pd.read_csv('data.csv')
genre_data = pd.read_csv('data_by_genres.csv')
data_by_year = pd.read_csv('data_by_year.csv')
216/8: spotify_data.head(10)
216/9: spotify_data.info()
216/10: genre_data.info()
216/11: data_by_year.info()
216/12:
import plotly.express as px 

sound_features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'valence']
fig = px.line(data_by_year, x='year', y=sound_features)

fig.show()
216/13:
fig = px.line(data_by_year, x='year', y='tempo')

fig.show()
216/14:
fig = px.bar(top10_genres, x='genres', y=['valence', 'energy', 'danceability', 'acousticness'], barmode='group')

fig.show()
216/15: top10_genres = genre_data.nlargest(10, 'popularity')
216/16:
fig = px.bar(top10_genres, x='genres', y=['valence', 'energy', 'danceability', 'acousticness'], barmode='group')

fig.show()
216/17:
import plotly.express as px

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'genres'])
fig.show()
216/18:
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

cluster_pipeline = Pipeline([('scaler', StandardScaler()), ('kmeans', KMeans(n_clusters=10, n_jobs=-1))])

X = genre_data.select_dtypes(np.number)
cluster_pipeline.fit(X)
genre_data['cluster'] = cluster_pipeline.predict(X)
216/19:
from sklearn.manifold import TSNE

tsne_pipeline = Pipeline([('scaler', StandardScaler()), ('tsne', TSNE(n_components=2, verbose=2))])
genre_embedding = tsne_pipeline.fit_transform(X)

projection = pd.DataFrame(columns=['x', 'y'], data=genre_embedding)
projection['genres'] = genre_data['genres']
projection['cluster'] = genre_data['cluster']
216/20:
import plotly.express as px

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'genres'])
fig.show()
216/21:
song_cluster_pipeline = Pipeline([('scaler', StandardScaler()), 
                                  ('kmeans', KMeans(n_clusters=20, 
                                   verbose=2, n_jobs=4))], verbose=True)
X = spotify_data.select_dtypes(np.number)
number_cols = list(X.columns)
song_cluster_pipeline.fit(X)
216/22: song_cluster_labels = song_cluster_pipeline.predict(X)
216/23: spotify_data['cluster_label'] = song_cluster_labels
216/24:
from sklearn.decomposition import PCA

pca_pipeline = Pipeline([('scaler', StandardScaler()), ('PCA', PCA(n_components=2))])
song_embedding = pca_pipeline.fit_transform(X)

projection = pd.DataFrame(columns=['x', 'y'], data=song_embedding)
projection['title'] = spotify_data['name']
projection['cluster'] = spotify_data['cluster_label']
216/25:
import plotly.express as px

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'title'])
fig.show()
219/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import spotipy
import os
%matplotlib inline
219/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import spotipy
import os
%matplotlib inline
219/3:
spotify_data = pd.read_csv('data.csv')
genre_data = pd.read_csv('data_by_genres.csv')
data_by_year = pd.read_csv('data_by_year.csv')
219/4: spotify_data.info()
219/5: genre_data.info()
219/6: data_by_year.info()
219/7: sns.distplot(spotify_data['popularity'])
219/8:
def get_decade(year):
    
    period_start = int(year/10) * 10
    decade = '{}s'.format(period_start)
    
    return decade

spotify_data['decade'] = spotify_data['year'].apply(get_decade)
219/9:
import plotly.express as px 

sound_features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'valence']
fig = px.line(data_by_year, x='year', y=sound_features)

fig.show()
219/10:
fig = px.line(data_by_year, x='year', y='tempo')

fig.show()
219/11: top10_genres = genre_data.nlargest(10, 'popularity')
219/12:
fig = px.bar(top10_genres, x='genres', y=['valence', 'energy', 'danceability', 'acousticness'], barmode='group')

fig.show()
219/13:
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

cluster_pipeline = Pipeline([('scaler', StandardScaler()), ('kmeans', KMeans(n_clusters=10, n_jobs=-1))])

X = genre_data.select_dtypes(np.number)
cluster_pipeline.fit(X)
genre_data['cluster'] = cluster_pipeline.predict(X)
219/14:
from sklearn.manifold import TSNE

tsne_pipeline = Pipeline([('scaler', StandardScaler()), ('tsne', TSNE(n_components=2, verbose=2))])
genre_embedding = tsne_pipeline.fit_transform(X)

projection = pd.DataFrame(columns=['x', 'y'], data=genre_embedding)
projection['genres'] = genre_data['genres']
projection['cluster'] = genre_data['cluster']
219/15:
import plotly.express as px

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'genres'])
fig.show()
219/16:
song_cluster_pipeline = Pipeline([('scaler', StandardScaler()), 
                                  ('kmeans', KMeans(n_clusters=20, 
                                   verbose=2, n_jobs=4))], verbose=True)
X = spotify_data.select_dtypes(np.number)
number_cols = list(X.columns)
song_cluster_pipeline.fit(X)
219/17: song_cluster_labels = song_cluster_pipeline.predict(X)
219/18: spotify_data['cluster_label'] = song_cluster_labels
219/19:
from sklearn.decomposition import PCA

pca_pipeline = Pipeline([('scaler', StandardScaler()), ('PCA', PCA(n_components=2))])
song_embedding = pca_pipeline.fit_transform(X)

projection = pd.DataFrame(columns=['x', 'y'], data=song_embedding)
projection['title'] = spotify_data['name']
projection['cluster'] = spotify_data['cluster_label']
219/20:
import plotly.express as px

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'title'])
fig.show()
221/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score,jaccard_score,f1_score,log_loss,confusion_matrix
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
221/2:
hr_df = pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")
hr_df.head()
221/3: hr_df.describe()
221/4: hr_df.shape
221/5: hr_df[hr_df.isnull().any(axis=1)]
221/6: df['Attrition'].value_counts()
221/7: hr_df['Attrition'].value_counts()
221/8:
r = df.groupby('Attrition')['Attrition'].count()
plt.pie(r, explode=[0.05, 0.1], labels=['No', 'Yes'], radius=1.5, autopct='%1.1f%%',  shadow=True);
221/9:
r = hr_df.groupby('Attrition')['Attrition'].count()
plt.pie(r, explode=[0.05, 0.1], labels=['No', 'Yes'], radius=1.5, autopct='%1.1f%%',  shadow=True);
221/10: hr_df['Department'].value_counts()
221/11: hr_df['EducationField'].value_counts()
221/12: hr_df['JobRole'].value_counts()
221/13: hr_df['Gender'].value_counts()
221/14: hr_df['Income'].value_counts()
221/15:
fig, axes = plt.subplots(1, 2, figsize=(16, 4))
sns.boxplot(ax = axes[0],palette = "Set3", x = df['Age'])
sns.distplot(ax = axes[1],color = "Green",a=df["Age"])
221/16:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='Age', hue="Attrition", data=df)
plt.title('Attrition Distribution by Age', fontsize=24)
plt.xlabel('Age', fontsize=18)
plt.show()
221/17:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score,jaccard_score,f1_score,log_loss,confusion_matrix
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import seaborn as sns
221/18:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='Age', hue="Attrition", data=df)
plt.title('Attrition Distribution by Age', fontsize=24)
plt.xlabel('Age', fontsize=18)
plt.show()
221/19:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='Age', hue="Attrition", data=hr_df)
plt.title('Attrition Distribution by Age', fontsize=24)
plt.xlabel('Age', fontsize=18)
plt.show()
221/20:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='BusinessTravel', hue="Attrition", data=hr_df)
plt.title('Attrition Distribution by Business Travel Frequency', fontsize=24)
plt.xlabel('Business Travel', fontsize=18)
plt.show()
221/21:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='Gender', hue="Attrition", data=df)
plt.title('Attrition Distribution by Gender', fontsize=24)
plt.xlabel('Gender', fontsize=18)
plt.show()
221/22:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='Gender', hue="Attrition", data=hr_df)
plt.title('Attrition Distribution by Gender', fontsize=24)
plt.xlabel('Gender', fontsize=18)
plt.show()
221/23:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='EnvironmentSatisfaction', hue="Attrition", data=hr_df)
plt.title('Attrition Distribution by Environment Satisfaction', fontsize=24)
plt.xlabel('Environment Satisfaction', fontsize=18)
plt.show()
221/24:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='Department', hue="Attrition", data=df)
plt.title('Attrition Distribution by Department', fontsize=24)
plt.xlabel('Department', fontsize=18)
plt.show()
221/25:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='Department', hue="Attrition", data=hr_df)
plt.title('Attrition Distribution by Department', fontsize=24)
plt.xlabel('Department', fontsize=18)
plt.show()
221/26:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='JobInvolvement', hue="Attrition", data=hr_df)
plt.title('Attrition Distribution by Job Involvement', fontsize=24)
plt.xlabel('Job Involvement', fontsize=18)
plt.show()
221/27:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='PerformanceRating', hue="Attrition", data=df)
plt.title('Attrition Distribution by Performance Rating', fontsize=24)
plt.xlabel('Performance Rating', fontsize=18)
plt.show()
221/28:
sns.set(style="darkgrid")
sns.set(rc={"figure.figsize":(15, 8)})
ax = sns.countplot(x='PerformanceRating', hue="Attrition", data=hr_df)
plt.title('Attrition Distribution by Performance Rating', fontsize=24)
plt.xlabel('Performance Rating', fontsize=18)
plt.show()
221/29: No significant difference shows between attrition rates of different performance ratings.
221/30:
fig,ax = plt.subplots(figsize=(15,7))
working_years_group = df.groupby(['TotalWorkingYears','Attrition']).size().reset_index(name='number')
working_years_group = working_years_group.pivot(columns='Attrition', index='TotalWorkingYears', values = 'number')
working_years_group.plot.bar(ax=ax, color=['orange', 'green'])
plt.xticks(rotation=0)
plt.title('Attrition Distribution by Total Working Years', fontsize=20)
plt.xlabel('Total Working Years', fontsize=16)
plt.show()
221/31:
fig,ax = plt.subplots(figsize=(15,7))
working_years_group = hr_df.groupby(['TotalWorkingYears','Attrition']).size().reset_index(name='number')
working_years_group = working_years_group.pivot(columns='Attrition', index='TotalWorkingYears', values = 'number')
working_years_group.plot.bar(ax=ax, color=['orange', 'green'])
plt.xticks(rotation=0)
plt.title('Attrition Distribution by Total Working Years', fontsize=20)
plt.xlabel('Total Working Years', fontsize=16)
plt.show()
221/32:
fig,ax = plt.subplots(figsize=(15,7))
work_life_balance = df.groupby(['WorkLifeBalance','Attrition']).size().reset_index(name='number')
work_life_balance = work_life_balance.pivot(columns='Attrition', index='WorkLifeBalance', values = 'number')
work_life_balance.plot.bar(ax=ax, color=['orange', 'green'])
plt.xticks(rotation=0)
plt.title('Attrition Distribution by Work-Life Balance', fontsize=20)
plt.xlabel('Work-Life Balance', fontsize=16)
plt.show()
221/33:
fig,ax = plt.subplots(figsize=(15,7))
work_life_balance = hr_df.groupby(['WorkLifeBalance','Attrition']).size().reset_index(name='number')
work_life_balance = work_life_balance.pivot(columns='Attrition', index='WorkLifeBalance', values = 'number')
work_life_balance.plot.bar(ax=ax, color=['orange', 'green'])
plt.xticks(rotation=0)
plt.title('Attrition Distribution by Work-Life Balance', fontsize=20)
plt.xlabel('Work-Life Balance', fontsize=16)
plt.show()
221/34:
fig,ax = plt.subplots(figsize=(15,7))
job_role = hr_df.groupby(['JobRole','Attrition']).size().reset_index(name='number')
job_role = job_role.pivot(columns='Attrition', index='JobRole', values = 'number')
job_role.plot.bar(ax=ax, color=['orange', 'green'])
plt.xticks(rotation=45)
plt.title('Attrition Distribution by Job Role', fontsize=20)
plt.xlabel('Job Role', fontsize=16)
plt.show()
221/35:
df_2 = df.copy()
df_2['Attrition'] = df_2['Attrition'].apply(lambda x: 0 if x=='No' else 1)
df_2['Gender'] = df_2['Gender'].apply(lambda x: 0 if x=='Male' else 1)
df_2['OverTime'] = df_2['OverTime'].apply(lambda x: 0 if x=='No' else 1)

BusinessTravel_map = {'Non-Travel':0, 'Travel_Rarely':1, 'Travel_Frequently':2}
df_2['BusinessTravel'] = df_2['BusinessTravel'].map(BusinessTravel_map)

MaritalStatus_map = {'Single':0, 'Divorced':1, 'Married':2}
df_2['MaritalStatus'] = df_2['MaritalStatus'].map(MaritalStatus_map)

EducationField_map = {'Other':0, 'Life Sciences':1, 'Medical':2, 'Marketing':3, 'Technical Degree':4,
                     'Human Resources':5}
df_2['EducationField'] = df_2['EducationField'].map(EducationField_map)

JobRole_map = {'Sales Executive':0,'Sales Representative':1,'Laboratory Technician':2,'Manufacturing Director':3,
              'Healthcare Representative':4,'Manager':5,'Research Scientist':6,'Research Director':7,'Human Resources':8}
df_2['JobRole'] = df_2['JobRole'].map(JobRole_map)

Department_map = {'Sales':0, 'Research & Development':1, 'Human Resources':2}
df_2['Department'] = df_2['Department'].map(Department_map)
df_2.info()
221/36:
df_2 = hr_df.copy()
df_2['Attrition'] = df_2['Attrition'].apply(lambda x: 0 if x=='No' else 1)
df_2['Gender'] = df_2['Gender'].apply(lambda x: 0 if x=='Male' else 1)
df_2['OverTime'] = df_2['OverTime'].apply(lambda x: 0 if x=='No' else 1)

BusinessTravel_map = {'Non-Travel':0, 'Travel_Rarely':1, 'Travel_Frequently':2}
df_2['BusinessTravel'] = df_2['BusinessTravel'].map(BusinessTravel_map)

MaritalStatus_map = {'Single':0, 'Divorced':1, 'Married':2}
df_2['MaritalStatus'] = df_2['MaritalStatus'].map(MaritalStatus_map)

EducationField_map = {'Other':0, 'Life Sciences':1, 'Medical':2, 'Marketing':3, 'Technical Degree':4,
                     'Human Resources':5}
df_2['EducationField'] = df_2['EducationField'].map(EducationField_map)

JobRole_map = {'Sales Executive':0,'Sales Representative':1,'Laboratory Technician':2,'Manufacturing Director':3,
              'Healthcare Representative':4,'Manager':5,'Research Scientist':6,'Research Director':7,'Human Resources':8}
df_2['JobRole'] = df_2['JobRole'].map(JobRole_map)

Department_map = {'Sales':0, 'Research & Development':1, 'Human Resources':2}
df_2['Department'] = df_2['Department'].map(Department_map)
df_2.info()
221/37:
plt.figure(figsize=(30,15))
correlation = sns.heatmap(df_2.corr(), vmin=-1, vmax=1, annot=True, linewidths=1)
correlation.set_title('Correlation Graph', fontdict={'fontsize': 24})
221/38: x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)
221/39:
# applying encoding
y = LabelEncoder().fit_transform(y)
print(y[:10])
221/40:
x = hr_df.iloc[:,:].values
x= np.delete(x,1,axis=1)
print(x[:10,:])
221/41:
y = hr_df.Attrition
y.head()
221/42:
# applying encoding
y = LabelEncoder().fit_transform(y)
print(y[:10])
221/43:
x[:,10]=LabelEncoder().fit_transform(x[:,10])
x[:,20]=LabelEncoder().fit_transform(x[:,20])
x[:,21]=LabelEncoder().fit_transform(x[:,21])
print(x[:10,:])
221/44:
ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1,3,6,14,16])],remainder='passthrough') # Apply tranformation to change education to binary vector
x = np.array(ct.fit_transform(x))
print(x[:10,:])
221/45: x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)
221/46:
# Normalize the data
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)
221/47:
model = LogisticRegression(solver='lbfgs')
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

print("Accuracy {0:.2f}%".format(100*accuracy_score(y_pred, y_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
221/48:
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

print("Accuracy {0:.2f}%".format(100*accuracy_score(y_pred, y_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
221/49:
result = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
result
221/50:
feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)
feature_imp
221/51:
feature_imp = pd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)
feature_imp
221/52:
feature_imp = pd.Series(model.feature_importances_,index=x.columns).sort_values(ascending=False)
feature_imp
221/53:
feature_imp = pd.Series(model.feature_importances_,index=x.columns).values(ascending=False)
feature_imp
221/54:
from sklearn.ensemble import RandomForestClassifier

clf=RandomForestClassifier(n_estimators=100, max_depth=3)
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)

print("Accuracy {0:.2f}%".format(100*accuracy_score(y_pred, y_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
221/55:
from sklearn.ensemble import RandomForestClassifier

clf=RandomForestClassifier(n_estimators=100, max_depth=3)
clf.fit(x_train,y_train)
y_pred=clf.predict(X_test)

print("Accuracy {0:.2f}%".format(100*accuracy_score(y_pred, y_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
221/56:
from sklearn.ensemble import RandomForestClassifier

clf=RandomForestClassifier(n_estimators=100, max_depth=3)
clf.fit(x_train,y_train)
y_pred=clf.predict(x_test)

print("Accuracy {0:.2f}%".format(100*accuracy_score(y_pred, y_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
221/57:
feature_imp = pd.Series(model.feature_importances_,index=x.columns).sort_values(ascending=False)
feature_imp
221/58:
feature_imp = pd.Series(model.feature_importances_,index=x.columns)
feature_imp
221/59:
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

print("Accuracy {0:.2f}%".format(100*accuracy_score(y_pred, y_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
221/60:
y_pred_proba = logreg.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()
221/61:
y_pred_proba = model.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()
221/62:
y_pred_proba = model.predict_proba(x_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()
221/63:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score,jaccard_score,f1_score,log_loss,confusion_matrix
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import seaborn as sns
from sklearn import metrics
221/64:
y_pred_proba = model.predict_proba(x_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()
221/65:
result_2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
result_2
221/66:
#Evaluating performance
results = []
names = []
scoring = 'accuracy'

for name, model in models:
    kfold = model_selection.KFold(n_splits=10, random_state=0)
    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    results.append(cv_results.mean())
    print(msg)

plt.bar(names,results)
221/67: names = "Random Forest", "Logistic Regression"]
221/68: names =["Random Forest", "Logistic Regression"]
221/69: classifiers = [RandomForestClassifier(),LogisticRegression(solver='lbfgs')]
221/70:
model_cols = []
df=pd.DataFrame(columns=model_cols)
index=0
221/71:
for name, clf in zip(names, classifiers):
    clf.fit(x_train,y_train)
    df.loc[index,'Classifiers'] = name
    df.loc[index,'Train Accuracy'] = clf.score(x_train,y_train)
    df.loc[index,'Test Accuracy'] = clf.score(x_test,y_test)
    df.loc[index,'Precision'] = precision_score(y_test,clf.predict(x_test))
    df.loc[index,'Recall'] = recall_score(y_test,clf.predict(x_test))
    df.loc[index,'F1 Score'] = f1_score(y_test,clf.predict(x_test))
    index+=1
221/72:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score,jaccard_score,f1_score,log_loss,confusion_matrix
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import seaborn as sns
from sklearn import metrics
from sklearn.metrics import precision_score, recall_score, auc,roc_curve, f1_score
221/73:
for name, clf in zip(names, classifiers):
    clf.fit(x_train,y_train)
    df.loc[index,'Classifiers'] = name
    df.loc[index,'Train Accuracy'] = clf.score(x_train,y_train)
    df.loc[index,'Test Accuracy'] = clf.score(x_test,y_test)
    df.loc[index,'Precision'] = precision_score(y_test,clf.predict(x_test))
    df.loc[index,'Recall'] = recall_score(y_test,clf.predict(x_test))
    df.loc[index,'F1 Score'] = f1_score(y_test,clf.predict(x_test))
    index+=1
221/74:
sns.barplot(x='Classifiers',y='Train Accuracy', data=df, palette='hot',
            edgecolor=sns.color_palette('dark',7))
plt.xticks(rotation=90)
plt.title('Model Train Accuracy Comparision')
plt.show()
221/75:
sns.barplot(x='Classifiers',y='Test Accuracy', data=df, palette='hot',
            edgecolor=sns.color_palette('dark',7))
plt.xticks(rotation=90)
plt.title('Model Test Accuracy Comparision')
plt.show()
221/76:
import seaborn as sns
sns.barplot(x='Classifiers',y='Precision', data=df, palette='hot',
            edgecolor=sns.color_palette('dark',7))
plt.xticks(rotation=90)
plt.title('Model Precision Comparision')
plt.show()
221/77:
sns.barplot(x='Classifiers',y='Precision', data=df, palette='hot',
            edgecolor=sns.color_palette('dark',7))
plt.xticks(rotation=90)
plt.title('Model Precision Comparision')
plt.show()
221/78:
sns.barplot(x='Classifiers',y='Test Accuracy', data=df, palette='hot',
            edgecolor=sns.color_palette('dark',7))
plt.xticks(rotation=90)
plt.title('Model Accuracy Comparision')
plt.show()
221/79:
result_2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
result_2
221/80:
for name, clf in zip(names, classifiers):
    clf.fit(x_train,y_train)
    df.loc[index,'Classifiers'] = name
    df.loc[index,'Train Accuracy'] = clf.score(x_train,y_train)
    df.loc[index,'Accuracy'] = clf.score(x_test,y_test)
    df.loc[index,'Precision'] = precision_score(y_test,clf.predict(x_test))
    df.loc[index,'Recall'] = recall_score(y_test,clf.predict(x_test))
    df.loc[index,'F1 Score'] = f1_score(y_test,clf.predict(x_test))
    index+=1
221/81:
sns.barplot(x='Classifiers',y='Accuracy', data=df, palette='hot',
            edgecolor=sns.color_palette('dark',7))
plt.xticks(rotation=90)
plt.title('Model Accuracy Comparision')
plt.show()
229/1:

import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

data=pd.read_csv('fire_archive_V1_96617.csv')

print(data)
data.plot(x='longitude',y='latitude',kind="scatter",figsize=(10,6))
plt.show()
229/2:
#Adjusting alpha and marker size to compensate for overplotting.

import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

data=pd.read_csv('fire_archive_V1_96617.csv')
#print(data)
data.plot(x='longitude',y='latitude',alpha=0.4,kind="scatter",figsize=(10,6),marker="o",s=5)
plt.show()
229/3:
#Hexbin Plot

import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

data=pd.read_csv('fire_archive_V1_96617.csv')
#print(data)
data.plot.hexbin(x='longitude',y='latitude',figsize=(10,6))#,gridsize=(10,6))
229/4:
import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

data=pd.read_csv('fire_archive_V1_96617.csv')
#print(data)
data.plot.hexbin(x='longitude',y='latitude',figsize=(10,6),gridsize=100)
229/5: data['bright_ti4'].hist(bins=50,range=(200,500),figsize =(10, 7))
229/6:
#Subsampling the dataset


import random
subsampled=pd.DataFrame(columns=data.columns)
subsample_per=0.02
new_samp_size=int(subsample_per*len(data))
for i in range(1,new_samp_size):
    randomrownr=random.randint(1,len(data)-1)
    subsampled.loc[i]=data.loc[randomrownr]
print(subsampled)
229/7:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml

data=pd.read_csv('fire_archive_V1_96617.csv');
#print(data)
229/8: data['bright_ti4'].hist()
229/9:
saturated = pd.DataFrame()
saturated = data.loc[(data['bright_ti4'] > 300)]

not_saturated = pd.DataFrame()
not_saturated = data.loc[(data['bright_ti4']) < 300]
229/10:
marker_size_1 = saturated['bright_ti4'] / 100
marker_size_2 = not_saturated['bright_ti4'] / 100
229/11:
%matplotlib inline
fig, ax = plt.subplots(1,2, figsize=(22,8),sharex = True, sharey = True)
plot1 = ax[0].scatter('longitude','latitude', data= saturated, alpha = 0.1, s = marker_size_1, label="Temperature",
                     c= 'bright_ti4', cmap = plt.get_cmap('jet'))
ax[0].set_ylabel('Latitude')
ax[0].set_title('Saturated Plot')
ax[0].legend()
fig.colorbar(plot1, ax= ax[0])

plot2  = ax[1].scatter('longitude','latitude',data = not_saturated, alpha = 0.1, s = marker_size_2, label="Temperature",
                     c= 'bright_ti4', cmap = plt.get_cmap('jet'))
ax[1].set_xlabel('Longitude')
ax[1].set_title('Non-Saturated Plot')
ax[1].legend()
fig.colorbar(plot2, ax = ax[1])

plt.show()
229/12:
plt.figure(figsize=(12,8))
plt.scatter('longitude','latitude', data= saturated, alpha = 0.1, s = marker_size_1, label="Saturated Temperature",
                     cmap = 'Reds')

plt.scatter('longitude','latitude',data = not_saturated, alpha = 0.1, s = marker_size_2, label="Non- Saturated Temperature",
                     cmap = 'Blues')
plt.title('Saturated   vs   Non-Saturated Plot')
plt.set_xlabel = 'Longitude'
plt.set_ylabel = 'Latitude'
plt.legend()
plt.show()
229/13:
import csv
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold, LeaveOneOut, cross_val_score, GridSearchCV, ParameterGrid
from sklearn.linear_model import Ridge, LogisticRegression, LinearRegression, Lasso, ElasticNet
from sklearn.metrics import confusion_matrix



import seaborn as sns
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import seaborn as sns
import warnings
229/14: house_price = pd.read_csv('data (1).csv')
229/15: house_price.head()
229/16: house_price.drop(house_price[house_price.price == 0.0].index, inplace=True)
229/17: house_price.head()
229/18:
categorical_variables = house_price.select_dtypes(exclude='number').columns.tolist()
categorical_variables
229/19:
continuous_variables = house_price.select_dtypes(include='number').columns.tolist()
continuous_variables
229/20:
prices = house_price['price']
bedrooms = house_price['bedrooms']
bathrooms = house_price['bathrooms']
sqft_living = house_price['sqft_living']
sqft_lot = house_price['sqft_lot']
floors = house_price['floors']
views = house_price['view']
conditions = house_price['condition']
sqft_above = house_price['sqft_above']
sqft_basement = house_price.sqft_basement
year_built = house_price.yr_built
year_renovated = house_price.yr_renovated
229/21:
plt.figure(figsize=(15, 40))
ax1 = plt.subplot(6,2,1)
ax1.hist(prices, bins=10)
ax1.set_xlabel('Prices')

ax2 = plt.subplot(6,2,2)
ax2.hist(bedrooms, bins=5)
ax2.set_xlabel('Bedrooms')

ax3 = plt.subplot(6,2,3)
ax3.hist(bathrooms, bins=5)
ax3.set_xlabel('Bathrooms')

ax4 = plt.subplot(6,2,4)
ax4.hist(sqft_living, bins=5)
ax4.set_xlabel('Sqft living')

ax5 = plt.subplot(6,2,5)
ax5.hist(sqft_lot, bins=10)
ax5.set_xlabel('Sqft lot')

ax6 = plt.subplot(6,2,6)
ax6.hist(floors, bins=5)
ax6.set_xlabel('Floors')

ax7 = plt.subplot(6,2,7)
ax7.hist(views, bins=5)
ax7.set_xlabel('Views')

ax8 = plt.subplot(6,2,8)
ax8.hist(conditions, bins=5)
ax8.set_xlabel('Condition')

ax9 = plt.subplot(6,2,9)
ax9.hist(bathrooms, bins=5)
ax9.set_xlabel('Sqft above')

ax10 = plt.subplot(6,2,10)
ax10.hist(sqft_basement, bins=5)
ax10.set_xlabel('Sqft basement')

ax11 = plt.subplot(6,2,11)
ax11.hist(year_built, bins=5)
ax11.set_xlabel('Year built')

ax12 = plt.subplot(6,2,12)
ax12.hist(year_renovated, bins=50)
ax12.set_xlabel('Year renovated')

plt.show()
229/22:
year_renovated = house_price.drop(house_price[house_price.yr_renovated == 0].index, inplace=False).yr_renovated
plt.figure(figsize=(8, 8))
plt.hist(year_renovated, bins=5)
plt.xlabel('Year renovated')
plt.show()
229/23:
bedrooms = house_price['bedrooms']
bathrooms = house_price['bathrooms']
sqft_living = house_price['sqft_living']
sqft_lot = house_price['sqft_lot']
floors = house_price['floors']
views = house_price['view']
conditions = house_price['condition']
sqft_above = house_price['sqft_above']
sqft_basement = house_price.sqft_basement
year_built = house_price.yr_built
year_renovated = house_price.yr_renovated
229/24:
plt.figure(figsize=(15, 40))
ax1 = plt.subplot(6,2,1)
ax1.scatter(bedrooms, prices)
ax1.set_yscale('log')
ax1.set_xlabel('bedrooms')
ax1.set_ylabel('price')

ax2 = plt.subplot(6,2,2)
ax2.scatter(bathrooms, prices)
ax2.set_yscale('log')
ax2.set_xlabel('bathrooms')
ax2.set_ylabel('price')

ax3 = plt.subplot(6,2,3)
ax3.scatter(sqft_living, prices)
ax3.set_yscale('log')
ax3.set_xlabel('sqft_living')
ax3.set_ylabel('price')

ax4 = plt.subplot(6,2,4)
ax4.scatter(sqft_lot, prices)
ax4.set_yscale('log')
ax4.set_xlabel('sqft_lot')
ax4.set_ylabel('price')

ax5 = plt.subplot(6,2,5)
ax5.scatter(floors, prices)
ax5.set_yscale('log')
ax5.set_xlabel('floors')
ax5.set_ylabel('price')

ax6 = plt.subplot(6,2,6)
ax6.scatter(views, prices)
ax6.set_yscale('log')
ax6.set_xlabel('view')
ax6.set_ylabel('price')

ax7 = plt.subplot(6,2,7)
ax7.scatter(conditions, prices)
ax7.set_yscale('log')
ax7.set_xlabel('condition')
ax7.set_ylabel('price')

ax8 = plt.subplot(6,2,8)
ax8.scatter(sqft_above, prices)
ax8.set_yscale('log')
ax8.set_xlabel('sqft_above')
ax8.set_ylabel('price')

ax9 = plt.subplot(6,2,9)
ax9.scatter(sqft_basement, prices)
ax9.set_yscale('log')
ax9.set_xlabel('sqft_basement')
ax9.set_ylabel('price')

ax10 = plt.subplot(6,2,10)
ax10.scatter(year_built, prices)
ax10.set_yscale('log')
ax10.set_xlabel('yr_built')
ax10.set_ylabel('price')

ax11 = plt.subplot(6,2,11)
ax11.scatter(year_renovated, prices)
ax11.set_yscale('log')
ax11.set_xlabel('yr_renovated')
ax11.set_ylabel('price')


plt.show()
229/25: house_price['yr_renovated'] = house_price['yr_renovated'].apply(lambda x: np.nan if x == 0 else x)
229/26: house_price.head()
229/27:
house_price[['waterfront']] = house_price[['waterfront']].astype('str')
categorical_features = ['city', 'statezip', 'country', 'waterfront']
continuous_features = [feature for feature in list(house_price.columns) if feature not in categorical_features and feature != 'date' and feature != 'price' and feature != 'street']
all_features = continuous_features + categorical_features
229/28:
price_df = house_price.iloc[:,1]
features_df = house_price[all_features]
X_trainval_pipe, X_test_pipe, y_trainval_pipe, y_test_pipe = train_test_split(features_df, price_df, test_size=0.2)
229/29:
category_check = features_df.dtypes == object
category_check
229/30:
preprocess = make_column_transformer(
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, LinearRegression())
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
229/31:
preprocess = make_column_transformer(
    (StandardScaler(), ~category_check),
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, LinearRegression())
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
229/32:
preprocess = make_column_transformer(
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model_ridge = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, Ridge())
model_ridge.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model_ridge, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
229/33:
preprocess = make_column_transformer(
    (StandardScaler(), ~category_check),
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model_ridge = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, Ridge())
model_ridge.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model_ridge, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
229/34:
preprocess = make_column_transformer(
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, Lasso(max_iter=1000, tol=10000.008))
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
229/35:
preprocess = make_column_transformer(
    (StandardScaler(), ~category_check),
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, Lasso(max_iter=5000, tol=1000.009))
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
229/36:
preprocess = make_column_transformer(
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, ElasticNet())
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
229/37:
preprocess = make_column_transformer(
    (StandardScaler(), ~category_check),
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, ElasticNet())
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
229/38:
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_openml
credit_data=fetch_openml('credit-g',version="active")
229/39: print(credit_data)
229/40: print(credit_data.data.shape)
229/41: print(credit_data.target.shape)
229/42: df = pd.DataFrame(data=credit_data.data, columns=credit_data.feature_names)
229/43: print(df.dtypes)
229/44:
# Categorical features

categorical_columns = [col for col in df.columns if col not in df.describe().columns]
print(categorical_columns)
229/45:
#Continuous features

numerical_columns = df.describe().columns
print (numerical_columns)
229/46:
# Categorical features

categorical_columns = [col for col in df.columns if col not in df.describe().columns]
print(categorical_columns)
229/47:
#Univariate Distribution of continuous variables


plt.subplot(14, 1, 1)
df['duration'].hist(bins=10,range=(20,50),figsize =(12, 20))
plt.xlabel('Duration')

plt.subplot(14, 1, 2)
df['credit_amount'].hist(bins=30,range=(200,500),figsize =(12, 20))
plt.xlabel('Credit Amount')

plt.subplot(14, 1, 3)
df['installment_commitment'].hist(bins=30,range=(1,10),figsize =(12, 20))
plt.xlabel('installment_commitment')

plt.subplot(14, 1, 4)
df['residence_since'].hist(bins=30,range=(1,10),figsize =(12, 20))
plt.xlabel('residence_since')

plt.subplot(14, 1, 5)
df['age'].hist(bins=30,range=(1,100),figsize =(12, 20))
plt.xlabel('age')

plt.subplot(14, 1, 6)
df['existing_credits'].hist(bins=30,range=(1,5),figsize =(12, 20))
plt.xlabel('existing_credits')

plt.subplot(14, 1, 7)
df['num_dependents'].hist(bins=30,range=(1,5),figsize =(12, 20))
plt.xlabel('num_dependents')

plt.subplots_adjust(top=2, bottom=0.01)
plt.show()
229/48:
#PLOTTING THE TARGET

cg=0
cb=0
for i in range(len(credit_data.target)):
    if credit_data.target[i]=='good':
        cg=cg+1
    else:
        cb=cb+1
labels=['Good','Bad']
values=[cg,cb]
fig = plt.figure(figsize = (6, 8))
plt.bar(labels, values, color ='purple',
        width = 0.2)
 

plt.title("Target")
plt.show()
229/49:
#PREPROCESSING THE DATA

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder()

for i in range(len(categorical_columns)):
    df[categorical_columns[i]]= label_encoder.fit_transform(df[categorical_columns[i]]) 
    
print(df.head())

#SPLITTING THE DATA 
#LOGISTIC REGRESSION

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler  
scaler = StandardScaler()
cols = credit_data.feature_names
X = df[cols]
y = credit_data.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42) 

X_train_std =scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)

logistic_model = LogisticRegression()

logistic_model.fit(X_train_std, y_train)

y_pred = logistic_model.predict(X_test_std)
229/50:
from sklearn.metrics import confusion_matrix, accuracy_score 
accuracy = accuracy_score(y_test,y_pred)*100
print(accuracy)
229/51:
#ColumnTransformer

#Without scaling
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import OneHotEncoder

categorical_features = ['checking_status', 'credit_history', 'employment', 'foreign_worker',
'housing',
'job',
'other_parties',
'other_payment_plans',
'own_telephone',
'personal_status',
'property_magnitude',
'purpose',
'savings_status']

preprocess_lr = make_column_transformer(
    (OneHotEncoder(), categorical_features))

X_train_pipe, X_test_pipe, y_train_pipe, y_test_pipe = train_test_split(credit_data.data, credit_data.target)

log_reg = make_pipeline(preprocess_lr, LogisticRegression())
log_reg.fit(X_train_pipe, y_train_pipe)
y_pred_pipe = log_reg.predict(X_test_pipe)
accuracy = accuracy_score(y_test_pipe,y_pred_pipe)*100
print(accuracy)
229/52:
# Categorical features

categorical_columns = [col for col in df.columns if col not in df.describe().columns]
print(categorical_columns)
229/53:
# Categorical features

credit_data.categories
229/54:
#ColumnTransformer

#Without scaling
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import OneHotEncoder

categorical_features = ['checking_status', 'credit_history', 'employment', 'foreign_worker',
'housing',
'job',
'other_parties',
'other_payment_plans',
'own_telephone',
'personal_status',
'property_magnitude',
'purpose',
'savings_status']

preprocess_lr = make_column_transformer(
    (OneHotEncoder(), categorical_features))

X_train_pipe, X_test_pipe, y_train_pipe, y_test_pipe = train_test_split(credit_data.data, credit_data.target)

log_reg = make_pipeline(preprocess_lr, LogisticRegression())
log_reg.fit(X_train_pipe, y_train_pipe)
y_pred_pipe = log_reg.predict(X_test_pipe)
accuracy = accuracy_score(y_test_pipe,y_pred_pipe)*100
print(accuracy)
229/55:
# Categorical features

categorical_columns = [col for col in df.columns if col not in df.describe().columns]
categorical_columns
229/56:
#Logistic Regression with scaling

numeric_features = [feature for feature in credit_data.feature_names if feature not in categorical_features]

preprocess_lr_s = make_column_transformer(
    (StandardScaler(), numeric_features),
    (OneHotEncoder(), categorical_features))

X_train_pipe, X_test_pipe, y_train_pipe, y_test_pipe = train_test_split(credit_data.data, credit_data.target)

log_reg_s = make_pipeline(preprocess_lr_s, LogisticRegression())
log_reg_s.fit(X_train_pipe, y_train_pipe)
y_pred_pipe = log_reg_s.predict(X_test_pipe)
accuracy = accuracy_score(y_test_pipe,y_pred_pipe)*100
print(accuracy)
236/1:

import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

data=pd.read_csv('fire_archive_V1_96617.csv')

print(data)
data.plot(x='longitude',y='latitude',kind="scatter",figsize=(10,6))
plt.show()
236/2:
#Adjusting alpha and marker size to compensate for overplotting.

import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

data=pd.read_csv('fire_archive_V1_96617.csv')
#print(data)
data.plot(x='longitude',y='latitude',alpha=0.4,kind="scatter",figsize=(10,6),marker="o",s=5)
plt.show()
236/3:
#Hexbin Plot

import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

data=pd.read_csv('fire_archive_V1_96617.csv')
#print(data)
data.plot.hexbin(x='longitude',y='latitude',figsize=(10,6))#,gridsize=(10,6))
236/4:
import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

data=pd.read_csv('fire_archive_V1_96617.csv')
#print(data)
data.plot.hexbin(x='longitude',y='latitude',figsize=(10,6),gridsize=100)
236/5: data['bright_ti4'].hist(bins=50,range=(200,500),figsize =(10, 7))
236/6:
#Subsampling the dataset


import random
subsampled=pd.DataFrame(columns=data.columns)
subsample_per=0.02
new_samp_size=int(subsample_per*len(data))
for i in range(1,new_samp_size):
    randomrownr=random.randint(1,len(data)-1)
    subsampled.loc[i]=data.loc[randomrownr]
print(subsampled)
236/7:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml

data=pd.read_csv('fire_archive_V1_96617.csv');
#print(data)
236/8: data['bright_ti4'].hist()
236/9:
saturated = pd.DataFrame()
saturated = data.loc[(data['bright_ti4'] > 300)]

not_saturated = pd.DataFrame()
not_saturated = data.loc[(data['bright_ti4']) < 300]
236/10:
marker_size_1 = saturated['bright_ti4'] / 100
marker_size_2 = not_saturated['bright_ti4'] / 100
236/11:
%matplotlib inline
fig, ax = plt.subplots(1,2, figsize=(22,8),sharex = True, sharey = True)
plot1 = ax[0].scatter('longitude','latitude', data= saturated, alpha = 0.1, s = marker_size_1, label="Temperature",
                     c= 'bright_ti4', cmap = plt.get_cmap('jet'))
ax[0].set_ylabel('Latitude')
ax[0].set_title('Saturated Plot')
ax[0].legend()
fig.colorbar(plot1, ax= ax[0])

plot2  = ax[1].scatter('longitude','latitude',data = not_saturated, alpha = 0.1, s = marker_size_2, label="Temperature",
                     c= 'bright_ti4', cmap = plt.get_cmap('jet'))
ax[1].set_xlabel('Longitude')
ax[1].set_title('Non-Saturated Plot')
ax[1].legend()
fig.colorbar(plot2, ax = ax[1])

plt.show()
236/12:
plt.figure(figsize=(12,8))
plt.scatter('longitude','latitude', data= saturated, alpha = 0.1, s = marker_size_1, label="Saturated Temperature",
                     cmap = 'Reds')

plt.scatter('longitude','latitude',data = not_saturated, alpha = 0.1, s = marker_size_2, label="Non- Saturated Temperature",
                     cmap = 'Blues')
plt.title('Saturated   vs   Non-Saturated Plot')
plt.set_xlabel = 'Longitude'
plt.set_ylabel = 'Latitude'
plt.legend()
plt.show()
236/13:
import csv
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold, LeaveOneOut, cross_val_score, GridSearchCV, ParameterGrid
from sklearn.linear_model import Ridge, LogisticRegression, LinearRegression, Lasso, ElasticNet
from sklearn.metrics import confusion_matrix



import seaborn as sns
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import seaborn as sns
import warnings
236/14: house_price = pd.read_csv('data (1).csv')
236/15: house_price.head()
236/16: house_price.drop(house_price[house_price.price == 0.0].index, inplace=True)
236/17: house_price.head()
236/18:
categorical_variables = house_price.select_dtypes(exclude='number').columns.tolist()
categorical_variables
236/19:
continuous_variables = house_price.select_dtypes(include='number').columns.tolist()
continuous_variables
236/20:
prices = house_price['price']
bedrooms = house_price['bedrooms']
bathrooms = house_price['bathrooms']
sqft_living = house_price['sqft_living']
sqft_lot = house_price['sqft_lot']
floors = house_price['floors']
views = house_price['view']
conditions = house_price['condition']
sqft_above = house_price['sqft_above']
sqft_basement = house_price.sqft_basement
year_built = house_price.yr_built
year_renovated = house_price.yr_renovated
236/21:
plt.figure(figsize=(15, 40))
ax1 = plt.subplot(6,2,1)
ax1.hist(prices, bins=10)
ax1.set_xlabel('Prices')

ax2 = plt.subplot(6,2,2)
ax2.hist(bedrooms, bins=5)
ax2.set_xlabel('Bedrooms')

ax3 = plt.subplot(6,2,3)
ax3.hist(bathrooms, bins=5)
ax3.set_xlabel('Bathrooms')

ax4 = plt.subplot(6,2,4)
ax4.hist(sqft_living, bins=5)
ax4.set_xlabel('Sqft living')

ax5 = plt.subplot(6,2,5)
ax5.hist(sqft_lot, bins=10)
ax5.set_xlabel('Sqft lot')

ax6 = plt.subplot(6,2,6)
ax6.hist(floors, bins=5)
ax6.set_xlabel('Floors')

ax7 = plt.subplot(6,2,7)
ax7.hist(views, bins=5)
ax7.set_xlabel('Views')

ax8 = plt.subplot(6,2,8)
ax8.hist(conditions, bins=5)
ax8.set_xlabel('Condition')

ax9 = plt.subplot(6,2,9)
ax9.hist(bathrooms, bins=5)
ax9.set_xlabel('Sqft above')

ax10 = plt.subplot(6,2,10)
ax10.hist(sqft_basement, bins=5)
ax10.set_xlabel('Sqft basement')

ax11 = plt.subplot(6,2,11)
ax11.hist(year_built, bins=5)
ax11.set_xlabel('Year built')

ax12 = plt.subplot(6,2,12)
ax12.hist(year_renovated, bins=50)
ax12.set_xlabel('Year renovated')

plt.show()
236/22:
year_renovated = house_price.drop(house_price[house_price.yr_renovated == 0].index, inplace=False).yr_renovated
plt.figure(figsize=(8, 8))
plt.hist(year_renovated, bins=5)
plt.xlabel('Year renovated')
plt.show()
236/23:
bedrooms = house_price['bedrooms']
bathrooms = house_price['bathrooms']
sqft_living = house_price['sqft_living']
sqft_lot = house_price['sqft_lot']
floors = house_price['floors']
views = house_price['view']
conditions = house_price['condition']
sqft_above = house_price['sqft_above']
sqft_basement = house_price.sqft_basement
year_built = house_price.yr_built
year_renovated = house_price.yr_renovated
236/24:
plt.figure(figsize=(15, 40))
ax1 = plt.subplot(6,2,1)
ax1.scatter(bedrooms, prices)
ax1.set_yscale('log')
ax1.set_xlabel('bedrooms')
ax1.set_ylabel('price')

ax2 = plt.subplot(6,2,2)
ax2.scatter(bathrooms, prices)
ax2.set_yscale('log')
ax2.set_xlabel('bathrooms')
ax2.set_ylabel('price')

ax3 = plt.subplot(6,2,3)
ax3.scatter(sqft_living, prices)
ax3.set_yscale('log')
ax3.set_xlabel('sqft_living')
ax3.set_ylabel('price')

ax4 = plt.subplot(6,2,4)
ax4.scatter(sqft_lot, prices)
ax4.set_yscale('log')
ax4.set_xlabel('sqft_lot')
ax4.set_ylabel('price')

ax5 = plt.subplot(6,2,5)
ax5.scatter(floors, prices)
ax5.set_yscale('log')
ax5.set_xlabel('floors')
ax5.set_ylabel('price')

ax6 = plt.subplot(6,2,6)
ax6.scatter(views, prices)
ax6.set_yscale('log')
ax6.set_xlabel('view')
ax6.set_ylabel('price')

ax7 = plt.subplot(6,2,7)
ax7.scatter(conditions, prices)
ax7.set_yscale('log')
ax7.set_xlabel('condition')
ax7.set_ylabel('price')

ax8 = plt.subplot(6,2,8)
ax8.scatter(sqft_above, prices)
ax8.set_yscale('log')
ax8.set_xlabel('sqft_above')
ax8.set_ylabel('price')

ax9 = plt.subplot(6,2,9)
ax9.scatter(sqft_basement, prices)
ax9.set_yscale('log')
ax9.set_xlabel('sqft_basement')
ax9.set_ylabel('price')

ax10 = plt.subplot(6,2,10)
ax10.scatter(year_built, prices)
ax10.set_yscale('log')
ax10.set_xlabel('yr_built')
ax10.set_ylabel('price')

ax11 = plt.subplot(6,2,11)
ax11.scatter(year_renovated, prices)
ax11.set_yscale('log')
ax11.set_xlabel('yr_renovated')
ax11.set_ylabel('price')


plt.show()
236/25: house_price['yr_renovated'] = house_price['yr_renovated'].apply(lambda x: np.nan if x == 0 else x)
236/26: house_price.head()
236/27:
house_price[['waterfront']] = house_price[['waterfront']].astype('str')
categorical_features = ['city', 'statezip', 'country', 'waterfront']
continuous_features = [feature for feature in list(house_price.columns) if feature not in categorical_features and feature != 'date' and feature != 'price' and feature != 'street']
all_features = continuous_features + categorical_features
236/28:
price_df = house_price.iloc[:,1]
features_df = house_price[all_features]
X_trainval_pipe, X_test_pipe, y_trainval_pipe, y_test_pipe = train_test_split(features_df, price_df, test_size=0.2)
236/29:
category_check = features_df.dtypes == object
category_check
236/30:
preprocess = make_column_transformer(
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, LinearRegression())
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
236/31:
preprocess = make_column_transformer(
    (StandardScaler(), ~category_check),
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, LinearRegression())
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
236/32:
preprocess = make_column_transformer(
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model_ridge = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, Ridge())
model_ridge.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model_ridge, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
236/33:
preprocess = make_column_transformer(
    (StandardScaler(), ~category_check),
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model_ridge = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, Ridge())
model_ridge.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model_ridge, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
236/34:
preprocess = make_column_transformer(
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, Lasso(max_iter=1000, tol=10000.008))
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
236/35:
preprocess = make_column_transformer(
    (StandardScaler(), ~category_check),
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, Lasso(max_iter=5000, tol=1000.009))
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
236/36:
preprocess = make_column_transformer(
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, ElasticNet())
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
236/37:
preprocess = make_column_transformer(
    (StandardScaler(), ~category_check),
    (OneHotEncoder(handle_unknown='ignore'), category_check))
model = make_pipeline(SimpleImputer(strategy='most_frequent'), preprocess, ElasticNet())
model.fit(X_trainval_pipe, y_trainval_pipe)
scores = cross_val_score(model, X_trainval_pipe, y_trainval_pipe, cv=5)
print("score: {:.3f}".format(np.mean(scores)))
236/38:
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_openml
credit_data=fetch_openml('credit-g',version="active")
236/39: print(credit_data)
236/40: print(credit_data.data.shape)
236/41: print(credit_data.target.shape)
236/42: df = pd.DataFrame(data=credit_data.data, columns=credit_data.feature_names)
236/43: print(df.dtypes)
236/44:
#Continuous features

numerical_columns = df.describe().columns
print (numerical_columns)
236/45:
# Categorical features

categorical_columns = [col for col in df.columns if col not in numercial_columns]
print(categorical_columns)
236/46:
# Categorical features

categorical_columns = [col for col in df.columns if col not in numercial_columns]
categorical_columns
236/47:
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold, LeaveOneOut, cross_val_score, GridSearchCV, ParameterGrid
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.metrics import confusion_matrix
# import category_encoders as ce
import seaborn as sns
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
236/48: print(credit_data)
236/49:

credit_data = fetch_openml('credit-g', version='active')
236/50: print(credit_data)
236/51: print(credit_data.data.shape)
236/52: print(credit_data.target.shape)
236/53: print(credit_data.target.shape)
236/54: df = pd.DataFrame(data=credit_data.data, columns=credit_data.feature_names)
236/55: print(df.dtypes)
236/56: credit_data.categories
236/57: credit_data.feature_names
236/58:
credit_dataframe = pd.DataFrame(credit_data.data, columns=credit_data.feature_names)
credit_dataframe['class'] = credit_data.target
236/59:
#Univariate Distribution of continuous variables


plt.subplot(14, 1, 1)
df['duration'].hist(bins=10,range=(20,50),figsize =(12, 20))
plt.xlabel('Duration')

plt.subplot(14, 1, 2)
df['credit_amount'].hist(bins=30,range=(200,500),figsize =(12, 20))
plt.xlabel('Credit Amount')

plt.subplot(14, 1, 3)
df['installment_commitment'].hist(bins=30,range=(1,10),figsize =(12, 20))
plt.xlabel('installment_commitment')

plt.subplot(14, 1, 4)
df['residence_since'].hist(bins=30,range=(1,10),figsize =(12, 20))
plt.xlabel('residence_since')

plt.subplot(14, 1, 5)
df['age'].hist(bins=30,range=(1,100),figsize =(12, 20))
plt.xlabel('age')

plt.subplot(14, 1, 6)
df['existing_credits'].hist(bins=30,range=(1,5),figsize =(12, 20))
plt.xlabel('existing_credits')

plt.subplot(14, 1, 7)
df['num_dependents'].hist(bins=30,range=(1,5),figsize =(12, 20))
plt.xlabel('num_dependents')

plt.subplots_adjust(top=2, bottom=0.01)
plt.show()
236/60:
#PLOTTING THE TARGET

cg=0
cb=0
for i in range(len(credit_data.target)):
    if credit_data.target[i]=='good':
        cg=cg+1
    else:
        cb=cb+1
labels=['Good','Bad']
values=[cg,cb]
fig = plt.figure(figsize = (6, 8))
plt.bar(labels, values, color ='purple',
        width = 0.2)
 

plt.title("Target")
plt.show()
236/61:
#PREPROCESSING THE DATA

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder()

for i in range(len(categorical_columns)):
    df[categorical_columns[i]]= label_encoder.fit_transform(df[categorical_columns[i]]) 
    
print(df.head())

#SPLITTING THE DATA 
#LOGISTIC REGRESSION

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler  
scaler = StandardScaler()
cols = credit_data.feature_names
X = df[cols]
y = credit_data.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42) 

X_train_std =scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)

logistic_model = LogisticRegression()

logistic_model.fit(X_train_std, y_train)

y_pred = logistic_model.predict(X_test_std)
236/62:
#PREPROCESSING THE DATA

credit_dataX, credit_datay = fetch_openml('credit-g', version='active', as_frame=True, return_X_y =True)
categorical_features = ['checking_status', 'credit_history', 'employment', 'foreign_worker',
'housing',
'job',
'other_parties',
'other_payment_plans',
'own_telephone',
'personal_status',
'property_magnitude',
'purpose',
'savings_status']
credit_dataX_ordinal = credit_dataX.copy()
for feature in credit_dataX.iteritems():
  if feature[0] in categorical_features:
    credit_dataX_ordinal[feature[0]] = credit_dataX[feature[0]].astype("category").cat.codes
236/63: credit_dataX_ordinal.head()
236/64:
#Label Encoding
le = LabelEncoder()
y = le.fit_transform(credit_datay)
236/65:
#train - test split
X_trainval, X_test, y_trainval, y_test = train_test_split(credit_dataX_ordinal, y)

# creating the training validation split
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval)
236/66: numeric_features = [feature for feature in credit_data.feature_names if feature not in categorical_features]
236/67:
scaler = StandardScaler()
X_train_scaled = X_train
X_train_scaled[numeric_features] = scaler.fit_transform(X_train_scaled[numeric_features])

X_val_scaled = X_val
X_val_scaled[numeric_features] = scaler.fit_transform(X_val_scaled[numeric_features])

logistic_regression = LogisticRegression().fit(X_train, y_train)
logistic_regression.predict(X_val)
logistic_regression.score(X_val, y_val)
236/68:
#new train - test split
X_trainval_pipe, X_test_pipe, y_trainval_pipe, y_test_pipe = train_test_split(credit_dataX, credit_datay)

# creating a Repeated Stratified K fold
skf = StratifiedKFold(n_splits=5, shuffle=True)
236/69:
preprocess_lr = make_column_transformer(
    (OneHotEncoder(), categorical_features))
model_lr = make_pipeline(preprocess_lr, LogisticRegression())
model_lr.fit(X_trainval_pipe, y_trainval_pipe)
scores_lr = cross_val_score(model_lr, X_trainval_pipe, y_trainval_pipe, cv=skf, scoring='accuracy')
print("score: {:.3f}".format(np.mean(scores_lr)))
236/70:
#Linear Regression without scaling

preprocess_lr = make_column_transformer(
    (OneHotEncoder(), categorical_features))
model_lr = make_pipeline(preprocess_lr, LogisticRegression())
model_lr.fit(X_trainval_pipe, y_trainval_pipe)
scores_lr = cross_val_score(model_lr, X_trainval_pipe, y_trainval_pipe, cv=skf, scoring='accuracy')
print("score: {:.3f}".format(np.mean(scores_lr)))
236/71:
#Linear Regression with scaling

preprocess_lrs = make_column_transformer(
    (StandardScaler(), numeric_features),
    (OneHotEncoder(), categorical_features))
model_lrs = make_pipeline(preprocess_lrs, LogisticRegression())
model_lrs.fit(X_trainval_pipe, y_trainval_pipe)
scores_lrs = cross_val_score(model_lrs, X_trainval_pipe, y_trainval_pipe, cv=skf, scoring='accuracy')
print("score: {:.3f}".format(np.mean(scores_lrs)))
236/72:
#SVC without scaling

preprocess_svc = make_column_transformer(
    (OneHotEncoder(), categorical_features))
model_svc = make_pipeline(preprocess_svc, LinearSVC())
model_svc.fit(X_trainval_pipe, y_trainval_pipe)
scores_svc = cross_val_score(model_svc, X_trainval_pipe, y_trainval_pipe, cv=skf, scoring='accuracy')
print("score: {:.3f}".format(np.mean(scores_svc)))
236/73:
#SVC with scaling

preprocess_svcs = make_column_transformer(
    (StandardScaler(), numeric_features),
    (OneHotEncoder(), categorical_features))
model_svcs = make_pipeline(preprocess_svcs, LinearSVC( max_iter=5000))
model_svcs.fit(X_trainval_pipe, y_trainval_pipe)
scores_svcs = cross_val_score(model_svcs, X_trainval_pipe, y_trainval_pipe, cv=skf, scoring='accuracy')
print("score: {:.3f}".format(np.mean(scores_svcs)))
236/74:
#Nearest Neighbors without scaling

preprocess_knn = make_column_transformer(
    (OneHotEncoder(), categorical_features))
model_knn = make_pipeline(preprocess_knn, KNeighborsClassifier())
model_knn.fit(X_trainval_pipe, y_trainval_pipe)
scores_knn = cross_val_score(model_knn, X_trainval_pipe, y_trainval_pipe, cv=skf, scoring='accuracy')
print("score: {:.3f}".format(np.mean(scores_knn)))
236/75:
#Nearest Neigbors with scaling

preprocess_knns = make_column_transformer(
    (StandardScaler(), numeric_features),
    (OneHotEncoder(), categorical_features))
model_knns = make_pipeline(preprocess_knns, KNeighborsClassifier())
model_knns.fit(X_trainval_pipe, y_trainval_pipe)
scores_knns = cross_val_score(model_knns, X_trainval_pipe, y_trainval_pipe, cv=skf, scoring='accuracy')
print("score: {:.3f}".format(np.mean(scores_knns)))
237/1: import pandas as pd
237/2: df = pd. read_pickle('train100c5k_v2.pkl')
237/3:
objects = []
with (open("train100c5k_v2.pkl')", "rb")) as openfile:
    while True:
        try:
            objects.append(pickle.load(openfile))
        except EOFError:
            break
237/4:
import pandas as pd
import pickle
237/5: df = pd.read_pickle("train100c5k_v2.pkl")
237/6: df = pd.read_pickle("test100c5k_nolabel.pkl")
237/7: df.head()
237/8: df = pd.read_pickle("train100c5k_v2.pkl")
237/9: df.head()
237/10: df.info()
237/11: df.describe
237/12: df.dtype
237/13: df.dtype()
237/14: df.dtypes()
237/15: df.dtypes
237/16: df.shape()
237/17: df.shape
238/1:
import pandas as pd
import pickle
from packaging import version
import sklearn

assert version.parse(sklearn.__version__) >= version.parse("1.0.1")
238/2:
import pandas as pd
import pickle
import sklearn
238/3: df = pd.read_pickle("train100c5k_v2.pkl")
238/4: df.head()
238/5:
X, y = df.data, df.target
X
238/6: X.shape
238/7: y
238/8: y.shape
238/9: 28 * 28
238/10:
import matplotlib.pyplot as plt

def plot_digit(image_data):
    image = image_data.reshape(28, 28)
    plt.imshow(image, cmap="binary")
    plt.axis("off")

some_digit = X[0]
plot_digit(some_digit)
save_fig("something")  # extra code
plt.show()
238/11: y[0]
238/12:
plt.figure(figsize=(9, 9))
for idx, image_data in enumerate(X[:100]):
    plt.subplot(10, 10, idx + 1)
    plot_digit(image_data)
plt.subplots_adjust(wspace=0, hspace=0)
save_fig("more_something", tight_layout=False)
plt.show()
238/13:
y_train_5 = (y_train == '5')  # True for all 5s, False for all other digits
y_test_5 = (y_test == '5')
238/14: X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
238/15:
y_train_5 = (y_train == '5')  # True for all 5s, False for all other digits
y_test_5 = (y_test == '5')
238/16:
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train_5)
238/17: df.tail()
238/18:
_, axes = plt.subplots(nrows=1, ncols=10, figsize=(16, 4))
for ax, image, label in zip(axes, df.data, df.target):
    ax.set_axis_off()
    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    ax.set_title('Training: %i' % label)
238/19: df.data
238/20: df.target
238/21:
import pandas as pd

n_samples = len(df.data)
data = df.data.reshape((n_samples, -1))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
238/22:
import pandas as pd

n_samples = len(df.data)
data = df.data.reshape((n_samples, 1))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
238/23:
import pandas as pd

n_samples = len(df.data)
data = df.data.reshape((n_samples, 1))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
238/24:
import pandas as pd

n_samples = len(df.data)
data = df.data.reshape((n_samples, 0))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
238/25:
import pandas as pd

n_samples = len(df.data)
data = df.data.reshape(-1,1)

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
238/26:
import pandas as pd

n_samples = len(df.data)
data = df.data.reshape((-1,1))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
238/27:
import pandas as pd

n_samples = len(df.data)
d = df.data.reshape((n_samples,-1))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(d)
238/28:
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df.data, df.target, test_size=0.5, shuffle=False)
238/29:
# Support vector classifier
from sklearn.svm import SVC

svm = SVC(gamma=0.001, random_state=42)
# Learn the digits on the train subset
svm.fit(X_train, y_train)
238/30:
# Support vector classifier
from sklearn.svm import SVC

svm = SVC(gamma=0.001, random_state=42, dtype=object)
# Learn the digits on the train subset
svm.fit(X_train, y_train)
238/31:
# Support vector classifier
from sklearn.svm import SVC

data_type=object

svm = SVC(gamma=0.001, random_state=42, dtype=data_type)
# Learn the digits on the train subset
svm.fit(X_train, y_train)
238/32:
# Support vector classifier
from sklearn.svm import SVC

data_type=none

svm = SVC(gamma=0.001, random_state=42, dtype=data_type)
# Learn the digits on the train subset
svm.fit(X_train, y_train)
238/33:
# Support vector classifier
from sklearn.svm import SVC

data_type=None

svm = SVC(gamma=0.001, random_state=42, dtype=data_type)
# Learn the digits on the train subset
svm.fit(X_train, y_train)
238/34:
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=500, random_state=42)
rf.fit(X_train, y_train)
238/35:
import pandas as pd
import pickle
import sklearn
import numpy as np
238/36:
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=500, random_state=42)
rf.fit(X_train, y_train)
238/37:
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=500, random_state=42)
rf.fitlist((X_train), y_train)
238/38:
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=500, random_state=42)
rf.fit(list(X_train), y_train)
238/39:
nsamples, nx, ny = df.shape
d2_train_dataset = df.reshape((nsamples,nx*ny))
238/40:
nsamples, df.data, df.target = df.shape
d2_train_dataset = df.reshape((nsamples,df.data*df.target))
238/41:
import pandas as pd

n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
238/42:
import pandas as pd

n_samples = len(df.data)
data = df.data.reshape((n_samples, -1))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
238/43:
import pandas as pd

n_samples = len(df.data)
data = df.data.values.reshape((n_samples, -1))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
238/44:
# Dimension of original image data
df.data.shape
238/45: data.shape
238/46: pd.DataFrame(df.target)
238/47:
# Split data into 50% train and 50% test subsets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    data, df.target, test_size=0.5, shuffle=False)
238/48:
# Support vector classifier
from sklearn.svm import SVC

svm = SVC(gamma=0.001, random_state=42)
# Learn the digits on the train subset
svm.fit(X_train, y_train)
238/49:
# Support vector classifier
from sklearn.svm import SVC

svm = SVC(gamma=0.001, random_state=42)
# Learn the digits on the train subset
svm.fit(X_train, y_train)
print(X_train)
print(y_train)
238/50:
import numpy as np
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

X = data
y = df.target
clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X, y)
238/51: df.target.shape
238/52:
# Support vector classifier
from sklearn.svm import SVC

svm = SVC(gamma=auto, random_state=42)
# Learn the digits on the train subset
svm.fit(X_train, y_train)
print(X_train)
print(y_train)
238/53:
clf = svm.SVC(C=1.0, cache_size=200,kernel='linear', max_iter=-1)
clf.fit(list(X_train),Y_train)
238/54:
from sklearn.svm import SVC
clf = svm.SVC(C=1.0, cache_size=200,kernel='linear', max_iter=-1)
clf.fit(list(X_train),Y_train)
238/55:
from sklearn.svm import SVC
clf = svm.svc(C=1.0, cache_size=200,kernel='linear', max_iter=-1)
clf.fit(list(X_train),Y_train)
239/1:
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import to_categorical
from keras.preprocessing import image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from tqdm import tqdm
239/2: !pip install tenserflow
239/3:
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import to_categorical
from keras.preprocessing import image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from tqdm import tqdm
241/1:
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import to_categorical
from keras.preprocessing import image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from tqdm import tqdm
241/2:
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import to_categorical
from keras.preprocessing import image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from tqdm import tqdm
241/3: train = pd.read_csv('train100c5k_v2.pkl')
241/4: train = pd.read_pickle('train100c5k_v2.pkl')
241/5:
# We have grayscale images, so while loading the images we will keep grayscale=True, if you have RGB images, you should set grayscale as False
train_image = []
for i in tqdm(range(train.shape[0])):
    img = image.load_img('train/'+train['id'][i].astype('str')+'.png', target_size=(28,28,1), grayscale=True)
    img = image.img_to_array(img)
    img = img/255
    train_image.append(img)
X = np.array(train_image)
241/6:
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import to_categorical
from tf.keras.utils.load_img import image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from tqdm import tqdm
241/7:
import keras
import tensorflow 
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import to_categorical
from tensorflow.keras.utils.load_img import image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from tqdm import tqdm
242/1:
import pandas as pd

n_samples = len(df.data)
data = df.data.values.reshape((n_samples, -1))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
242/2:
import pandas as pd
import pickle
import sklearn
import numpy as np
242/3: df = pd.read_pickle("train100c5k_v2.pkl")
242/4: df.head()
242/5:
import pandas as pd

n_samples = len(df.data)
data = df.data.values.reshape((n_samples, -1))

# Displaying the contents of the processsed X data matrix as a DataFrame
pd.DataFrame(data)
242/6:
# Split data into 50% train and 50% test subsets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    data, df.target, test_size=0.5, shuffle=False)
242/7:
# Support vector classifier
from sklearn.svm import SVC

svm = SVC(gamma=0.001, random_state=42)
# Learn the digits on the train subset
svm.fit(X_train, y_train)
242/8:
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train)
242/9:
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fitlist((X_train), y_train)
242/10:
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(list(X_train), y_train)
242/11:
# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before
# recall Xtr_rows is 50,000 x 3072 matrix
Xval_rows = Xtr_rows[:1000, :] # take first 1000 for validation
Yval = Ytr[:1000]
Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for train
Ytr = Ytr[1000:]

# find hyperparameters that work best on the validation set
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:

  # use a particular value of k and evaluation on validation data
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  # here we assume a modified NearestNeighbor class that can take a k as input
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
  print 'accuracy: %f' % (acc,)
242/12:
# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before
# recall Xtr_rows is 50,000 x 3072 matrix
Xval_rows = Xtr_rows[:1000, :] # take first 1000 for validation
Yval = Ytr[:1000]
Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for train
Ytr = Ytr[1000:]

# find hyperparameters that work best on the validation set
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:

  # use a particular value of k and evaluation on validation data
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  # here we assume a modified NearestNeighbor class that can take a k as input
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
242/13:
# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before
# recall Xtr_rows is 50,000 x 3072 matrix
Xval_rows = data_rows[:1000, :] # take first 1000 for validation
Yval = df.target[:1000]
Xtr_rows = data_rows[1000:, :] # keep last 49,000 for train
Ytr = df.target[1000:]

# find hyperparameters that work best on the validation set
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:

  # use a particular value of k and evaluation on validation data
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  # here we assume a modified NearestNeighbor class that can take a k as input
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
242/14:
# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before
# recall Xtr_rows is 50,000 x 3072 matrix
Xval_rows = df.data_rows[:1000, :] # take first 1000 for validation
Yval = df.target[:1000]
Xtr_rows = data_rows[1000:, :] # keep last 49,000 for train
Ytr = df.target[1000:]

# find hyperparameters that work best on the validation set
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:

  # use a particular value of k and evaluation on validation data
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  # here we assume a modified NearestNeighbor class that can take a k as input
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
242/15:
from sklearn import KNearestNeighbor

# Create a kNN classifier instance. 
# Remember that training a kNN classifier is a noop: 
# the Classifier simply remembers the data and does no further processing 
classifier = KNearestNeighbor()
classifier.train(X_train, y_train)
242/16:
from sklearn.neighbors import KNearestNeighbor

# Create a kNN classifier instance. 
# Remember that training a kNN classifier is a noop: 
# the Classifier simply remembers the data and does no further processing 
classifier = KNearestNeighbor()
classifier.train(X_train, y_train)
242/17:
from sklearn.neighbors import NearestNeighbors

# Create a kNN classifier instance. 
# Remember that training a kNN classifier is a noop: 
# the Classifier simply remembers the data and does no further processing 
classifier = KNearestNeighbor()
classifier.train(X_train, y_train)
242/18:
from sklearn.neighbors import NearestNeighbors

# Create a kNN classifier instance. 
# Remember that training a kNN classifier is a noop: 
# the Classifier simply remembers the data and does no further processing 
classifier = NearestNeighbors()
classifier.train(X_train, y_train)
242/19:
from sklearn.neighbors import NearestNeighbors

# Create a kNN classifier instance. 
# Remember that training a kNN classifier is a noop: 
# the Classifier simply remembers the data and does no further processing 
classifier = NearestNeighbors()
classifier.train(X,y)
242/20:
from sklearn.neighbors import NearestNeighbors

# Create a kNN classifier instance. 
# Remember that training a kNN classifier is a noop: 
# the Classifier simply remembers the data and does no further processing 
classifier = NearestNeighbors()
classifier.df(X_train,y_train)
242/21:
def dist(a1,a2):
    return np.sum((a1-a2)**2)**.5
242/22:
def KNN(X,Y,test_point,k=5):
    tt =cv2.imread("Train/Images/" + X[0])
    t2 = cv2.resize(tt,(100,100))
    m=t2.shape[0]
    vals = []
    for i in range(300):
        con = cv2.imread("Train/Images/" + X[i])
        con1 = cv2.cvtColor(con, cv2.COLOR_BGR2RGB)
        con2 = cv2.resize(con1,(100,100))
        d = dist(con2,test_point)
        vals.append((d,Y[i]))
    
    vals = sorted(vals)
    vals = vals[:k]
    
    return vals
242/23:
plt.figure()
i=0
for i in range(4):
    s1 = cv2.imread('Train/Images/' + X[i])
    s2 = cv2.cvtColor(s1, cv2.COLOR_BGR2RGB)
    c1 = cv2.resize(s2,(100,100))
    plt.subplot(2,5,i+1)
    plt.imshow(c1)
    plt.title("label: " + str(Y[i]))
    plt.axis('off')
    plt.show()
242/24:
import matplotlib.pyplot as plt

plt.figure()
i=0
for i in range(4):
    s1 = cv2.imread('Train/Images/' + X[i])
    s2 = cv2.cvtColor(s1, cv2.COLOR_BGR2RGB)
    c1 = cv2.resize(s2,(100,100))
    plt.subplot(2,5,i+1)
    plt.imshow(c1)
    plt.title("label: " + str(Y[i]))
    plt.axis('off')
    plt.show()
242/25:
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train)
242/26:
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fitlist((X_train), y_train)
242/27:
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(list(X_train), y_train)
242/28:
from sklearn.linear_model import SGDClassifier

X = data
y = df.target

clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X, y)
242/29:
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
X = data
y = df.target

clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X, y)
242/30:
import numpy as np
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
X = np.array([data])
y = np.array([df.target])
clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X, y)
242/31:
import numpy as np
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
X = np.array([df.data])
y = np.array([df.target])
clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X, y)
244/1:
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
244/2:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
244/3:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
244/4:

test.head()
244/5: test.tail()
244/6: train.iloc[:,0].value_counts()
246/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
246/2:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
246/3:

test.head()
246/4: test.tail()
246/5: train.iloc[:,0].value_counts()
247/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
247/2:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
247/3:
train_x = train.iloc[:,1:785]
train_y = train.iloc[:,0]
247/4:
print(train_x.shape)
print(train_y.shape)
247/5:
from sklearn.cross_validation import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
247/6:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
247/7:
print(x_train.shape)
print(x_cv.shape)
print(y_train.shape)
print(y_cv.shape)
247/8:
x_train = np.asmatrix(x_train).reshape(45000, 784)
x_cv = np.asmatrix(x_cv).reshape(15000, 784)
247/9:
x_train = np.asmatrix(x_train).reshape(375000, 1)
x_cv = np.asmatrix(x_cv).reshape(125000, 1)
247/10:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
247/11:
x_train = x_train/255
x_cv = x_cv/255
247/12:
import keras

digits = 10
y_train = keras.utils.to_categorical(y_train, digits)
y_cv = keras.utils.to_categorical(y_cv, digits)
247/13:
import keras 
from keras.layers import Dense 
from keras.models import Sequential
from keras import optimizers
247/14:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(output_dim = 400, init = 'uniform', activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# third hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# fourth hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# fifth hidden layer
model.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
model.add(Dense(output_dim = 10, init = 'uniform', activation = 'softmax'))
247/15:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(400, init = 'uniform', activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# third hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# fourth hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# fifth hidden layer
model.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
model.add(Dense(output_dim = 10, init = 'uniform', activation = 'softmax'))
247/16:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(400, init = 'uniform', activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(300, init = 'uniform', activation = 'relu'))

# third hidden layer
model.add(Dense(300, init = 'uniform', activation = 'relu'))

# fourth hidden layer
model.add(Dense(300, init = 'uniform', activation = 'relu'))

# fifth hidden layer
model.add(Dense(100, init = 'uniform', activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
model.add(Dense(10, init = 'uniform', activation = 'softmax'))
247/17:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(400, activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(300, activation = 'relu'))

# third hidden layer
model.add(Dense(300, activation = 'relu'))

# fourth hidden layer
model.add(Dense(300, activation = 'relu'))

# fifth hidden layer
model.add(Dense(100, activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
model.add(Dense(10, activation = 'softmax'))
247/18:

model.summary()
247/19:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)


# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 60, verbose = 2, validation_data = (x_cv, y_cv))
247/20:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)

x_train = np.asarray(x_train).astype(np.float32)
y_train = np.asarray(y_train).astype(np.float32)
# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 60, verbose = 2, validation_data = (x_cv, y_cv))
247/21:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)

x_train = np.asarray(x_train).astype(np.float32)
y_train = np.asarray(y_train).astype(np.object)
# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 60, verbose = 2, validation_data = (x_cv, y_cv))
247/22:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)

x_train = np.asarray(x_train).astype(np.float32)
y_train = np.asarray(y_train).astype(np.string)
# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 60, verbose = 2, validation_data = (x_cv, y_cv))
247/23:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)

x_train = np.asarray(x_train).astype(np.float32)
y_train = np.asarray(y_train).astype(np.str)
# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 60, verbose = 2, validation_data = (x_cv, y_cv))
248/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
248/2:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
248/3:
train_x = train.iloc[:,1:785]
train_y = train.iloc[:,0]
248/4:
print(train_x.shape)
print(train_y.shape)
248/5:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
248/6:
print(x_train.shape)
print(x_cv.shape)
print(y_train.shape)
print(y_cv.shape)
248/7:
x_train = np.asmatrix(x_train).reshape(375000, 1)
x_cv = np.asmatrix(x_cv).reshape(125000, 1)
248/8:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
248/9:
x_train = x_train/255
x_cv = x_cv/255
248/10:
import keras 
from keras.layers import Dense 
from keras.models import Sequential
from keras import optimizers
248/11:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(400, activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(300, activation = 'relu'))

# third hidden layer
model.add(Dense(300, activation = 'relu'))

# fourth hidden layer
model.add(Dense(300, activation = 'relu'))

# fifth hidden layer
model.add(Dense(100, activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
model.add(Dense(10, activation = 'softmax'))
248/12:

model.summary()
248/13:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)

x_train = np.asarray(x_train).astype(np.float32)
y_train = np.asarray(y_train).astype(np.str_)
# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 12, verbose = 2, validation_data = (x_cv, y_cv))
249/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
249/2:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
249/3:
train_x = train.iloc[:,1:785]
train_y = train.iloc[:,0]
249/4:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
249/5:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
249/6:
x_train = np.asmatrix(x_train).reshape(375000, 1)
x_cv = np.asmatrix(x_cv).reshape(125000, 1)
249/7:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
249/8:
x_train = x_train/255
x_cv = x_cv/255
249/9:
import keras 
from keras.layers import Dense 
from keras.models import Sequential
from keras import optimizers
249/10:
from sklearn.ensemble import RandomForestClassifier

# creating model
model = RandomForestClassifier()

# feeding the training data into the model
model.fit(x_train, y_train)

# calculating the accuracy
print("Training accuracy :", model.score(x_train, y_train))
249/11:
x_train = np.asarray(x_train).astype(np.float32)
y_train = np.asarray(y_train).astype(np.str)
250/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
250/2:
train_x = train.iloc[:,1:785]
train_y = train.iloc[:,0]
250/3:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
250/4:
train_x = train.iloc[:,1:785]
train_y = train.iloc[:,0]
250/5:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
250/6:
x_train = np.asmatrix(x_train).reshape(375000, 1)
x_cv = np.asmatrix(x_cv).reshape(125000, 1)
250/7:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
250/8:
x_train = x_train/255
x_cv = x_cv/255
250/9:
import keras 
from keras.layers import Dense 
from keras.models import Sequential
from keras import optimizers
250/10:
x_train = np.asarray(x_train)
y_train = np.asarray(y_train)
250/11:
from sklearn.ensemble import RandomForestClassifier

# creating model
model = RandomForestClassifier()

# feeding the training data into the model
model.fit(x_train, y_train)

# calculating the accuracy
print("Training accuracy :", model.score(x_train, y_train))
250/12:
x_train = np.asarray(x_train).astype(np.str)
y_train = np.asarray(y_train).astype(np.str)
251/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
251/2:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
251/3:

test.head()
251/4:
train_x = train.iloc[:,0]
train_y = train.iloc[:,0]
251/5:
print(train_x.shape)
print(train_y.shape)
251/6:
train_x = train.iloc[:,2]
train_y = train.iloc[:,2]
251/7:
train_x = train.iloc[:,1]
train_y = train.iloc[:,1]
251/8:
print(train_x.shape)
print(train_y.shape)
251/9:
train_x = train.iloc[2000:,1]
train_y = train.iloc[2000:,1]
251/10:
print(train_x.shape)
print(train_y.shape)
251/11:
train_x = train.iloc[20000:,1]
train_y = train.iloc[20000:,1]
251/12:
print(train_x.shape)
print(train_y.shape)
251/13:
train_x = train.iloc[50000:,1]
train_y = train.iloc[50000:,1]
251/14:
print(train_x.shape)
print(train_y.shape)
251/15:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
251/16:
x_train = np.asmatrix(x_train).reshape(375000, 1)
x_cv = np.asmatrix(x_cv).reshape(125000, 1)
251/17:
print(x_train.shape)
print(x_cv.shape)
print(y_train.shape)
print(y_cv.shape)
251/18:
x_train = np.asmatrix(x_train).reshape(337500, 1)
x_cv = np.asmatrix(x_cv).reshape(112500, 1)
251/19:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
251/20:
x_train = x_train/255
x_cv = x_cv/255
251/21:
x_train = np.asarray(x_train).astype(np.str_)
y_train = np.asarray(y_train).astype(np.str_)
251/22:
from sklearn.ensemble import RandomForestClassifier

# creating model
model = RandomForestClassifier()

# feeding the training data into the model
model.fit(x_train, y_train)

# calculating the accuracy
print("Training accuracy :", model.score(x_train, y_train))
251/23:
test_x = test.iloc[50000:,1]
test_y = test.iloc[50000:,1]
251/24:
test_x = test.iloc[50000:,0]
test_y = test.iloc[50000:,0]
251/25:
print(test_x.shape)
print(test_y.shape)
251/26: test_x = np.asmatrix(test_x).reshape(10000, 784)
251/27: test_x = np.asmatrix(test_x).reshape(50000, )
251/28: test_x = test_x.astype('float32')
251/29: test_x = test_x.astype('str')
252/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
252/2:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
252/3:

test.head()
252/4:
train_x = train.iloc[50000:,1]
train_y = train.iloc[50000:,1]
252/5:
print(train_x.shape)
print(train_y.shape)
252/6:
test_x = test.iloc[10000:,0]
test_y = test.iloc[10000:,0]
252/7:
print(test_x.shape)
print(test_y.shape)
252/8:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
252/9:
print(x_train.shape)
print(x_cv.shape)
print(y_train.shape)
print(y_cv.shape)
252/10: test_x = np.asmatrix(test_x).reshape(10000, )
252/11: test_x = np.asmatrix(test_x).reshape(90000, )
252/12:
x_train = np.asmatrix(x_train).reshape(337500, 1)
x_cv = np.asmatrix(x_cv).reshape(112500, 1)
252/13:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
252/14: test_x = test_x.astype('float32')
252/15: test_x = test_x.astype('str_')
252/16:
test_x = test.iloc[50000:,0]
test_y = test.iloc[50000:,0]
252/17:
print(test_x.shape)
print(test_y.shape)
252/18:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
252/19:
print(x_train.shape)
print(x_cv.shape)
print(y_train.shape)
print(y_cv.shape)
252/20: test_x = np.asmatrix(test_x).reshape(50000, )
252/21:
x_train = np.asmatrix(x_train).reshape(337500, 1)
x_cv = np.asmatrix(x_cv).reshape(112500, 1)
252/22:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
252/23: test_x = test_x.astype('float32')
252/24: test_x = test_x.astype('str')
252/25: test_x = np.asarray(test_x).astype(str_)
252/26: test_x = np.asarray(test_x).astype('str_')
252/27: test_x = test_x.astype('float32')
252/28: test_x = test_x.astype('object')
252/29:
x_train = x_train/255
x_cv = x_cv/255
252/30: test_x = test_x/255
252/31:
x_train = np.asarray(x_train).astype(np.str_)
y_train = np.asarray(y_train).astype(np.str_)
252/32:
from sklearn.ensemble import RandomForestClassifier

# creating model
model = RandomForestClassifier()

# feeding the training data into the model
model.fit(x_train, y_train)

# calculating the accuracy
print("Training accuracy :", model.score(x_train, y_train))
print("Testing accuracy :", model.score(x_cv, y_cv))
252/33:
from sklearn.neural_network import MLPClassifier

# creating the model
model = MLPClassifier()

# feeding the training set to the modeel
model.fit(x_train, y_train)

# predicting the results for test set
y_pred = model.predict(test_x)

# calculating the accuracies
print("Training accuracy :", model.score(x_train, y_train))
print("Testing accuracy :", model.score(x_cv, y_cv))
252/34:
import keras

digits = 10
y_train = keras.utils.to_categorical(y_train, digits)
y_cv = keras.utils.to_categorical(y_cv, digits)
252/35:
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train)
252/36:
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(x_train, y_train)
252/37: sgd_clf.predict([some_digit])
252/38: sgd_clf.predict([data])
252/39:
from sklearn.model_selection import cross_val_score

cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")
252/40:
from sklearn.model_selection import cross_val_score

cross_val_score(sgd_clf, x_train, y_train, cv=3, scoring="accuracy")
253/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
253/2:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
253/3:
train_x = train.iloc[45000:,1]
train_y = train.iloc[45000:,1]
253/4:
print(train_x.shape)
print(train_y.shape)
253/5:
train_x = train.iloc[100000:,1]
train_y = train.iloc[100000:,1]
253/6:
print(train_x.shape)
print(train_y.shape)
253/7:
test_x = test.iloc[100000:,0]
test_y = test.iloc[100000:,0]
253/8:
print(test_x.shape)
print(test_y.shape)
253/9:
test_x = test.iloc[50000:,0]
test_y = test.iloc[50000:,0]
253/10:
print(test_x.shape)
print(test_y.shape)
253/11:
import keras 
from keras.layers import Dense 
from keras.models import Sequential
from keras import optimizers
253/12:
model = Sequential()                                 # Linear stacking of layers

# Convolution Layer 1
model.add(Conv2D(32, (3, 3), input_shape=(28,28,1))) # 32 different 3x3 kernels -- so 32 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer01 = Activation('relu')                     # activation
model.add(convLayer01)

# Convolution Layer 2
model.add(Conv2D(32, (3, 3)))                        # 32 different 3x3 kernels -- so 32 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model.add(Activation('relu'))                        # activation
convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model.add(convLayer02)

# Convolution Layer 3
model.add(Conv2D(64,(3, 3)))                         # 64 different 3x3 kernels -- so 64 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer03 = Activation('relu')                     # activation
model.add(convLayer03)

# Convolution Layer 4
model.add(Conv2D(64, (3, 3)))                        # 64 different 3x3 kernels -- so 64 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model.add(Activation('relu'))                        # activation
convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model.add(convLayer04)
model.add(Flatten())                                 # Flatten final 4x4x64 output matrix into a 1024-length vector

# Fully Connected Layer 5
model.add(Dense(512))                                # 512 FCN nodes
model.add(BatchNormalization())                      # normalization
model.add(Activation('relu'))                        # activation

# Fully Connected Layer 6                       
model.add(Dropout(0.2))                              # 20% dropout of randomly selected nodes
model.add(Dense(10))                                 # final 10 FCN nodes
model.add(Activation('softmax'))                     # softmax activation
253/13:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
253/14:
print(x_train.shape)
print(x_cv.shape)
print(y_train.shape)
print(y_cv.shape)
253/15: test_x = np.asmatrix(test_x).reshape(50000, )
253/16:
x_train = np.asmatrix(x_train).reshape(300000, 1)
x_cv = np.asmatrix(x_cv).reshape(100000, 1)
253/17:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
253/18: test_x = test_x.astype('float32')
253/19: test_x = test_x.astype('object')
253/20:
x_train = x_train/255
x_cv = x_cv/255
253/21: test_x = test_x/255
253/22:
model = Sequential()                                 # Linear stacking of layers

# Convolution Layer 1
model.add(Conv2D(32, (3, 3), input_shape=(28,28,1))) # 32 different 3x3 kernels -- so 32 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer01 = Activation('relu')                     # activation
model.add(convLayer01)

# Convolution Layer 2
model.add(Conv2D(32, (3, 3)))                        # 32 different 3x3 kernels -- so 32 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model.add(Activation('relu'))                        # activation
convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model.add(convLayer02)

# Convolution Layer 3
model.add(Conv2D(64,(3, 3)))                         # 64 different 3x3 kernels -- so 64 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer03 = Activation('relu')                     # activation
model.add(convLayer03)

# Convolution Layer 4
model.add(Conv2D(64, (3, 3)))                        # 64 different 3x3 kernels -- so 64 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model.add(Activation('relu'))                        # activation
convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model.add(convLayer04)
model.add(Flatten())                                 # Flatten final 4x4x64 output matrix into a 1024-length vector

# Fully Connected Layer 5
model.add(Dense(512))                                # 512 FCN nodes
model.add(BatchNormalization())                      # normalization
model.add(Activation('relu'))                        # activation

# Fully Connected Layer 6                       
model.add(Dropout(0.2))                              # 20% dropout of randomly selected nodes
model.add(Dense(10))                                 # final 10 FCN nodes
model.add(Activation('softmax'))                     # softmax activation
253/23:
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten
from tensorflow.keras.layers import BatchNormalization
from keras.models import Sequential  # Model type to be used
from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model
from keras.utils import np_utils
import warnings
warnings.filterwarnings('ignore')
253/24:
model = Sequential()                                 # Linear stacking of layers

# Convolution Layer 1
model.add(Conv2D(32, (3, 3), input_shape=(28,28,1))) # 32 different 3x3 kernels -- so 32 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer01 = Activation('relu')                     # activation
model.add(convLayer01)

# Convolution Layer 2
model.add(Conv2D(32, (3, 3)))                        # 32 different 3x3 kernels -- so 32 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model.add(Activation('relu'))                        # activation
convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model.add(convLayer02)

# Convolution Layer 3
model.add(Conv2D(64,(3, 3)))                         # 64 different 3x3 kernels -- so 64 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer03 = Activation('relu')                     # activation
model.add(convLayer03)

# Convolution Layer 4
model.add(Conv2D(64, (3, 3)))                        # 64 different 3x3 kernels -- so 64 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model.add(Activation('relu'))                        # activation
convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model.add(convLayer04)
model.add(Flatten())                                 # Flatten final 4x4x64 output matrix into a 1024-length vector

# Fully Connected Layer 5
model.add(Dense(512))                                # 512 FCN nodes
model.add(BatchNormalization())                      # normalization
model.add(Activation('relu'))                        # activation

# Fully Connected Layer 6                       
model.add(Dropout(0.2))                              # 20% dropout of randomly selected nodes
model.add(Dense(10))                                 # final 10 FCN nodes
model.add(Activation('softmax'))                     # softmax activation
253/25: model.summary()
253/26: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
253/27:
model.fit_generator(train_generator, steps_per_epoch=X_train.shape[0] // batch_size, epochs = epochs, verbose=1, 
                    validation_data = (X_val,Y_val), validation_steps=X_val.shape[0] // batch_size)
253/28:
gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,
                         height_shift_range=0.08, zoom_range=0.08)
253/29:
model.fit_generator(train_generator, steps_per_epoch=X_train.shape[0] // batch_size, epochs = epochs, verbose=1, 
                    validation_data = (X_val,Y_val), validation_steps=X_val.shape[0] // batch_size)
253/30: train_generator = gen.flow(X_train, Y_train, batch_size=64)
253/31: train_generator = gen.flow(x_train, Y_train, batch_size=64)
253/32: train_generator = gen.flow(x_train, y_train, batch_size=64)
253/33:
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(x_train, y_train)
253/34:
x_train = np.asarray(x_train).astype(np.str_)
y_train = np.asarray(y_train).astype(np.str_)
253/35:
from sklearn.ensemble import RandomForestClassifier

# creating model
model = RandomForestClassifier()

# feeding the training data into the model
model.fit(x_train, y_train)

# calculating the accuracy
print("Training accuracy :", model.score(x_train, y_train))
print("Testing accuracy :", model.score(x_cv, y_cv))
253/36:
import keras 
from keras.layers import Dense 
from keras.models import Sequential
from keras import optimizers
253/37:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(400, activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(300, activation = 'relu'))

# third hidden layer
model.add(Dense(300, activation = 'relu'))

# fourth hidden layer
model.add(Dense(300, activation = 'relu'))

# fifth hidden layer
model.add(Dense(100, activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
model.add(Dense(10, activation = 'softmax'))
253/38:

model.summary()
253/39:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)

x_train = np.asarray(x_train).astype(np.float32)
y_train = np.asarray(y_train).astype(np.str)
# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 12, verbose = 2, validation_data = (x_cv, y_cv))
254/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
254/2:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)
254/3:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import random                        # for generating random numbers
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten
from tensorflow.keras.layers import BatchNormalization
from keras.models import Sequential  # Model type to be used
from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')
254/4:
X_train = (train.iloc[100000:,1].values).astype('float32') # all pixel values
Y_train = train.iloc[100000:,1].values.astype('int32') # only labels i.e targets digits
X_test = test.values.astype('float32')
254/5:
X_train = (train.iloc[100000:,1].values).astype('float32') # all pixel values
Y_train = train.iloc[100000:,1].values.astype('int32') # only labels i.e targets digits
X_test = test.values.astype('object')
254/6:
X_train = X_train.reshape(X_train.shape) #add an additional dimension to represent the single-channel
X_test = X_test.reshape(X_test.shape)

X_train /= 255                              # normalize each value for each pixel for the entire vector for each input
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)
254/7:
X_train = X_train.reshape(X_train.shape) #add an additional dimension to represent the single-channel
X_test = X_test.reshape(X_test.shape(50000,1))

X_train /= 255                              # normalize each value for each pixel for the entire vector for each input
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)
254/8:
X_train = X_train.reshape(X_train.shape) #add an additional dimension to represent the single-channel
X_test = X_test.reshape(X_test.shape[50000,])

X_train /= 255                              # normalize each value for each pixel for the entire vector for each input
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)
254/9:
X_train = X_train.reshape(X_train.shape) #add an additional dimension to represent the single-channel
X_test = X_test.reshape(X_test.shape)

X_train /= 255                              # normalize each value for each pixel for the entire vector for each input
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)
254/10: X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=42)
254/11:
model = Sequential()                                 # Linear stacking of layers

# Convolution Layer 1
model.add(Conv2D(32, (3, 3), input_shape=(28,28,1))) # 32 different 3x3 kernels -- so 32 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer01 = Activation('relu')                     # activation
model.add(convLayer01)

# Convolution Layer 2
model.add(Conv2D(32, (3, 3)))                        # 32 different 3x3 kernels -- so 32 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model.add(Activation('relu'))                        # activation
convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model.add(convLayer02)

# Convolution Layer 3
model.add(Conv2D(64,(3, 3)))                         # 64 different 3x3 kernels -- so 64 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer03 = Activation('relu')                     # activation
model.add(convLayer03)

# Convolution Layer 4
model.add(Conv2D(64, (3, 3)))                        # 64 different 3x3 kernels -- so 64 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model.add(Activation('relu'))                        # activation
convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model.add(convLayer04)
model.add(Flatten())                                 # Flatten final 4x4x64 output matrix into a 1024-length vector

# Fully Connected Layer 5
model.add(Dense(512))                                # 512 FCN nodes
model.add(BatchNormalization())                      # normalization
model.add(Activation('relu'))                        # activation

# Fully Connected Layer 6                       
model.add(Dropout(0.2))                              # 20% dropout of randomly selected nodes
model.add(Dense(10))                                 # final 10 FCN nodes
model.add(Activation('softmax'))                     # softmax activation
254/12: model.summary()
254/13: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
254/14:
#CNN Architecture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> 
                           #Flatten -> Dense -> Dropout -> Out
model = tf.keras.Sequential()

model.add(layers.Conv2D(filters=32, kernel_size=(5,5), padding='Same', 
                       activation=tf.nn.relu, input_shape = (28,28,1)))
model.add(layers.Conv2D(filters=32, kernel_size=(5,5), padding='Same', 
                       activation=tf.nn.relu))
model.add(layers.MaxPool2D(pool_size=(2,2)))
model.add(layers.Dropout(0.25))


model.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding='Same', 
                       activation=tf.nn.relu, input_shape = (28,28,1)))
model.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding='Same', 
                       activation=tf.nn.relu))
model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(layers.Dropout(0.25))

model.add(layers.Flatten())
model.add(layers.Dense(256,activation=tf.nn.relu))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(10,activation=tf.nn.softmax))
254/15:
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
254/16:
#CNN Architecture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> 
                           #Flatten -> Dense -> Dropout -> Out
model = tf.keras.Sequential()

model.add(layers.Conv2D(filters=32, kernel_size=(5,5), padding='Same', 
                       activation=tf.nn.relu, input_shape = (28,28,1)))
model.add(layers.Conv2D(filters=32, kernel_size=(5,5), padding='Same', 
                       activation=tf.nn.relu))
model.add(layers.MaxPool2D(pool_size=(2,2)))
model.add(layers.Dropout(0.25))


model.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding='Same', 
                       activation=tf.nn.relu, input_shape = (28,28,1)))
model.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding='Same', 
                       activation=tf.nn.relu))
model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(layers.Dropout(0.25))

model.add(layers.Flatten())
model.add(layers.Dense(256,activation=tf.nn.relu))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(10,activation=tf.nn.softmax))
254/17: model.summary()
254/18: optimizer = tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
254/19: optimizer = tf.keras.optimizers.legacy.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
254/20:

model.compile(optimizer = optimizer, loss='categorical_crossentropy', 
             metrics=["accuracy"])
254/21:
learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc',
                                           patience=3,
                                           verbose=1,
                                           factor=0.5,
                                           min_lr=0.00001)
254/22:
epochs=10
batch_size = 112
254/23:
datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.1, # Randomly zoom image 
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False)  # randomly flip images


datagen.fit(X_train)
254/24:
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size
                              , callbacks=[learning_rate_reduction])
254/25: model = models.Sequential()
254/26: model = Sequential()
254/27:
# Block 1
model.add(Conv2D(32,3, padding  ="same",input_shape=(28,28,1)))
model.add(LeakyReLU())
model.add(Conv2D(32,3, padding  ="same"))
model.add(LeakyReLU())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

# Block 2
model.add(Conv2D(64,3, padding  ="same"))
model.add(LeakyReLU())
model.add(Conv2D(64,3, padding  ="same"))
model.add(LeakyReLU())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())

model.add(Dense(256,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation="sigmoid"))
254/28:
import cv2 as cv

from keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D
from keras import models
from keras.optimizers import Adam,RMSprop 
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
254/29:


from keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D
from keras import models
from keras.optimizers import Adam,RMSprop 
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
254/30:
# Block 1
model.add(Conv2D(32,3, padding  ="same",input_shape=(28,28,1)))
model.add(LeakyReLU())
model.add(Conv2D(32,3, padding  ="same"))
model.add(LeakyReLU())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

# Block 2
model.add(Conv2D(64,3, padding  ="same"))
model.add(LeakyReLU())
model.add(Conv2D(64,3, padding  ="same"))
model.add(LeakyReLU())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())

model.add(Dense(256,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation="sigmoid"))
254/31:
initial_lr = 0.001
loss = "sparse_categorical_crossentropy"
model.compile(Adam(lr=initial_lr), loss=loss ,metrics=['accuracy'])
model.summary()
254/32:
epochs = 20
batch_size = 256
history_1 = model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,validation_data=[val_x,val_y])
254/33:
epochs = 20
batch_size = 256
history_1 = model.fit(X_train,Y_train,batch_size=batch_size,epochs=epochs,validation_data=[val_x,val_y])
254/34: X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=42)
254/35:
epochs = 20
batch_size = 256
history_1 = model.fit(X_train,Y_train,batch_size=batch_size,epochs=epochs,validation_data=[X_val,Y_val])
254/36:
X_train = X_train.reshape(-1,1) #add an additional dimension to represent the single-channel
X_test = X_test.reshape(-1,1)

X_train /= 255                              # normalize each value for each pixel for the entire vector for each input
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)
254/37:
epochs = 20
batch_size = 256
history_1 = model.fit(X_train,Y_train,batch_size=batch_size,epochs=epochs,validation_data=[X_val,Y_val])
254/38: X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=42)
254/39: model = Sequential()
254/40:


from keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D
from keras import models
from keras.optimizers import Adam,RMSprop 
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
254/41:
# Block 1
model.add(Conv2D(32,3, padding  ="same",input_shape=(28,28,1)))
model.add(LeakyReLU())
model.add(Conv2D(32,3, padding  ="same"))
model.add(LeakyReLU())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

# Block 2
model.add(Conv2D(64,3, padding  ="same"))
model.add(LeakyReLU())
model.add(Conv2D(64,3, padding  ="same"))
model.add(LeakyReLU())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())

model.add(Dense(256,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation="sigmoid"))
254/42:
epochs = 20
batch_size = 256
history_1 = model.fit(X_train,Y_train,batch_size=batch_size,epochs=epochs,validation_data=[X_val,Y_val])
254/43:
initial_lr = 0.001
loss = "sparse_categorical_crossentropy"
model.compile(Adam(lr=initial_lr), loss=loss ,metrics=['accuracy'])
model.summary()
254/44:
epochs = 20
batch_size = 256
history_1 = model.fit(X_train,Y_train,batch_size=batch_size,epochs=epochs,validation_data=[X_val,Y_val])
255/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt 
import cv2 as cv

from keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D
from keras import models
from keras.optimizers import Adam,RMSprop 
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

import pickle

%matplotlib inline
255/2:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt 
import cv2 as cv

from keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D
from keras import models
from keras.optimizers import Adam,RMSprop 
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

import pickle

%matplotlib inline
255/3: np.random.seed(1)
255/4:
df_train = pd.read_pickle('train100c5k_v2.pkl')
df_test = pd.read_pickle('test100c5k_nolabel.pkl')
255/5: df_train.head(5)
255/6: df_train.shape
255/7:
sample_size = df_train.shape[0] # Training set size
validation_size = int(df_train.shape[0]*0.1) # Validation set size 

# train_x and train_y
train_x = np.asarray(df_train.iloc[:sample_size-validation_size,1:]).reshape([sample_size-validation_size,28,28,1]) # taking all columns expect column 0
train_y = np.asarray(df_train.iloc[:sample_size-validation_size,0]).reshape([sample_size-validation_size,1]) # taking column 0

# val_x and val_y
val_x = np.asarray(df_train.iloc[sample_size-validation_size:,1:]).reshape([validation_size,28,28,1])
val_y = np.asarray(df_train.iloc[sample_size-validation_size:,0]).reshape([validation_size,1])
255/8:
train_x = train.iloc[100000:,1]
train_y = train.iloc[100000:,1]

print(train_x.shape)
print(train_y.shape)


test_x = test.iloc[50000:,0]
test_y = test.iloc[50000:,0]

print(test_x.shape)
print(test_y.shape)
255/9:
train_x = df_train.iloc[100000:,1]
train_y = df_train.iloc[100000:,1]

print(train_x.shape)
print(train_y.shape)


test_x = df_test.iloc[50000:,0]
test_y = df_test.iloc[50000:,0]

print(test_x.shape)
print(test_y.shape)
255/10: train_x.shape,train_y.shape
255/11:
train_x = train_x/255
val_x = val_x/255
test_x = test_x/255
255/12:
x_train = np.asmatrix(x_train).reshape(300000, 1)
x_cv = np.asmatrix(x_cv).reshape(100000, 1)
255/13:
train_x = train_x/255

test_x = test_x/255
255/14: model = models.Sequential()
255/15:
# Block 1
model.add(Conv2D(32,3, padding  ="same",input_shape=(28,28,1)))
model.add(LeakyReLU())
model.add(Conv2D(32,3, padding  ="same"))
model.add(LeakyReLU())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

# Block 2
model.add(Conv2D(64,3, padding  ="same"))
model.add(LeakyReLU())
model.add(Conv2D(64,3, padding  ="same"))
model.add(LeakyReLU())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())

model.add(Dense(256,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation="sigmoid"))
Compiling Model
255/16:
# Block 1
model.add(Conv2D(32,3, padding  ="same",input_shape=(28,28,1)))
model.add(LeakyReLU())
model.add(Conv2D(32,3, padding  ="same"))
model.add(LeakyReLU())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

# Block 2
model.add(Conv2D(64,3, padding  ="same"))
model.add(LeakyReLU())
model.add(Conv2D(64,3, padding  ="same"))
model.add(LeakyReLU())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())

model.add(Dense(256,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation="sigmoid"))
#Compiling Model
255/17:
initial_lr = 0.001
loss = "sparse_categorical_crossentropy"
model.compile(Adam(lr=initial_lr), loss=loss ,metrics=['accuracy'])
model.summary()
255/18:
epochs = 20
batch_size = 256
history_1 = model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,validation_data=[val_x,val_y])
255/19:
epochs = 20
batch_size = 256
history_1 = model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,validation_data=[train_x,train_y)
255/20:
epochs = 20
batch_size = 256
history_1 = model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,validation_data=[train_x,train_y]
255/21:
epochs = 20
batch_size = 256
history_1 = model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,validation_data=[train_x,train_y])
255/22:
model = Sequential()

model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=INPUT_SHAPE))
model.add(MaxPool2D((2,2)))

model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
model.add(MaxPool2D((2,2)))

model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(10, activation='softmax'))
255/23:


model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=INPUT_SHAPE))
model.add(MaxPool2D((2,2)))

model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
model.add(MaxPool2D((2,2)))

model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(10, activation='softmax'))
255/24:
INPUT_SHAPE = (400000,)
OUTPUT_SHAPE = 10
BATCH_SIZE = 128
EPOCHS = 10
VERBOSE = 2
255/25:


model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=INPUT_SHAPE))
model.add(MaxPool2D((2,2)))

model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
model.add(MaxPool2D((2,2)))

model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(10, activation='softmax'))
253/40:
cnn=models.Sequential([
    layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(28,28,1)),
    layers.MaxPooling2D((2,2)),
    
    layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10,activation='softmax')
])
cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
253/41:
cnn=Sequential([
    layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(28,28,1)),
    layers.MaxPooling2D((2,2)),
    
    layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10,activation='softmax')
])
cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
253/42:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers,models
253/43:
cnn=Sequential([
    layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(28,28,1)),
    layers.MaxPooling2D((2,2)),
    
    layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10,activation='softmax')
])
cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
253/44: cnn.fit(x_train,ytrain,epochs=10)
253/45: cnn.fit(x_train,y_train,epochs=10)
253/46:
cnn=Sequential([
    layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(28,28,)),
    layers.MaxPooling2D((2,2)),
    
    layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10,activation='softmax')
])
cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
253/47:
cnn=Sequential([
    layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(400000,1)),
    layers.MaxPooling2D((2,2)),
    
    layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10,activation='softmax')
])
cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
253/48:
cnn=Sequential([
    layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(1,1,1)),
    layers.MaxPooling2D((2,2)),
    
    layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10,activation='softmax')
])
cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
253/49:
cnn=Sequential([
    layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(400000,1,1)),
    layers.MaxPooling2D((2,2)),
    
    layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10,activation='softmax')
])
cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
253/50:
cnn=Sequential([
    layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(400000,28,1)),
    layers.MaxPooling2D((2,2)),
    
    layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10,activation='softmax')
])
cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
253/51:
import keras 
from keras.layers import Dense 
from keras.models import Sequential
from keras import optimizers
253/52:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(400, activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(300, activation = 'relu'))

# third hidden layer
model.add(Dense(300, activation = 'relu'))

# fourth hidden layer
model.add(Dense(300, activation = 'relu'))

# fifth hidden layer
model.add(Dense(100, activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
model.add(Dense(10, activation = 'softmax'))
253/53:

from sklearn.neural_network import MLPClassifier

# creating the model
model = MLPClassifier()

# feeding the training set to the modeel
model.fit(x_train, y_train)

# predicting the results for test set
y_pred = model.predict(test_x)

# calculating the accuracies
print("Training accuracy :", model.score(x_train, y_train))
print("Testing accuracy :", model.score(x_cv, y_cv))
256/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
256/2:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
256/3:
train_x = train.iloc[100000:,1]
train_y = train.iloc[100000:,1]
256/4:
print(train_x.shape)
print(train_y.shape)
256/5:
test_x = test.iloc[50000:,0]
test_y = test.iloc[50000:,0]
256/6:
print(test_x.shape)
print(test_y.shape)
256/7:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
256/8:
print(x_train.shape)
print(x_cv.shape)
print(y_train.shape)
print(y_cv.shape)
256/9:
x_train = np.asmatrix(x_train).reshape(300000, 1)
x_cv = np.asmatrix(x_cv).reshape(100000, 1)
256/10: test_x = np.asmatrix(test_x).reshape(50000, )
256/11:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
256/12: test_x = test_x.astype('object')
256/13:
x_train = x_train/255
x_cv = x_cv/255
256/14: test_x = test_x/255
256/15:
import keras 
from keras.layers import Dense 
from keras.models import Sequential
from keras import optimizers
256/16:

from sklearn.neural_network import MLPClassifier

# creating the model
model = MLPClassifier()

# feeding the training set to the modeel
model.fit(x_train, y_train)

# predicting the results for test set
y_pred = model.predict(test_x)

# calculating the accuracies
print("Training accuracy :", model.score(x_train, y_train))
print("Testing accuracy :", model.score(x_cv, y_cv))
257/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
257/2:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
257/3:
train_x = train.iloc[100000:,1]
train_y = train.iloc[100000:,1]
257/4:
print(train_x.shape)
print(train_y.shape)
257/5:
test_x = test.iloc[50000:,0]
test_y = test.iloc[50000:,0]
257/6:
print(test_x.shape)
print(test_y.shape)
257/7:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
257/8:
print(x_train.shape)
print(x_cv.shape)
print(y_train.shape)
print(y_cv.shape)
257/9:
x_train = np.asmatrix(x_train).reshape(300000, 1)
x_cv = np.asmatrix(x_cv).reshape(100000, 1)
257/10: test_x = np.asmatrix(test_x).reshape(50000, )
257/11:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
257/12: test_x = test_x.astype('object')
257/13:
x_train = x_train/255
x_cv = x_cv/255
257/14: test_x = test_x/255
257/15:
import keras 
from keras.layers import Dense 
from keras.models import Sequential
from keras import optimizers
257/16:
x_train = np.asarray(x_train).astype(np.str_)
y_train = np.asarray(y_train).astype(np.str_)
257/17:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(output_dim = 400, init = 'uniform', activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# third hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# fourth hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# fifth hidden layer
model.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
257/18:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(output_dim = 400, init = 'uniform', activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# third hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# fourth hidden layer
model.add(Dense(output_dim = 300, init = 'uniform', activation = 'relu'))

# fifth hidden layer
model.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
model.add(Dense(output_dim = 10, init = 'uniform', activation = 'softmax'))
257/19:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(400, activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(300, activation = 'relu'))

# third hidden layer
model.add(Dense(300, activation = 'relu'))

# fourth hidden layer
model.add(Dense(300, activation = 'relu'))

# fifth hidden layer
model.add(Dense(100, activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
257/20:

model.summary()
257/21:
# setting the learning rate
learning_rate = 0.01
adam = optimizers.Adam(lr = learning_rate)


# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 20, verbose = 2, validation_data = (x_cv, y_cv))
257/22:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)


# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 60, verbose = 2, validation_data = (x_cv, y_cv))
257/23:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(400, activation = 'relu', input_dim = 784))

# second hidden layer
model.add(Dense(300, activation = 'relu'))

# third hidden layer
model.add(Dense(300, activation = 'relu'))

# fourth hidden layer
model.add(Dense(300, activation = 'relu'))

# fifth hidden layer
model.add(Dense(100, activation = 'softmax'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
257/24:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)


# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 60, verbose = 2, validation_data = (x_cv, y_cv))
257/25:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(400, activation = 'relu', input_dim = 50000))

# second hidden layer
model.add(Dense(300, activation = 'relu'))

# third hidden layer
model.add(Dense(300, activation = 'relu'))

# fourth hidden layer
model.add(Dense(300, activation = 'relu'))

# fifth hidden layer
model.add(Dense(100, activation = 'relu'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
model.add(Dense(10, activation = 'softmax'))
257/26:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)

x_train = np.asarray(x_train).astype(np.float32)
y_train = np.asarray(y_train).astype(np.str)
# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 12, verbose = 2, validation_data = (x_cv, y_cv))
257/27:
# creating the model
model = Sequential()

# first hidden layer
model.add(Dense(400, activation = 'relu', input_dim = 50000))

# second hidden layer
model.add(Dense(300, activation = 'relu'))

# third hidden layer
model.add(Dense(300, activation = 'relu'))

# fourth hidden layer
model.add(Dense(300, activation = 'relu'))

# fifth hidden layer
model.add(Dense(100, activation = 'softmax'))

# output layer
# output_dim = no. of digits
# softmax activation function is used for multiple outputs
257/28:
# setting the learning rate
learning_rate = 0.01
sgd = optimizers.SGD(lr = learning_rate)


# compiling the model
# using the plain vanilla stochastic gradient descent as our optimizing technique
# using categorical cross entropy for multiple outputs
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# feeding the training data to the model
model.fit(x_train, y_train, batch_size = 100, epochs = 60, verbose = 2, validation_data = (x_cv, y_cv))
257/29:
cnn=Sequential([
    layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_dim=500000),
    layers.MaxPooling2D((2,2)),
    
    layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10,activation='softmax')
])
cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
257/30:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers,models
257/31:
cnn=Sequential([
    layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_dim=500000),
    layers.MaxPooling2D((2,2)),
    
    layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10,activation='softmax')
])
cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
257/32:

from sklearn.neural_network import MLPClassifier

# creating the model
model = MLPClassifier()

# feeding the training set to the modeel
model.fit(x_train, y_train)

# predicting the results for test set
y_pred = model.predict(test_x)

# calculating the accuracies
print("Training accuracy :", model.score(x_train, y_train))
print("Testing accuracy :", model.score(x_cv, y_cv))
258/1:
INPUT_SHAPE = (28,28,1)
OUTPUT_SHAPE = 10
BATCH_SIZE = 128
EPOCHS = 10
VERBOSE = 2
258/2:
model = Sequential()

model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=INPUT_SHAPE))
model.add(MaxPool2D((2,2)))

model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
model.add(MaxPool2D((2,2)))

model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
258/3:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
258/4:
train = pd.read_pickle('train100c5k_v2.pkl')
test = pd.read_pickle('test100c5k_nolabel.pkl')

print(train.shape)
print(test.shape)

train.head()
258/5:
train_x = train.iloc[100000:,1]
train_y = train.iloc[100000:,1]
258/6:
train_x = train.iloc[100000:,1]
train_y = train.iloc[100000:,1]
258/7:
test_x = test.iloc[50000:,0]
test_y = test.iloc[50000:,0]
258/8:
from sklearn.model_selection import train_test_split

x_train, x_cv, y_train, y_cv = train_test_split(train_x, train_y, test_size = 0.25, random_state = 35)
258/9:
print(x_train.shape)
print(x_cv.shape)
print(y_train.shape)
print(y_cv.shape)
258/10:
x_train = np.asmatrix(x_train).reshape(300000, 1)
x_cv = np.asmatrix(x_cv).reshape(100000, 1)
258/11: test_x = np.asmatrix(test_x).reshape(50000, )
258/12:
x_train = x_train.astype('float32')
x_cv = x_cv.astype('float32')
258/13: test_x = test_x.astype('object')
258/14:
x_train = x_train/255
x_cv = x_cv/255
258/15: test_x = test_x/255
258/16:
x_train = np.asarray(x_train).astype(np.str_)
y_train = np.asarray(y_train).astype(np.str_)
258/17:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers,models
258/18:
INPUT_SHAPE = (40000,28,1)
OUTPUT_SHAPE = 10
BATCH_SIZE = 1
EPOCHS = 10
VERBOSE = 2
258/19:
model = Sequential()

model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=INPUT_SHAPE))
model.add(MaxPool2D((2,2)))

model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
model.add(MaxPool2D((2,2)))

model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
258/20:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers,models
258/21:
model = Sequential()

model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=INPUT_SHAPE))
model.add(MaxPool2D((2,2)))

model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
model.add(MaxPool2D((2,2)))

model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
258/22:
# utility functions
from tensorflow.keras.utils import to_categorical
# sequential model
from tensorflow.keras.models import Sequential
# layers
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout
258/23:
model = Sequential()

model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=INPUT_SHAPE))
model.add(MaxPool2D((2,2)))

model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
model.add(MaxPool2D((2,2)))

model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
262/1: model = keras.model.load_model('models/model_yhnikam.h5')
262/2: from tensorflow import keras
263/1: from tensorflow import keras
263/2: model = keras.model.load_model('models/model_yhnikam.h5')
263/3: model = model.save('model_yhnikam.h5')
263/4: model = sequential()
263/5: from keras.models import Sequential
263/6: model = sequential()
263/7: model = sequential
263/8: model = Sequential
263/9: model = model.save('model_yhnikam.h5')
263/10: model = model.save('IU AML Mini Project/model_yhnikam.h5')
263/11: model = model.save('Downloads/model_yhnikam.h5')
269/1:
import time
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as se
269/2: plt.style.use('fivethirtyeight')
269/3: data = pd.read_csv('01.Mental Health.csv')
269/4: data
269/5: data.columns
269/6:
renamed_columns = ['self_empl_flag', 'comp_no_empl', 'tech_comp_flag', 'tech_role_flag', 'mh_coverage_flag',
                  'mh_coverage_awareness_flag', 'mh_employer_discussion', 'mh_resources_provided', 'mh_anonimity_flag',
                  'mh_medical_leave', 'mh_discussion_neg_impact', 'ph_discussion_neg_impact', 'mh_discussion_cowork',
                  'mh_discussion_supervis', 'mh_eq_ph_employer', 'mh_conseq_coworkers', 'mh_coverage_flag2', 'mh_online_res_flag',
                  'mh_diagnosed&reveal_clients_flag', 'mh_diagnosed&reveal_clients_impact', 'mh_diagnosed&reveal_cowork_flag', 'mh_cowork_reveal_neg_impact',
                  'mh_prod_impact', 'mh_prod_impact_perc', 'prev_employers_flag', 'prev_mh_benefits', 'prev_mh_benefits_awareness',
                  'prev_mh_discussion', 'prev_mh_resources', 'prev_mh_anonimity', 'prev_mh_discuss_neg_conseq', 'prev_ph_discuss_neg_conseq',
                  'prev_mh_discussion_cowork', 'prev_mh_discussion_supervisor', 'prev_mh_importance_employer', 'prev_mh_conseq_coworkers',
                  'future_ph_specification', 'why/why_not', 'future_mh_specification', 'why/why_not2', 'mh_hurt_on_career', 'mh_neg_view_cowork',
                  'mh_sharing_friends/fam_flag', 'mh_bad_response_workplace', 'mh_for_others_bad_response_workplace', 'mh_family_hist',
                  'mh_disorder_past', 'mh_disorder_current', 'yes:what_diagnosis?', 'maybe:whats_your_diag', 'mh_diagnos_proffesional',
                  'yes:condition_diagnosed', 'mh_sought_proffes_treatm', 'mh_eff_treat_impact_on_work', 'mh_not_eff_treat_impact_on_work',
                  'age', 'sex', 'country_live', 'live_us_teritory', 'country_work', 'work_us_teritory', 'work_position', 'remote_flag']
269/7: data.columns = renamed_columns
269/8: data.shape
269/9:
plt.figure(figsize = (50,25))
se.heatmap(data.isnull())
269/10:
temp1 = data.isnull().sum()
temp1
269/11:
plt.figure(figsize = (12,8))
plt.plot(range(len(temp1.values)),[292]*len(temp1.values), alpha = 0.5, linewidth = 1.8, color = 'green')
plt.scatter(range(len(temp1.values)),temp1.values,alpha = 0.8,color = 'red')
269/12:
for c in range(len(temp1.values)):
    if temp1.values[c] > 292:
        print(renamed_columns[c])
        data.drop(axis = 1,columns = [renamed_columns[c]],inplace = True)
269/13:
temp1 = data.isnull().sum()
temp1
269/14:
plt.figure(figsize = (12,8))
plt.ylim((-100,1200))
plt.plot(range(len(temp1.values)),[292]*len(temp1.values), alpha = 0.5, linewidth = 1.8, color = 'green')
plt.scatter(range(len(temp1.values)),temp1.values,alpha = 0.8,color = 'red')
269/15:
plt.figure(figsize = (50,25))
se.heatmap(data.isnull())
269/16:
def unique_values_in_columns(df):
    for c in df.columns:
        print(c,':',df[c].unique())
269/17:
def unique_object_values_in_columns(df):
    for c in df.columns:
        if df[c].dtypes == 'object':
            print(c,':',df[c].unique())
269/18: unique_values_in_columns(data)
269/19:
data['comp_no_empl'] = data['comp_no_empl'].replace('1-5', 5)
data['comp_no_empl'] = data['comp_no_empl'].replace('6-25',25)
data['comp_no_empl'] = data['comp_no_empl'].replace('26-100', 100)
data['comp_no_empl'] = data['comp_no_empl'].replace('100-500',500)
data['comp_no_empl'] = data['comp_no_empl'].replace('500-1000',1000)
data['comp_no_empl'] = data['comp_no_empl'].replace('More than 1000',5000)
269/20: se.barplot(x=data['comp_no_empl'].value_counts().index,y = data['comp_no_empl'].value_counts().values,alpha=0.8)
269/21:
data['sex'] = data['sex'].replace([
    'male', 'Male ', 'M', 'm', 'man', 'Cis male',
    'Male.', 'Male (cis)', 'Man', 'Sex is male',
    'cis male', 'Malr', 'Dude', "I'm a man why didn't you make this a drop down question. You should of asked sex? And I would of answered yes please. Seriously how much text can this take? ",
    'mail', 'M|', 'male ', 'Cis Male', 'Male (trans, FtM)',
    'cisdude', 'cis man', 'MALE'], 'Male')

data['sex'] = data['sex'].replace([
    'female', 'I identify as female.', 'female ',
    'Female assigned at birth ', 'F', 'Woman', 'fm', 'f',
    'Cis female', 'Transitioned, M2F', 'Female or Multi-Gender Femme',
    'Female ', 'woman', 'female/woman', 'Cisgender Female', 
    'mtf', 'fem', 'Female (props for making this a freeform field, though)',
    ' Female', 'Cis-woman', 'AFAB', 'Transgender woman',
    'Cis female '], 'Female')

data['sex'] = data['sex'].replace([
    'Bigender', 'non-binary,', 'Genderfluid (born female)',
    'Other/Transfeminine', 'Androgynous', 'male 9:1 female, roughly',
    'nb masculine', 'genderqueer', 'Human', 'Genderfluid',
    'Enby', 'genderqueer woman', 'Queer', 'Agender', 'Fluid',
    'Genderflux demi-girl', 'female-bodied; no feelings about gender',
    'non-binary', 'Male/genderqueer', 'Nonbinary', 'Other', 'none of your business',
    'Unicorn', 'human', 'Genderqueer'], 'Genderqueer/Other')
269/22: data.sex.value_counts().plot.pie()
269/23: data.replace(to_replace = ['No','Very easy','No, none did','N/A (not currently aware)','No, I only became aware later','None did','None of them','No, at none of my previous employers',"No, I don't think it would",'Neutral','Never','Rarely','Female','No, it has not','Not eligible for coverage / N/A','No, they do not',"No, I don't think they would",], value = 0, inplace= True)
269/24: data.replace(to_replace = ['Yes','Somewhat easy','Yes, they all did','I was aware of some','Yes, I was aware of all of them','Yes, always','Yes, all of them','Yes, at all of my previous employers','Yes, I think it would','Yes, it has','Yes, they do','Yes, I think they would','Somewhat open','Yes, I experienced','Often','Male','Always','Yes, I observed'], value = 1, inplace= True)
269/25: data.replace(to_replace = ["I don't know",'I am not sure','Neither easy nor difficult','Maybe','Very open','Maybe/Not sure','Genderqueer/Other'], value = 2, inplace= True)
269/26: data.replace(to_replace = ['Sometimes','Some did','Some of them','Some of my previous employers',], value = 3, inplace= True)
269/27: data.replace(to_replace = ['Not applicable to me (I do not have a mental illness)','Not applicable to me',], value = 4, inplace= True)
269/28: data.replace(to_replace =['Very difficult','Somewhat not open',], value = -1, inplace= True)
269/29: data.replace(to_replace =['Not open at all','Somewhat difficult'], value = -2, inplace= True)
269/30: unique_values_in_columns(data)
269/31: data['age'].value_counts()
269/32:
plt.figure(figsize = (25,12))
se.barplot(data['age'].value_counts().index, data['age'].value_counts().values)
269/33:
data.loc[(data['age'] > 65), 'age'] = 30
data.loc[(data['age'] < 18), 'age'] = 30
269/34:
plt.figure(figsize = (25,12))
se.barplot(data['age'].value_counts().index, data['age'].value_counts().values)
269/35: data['country_live'].fillna('United States of America',inplace = True)
269/36:
plt.figure(figsize = (25,10))
plt.xticks(rotation = 90)
se.barplot(data['country_live'].value_counts().index, data['country_live'].value_counts().values)
269/37:
def apply_country_live(x):
    if x in ['United States of America', 'United Kingdom','Canada', 'Germany', 'Netherlands', 'Australia']:
        return x
    else:
        return 'United States of America'
269/38: data['country_live'] = data.country_live.apply(apply_country_live)
269/39:
plt.figure(figsize = (25,10))
plt.xticks(rotation = 90)
se.barplot(data['country_live'].value_counts().index, data['country_live'].value_counts().values)
269/40: data['country_work'] = data.country_work.apply(apply_country_live)
269/41: data['country_work'].value_counts().plot.pie()
269/42: data.head(5)
269/43: unique_object_values_in_columns(data)
269/44: data.head(5)
269/45:
plt.figure(figsize = (100,10))
plt.xticks(rotation = 90)
se.barplot(data['work_position'].value_counts().index, data['work_position'].value_counts().values)
269/46: data.to_csv("03.Cleaned Mental Health.csv")
271/1:
import time
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from mlxtend.preprocessing import TransactionEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelBinarizer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
271/2:
import time
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import mlxtend
from mlxtend.preprocessing import TransactionEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelBinarizer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
271/3:
import time
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import mlxtend
from mlxtend.preprocessing import TransactionEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelBinarizer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
272/1:
import time
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import mlxtend
from mlxtend.preprocessing import TransactionEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelBinarizer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
272/2: plt.style.use('fivethirtyeight')
272/3:
data = pd.read_csv("03.Cleaned Mental Health.csv").drop(['Unnamed: 0'], axis = 1)
data
272/4:
def unique_object_values_in_columns(df):
    for c in df.columns:
        if df[c].dtypes == 'object':
            print(c,':',df[c].unique())
272/5: data.shape
272/6: data.head(5)
272/7:
plt.figure(figsize = (100,50))
plt.xticks(rotation = 90)
sns.barplot(data['work_position'].value_counts().index, data['work_position'].value_counts().values)
272/8: tech_list = data['work_position'].value_counts().index.to_list()
272/9:
l = []
for e in data['work_position']:
    l1 = []
    for i in e.split('|'):
        for j in i.split('/'):
            l1.append(j)
    l.append(l1)
l
272/10: TE = TransactionEncoder()
272/11: data1 = TE.fit(l).transform(l)
272/12:
data1 = pd.DataFrame(data = data1, columns = TE.columns_).replace([True,False],[1,0])
data1.head(5)
272/13: data.drop(['work_position'], axis = 1, inplace= True)
272/14: data = data.join(data1)
272/15: data.head(5)
272/16: unique_object_values_in_columns(data)
272/17: OHE = OneHotEncoder(sparse = False)
272/18: data_t1 = OHE.fit_transform(data[['country_live']])
272/19:
col1 = []
for e in OHE.categories_[0]:
    col1.append('live_in_' + str(e))
272/20:
data_t1 = pd.DataFrame(data = data_t1, columns = col1)
data_t1.head(5)
272/21: data.drop(['country_live'], axis = 1, inplace= True)
272/22:
data = data.join(data_t1)
data.head(5)
272/23: LB = LabelBinarizer()
272/24: data_t2 = LB.fit_transform(data['country_work'])
272/25:
col2 = []
for e in LB.classes_:
    col2.append('work_at_' + str(e))
272/26:
data_t2 = pd.DataFrame(data = data_t2, columns = col2)
data_t2
272/27: data.drop(['country_work'], axis = 1, inplace= True)
272/28:
data = data.join(data_t2)
data.head(5)
272/29: unique_object_values_in_columns(data)
272/30: data.isnull().sum()
272/31:
plt.figure(figsize = (50,25))
sns.heatmap(data.isnull())
272/32: temp1 = data.isnull().sum()
272/33:
plt.figure(figsize = (12,8))
#plt.plot(range(len(temp1.values)),[169]*len(temp1.values), alpha = 0.5, linewidth = 1.8, color = 'green')
plt.scatter(range(len(temp1.values)),temp1.values,alpha = 0.8,color = 'red')
272/34:
SI = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
data = pd.DataFrame(data = SI.fit_transform(data), columns = data.columns)
272/35:
plt.figure(figsize = (50,25))
sns.heatmap(data.isnull())
272/36:
temp1 = data.isnull().sum()
plt.figure(figsize = (12,8))
plt.ylim(-10,200)
plt.scatter(range(len(temp1.values)),temp1.values,alpha = 0.8,color = 'red')
272/37: data.dtypes
272/38: data.dtypes[data.dtypes != 'float64'].sum()
272/39: SS = StandardScaler()
272/40: SS.fit(data[['comp_no_empl']])
272/41: data['comp_no_empl'] = pd.DataFrame(SS.transform(data[['comp_no_empl']]))
272/42: MMS = MinMaxScaler()
272/43: MMS.fit(data[['age']])
272/44: data['age'] = pd.DataFrame(MMS.transform(MMS.transform(data[['age']])))
272/45: data.to_csv("05.Normalized Mental Health.csv")
274/1:
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from kmodes.kmodes import KModes
from sklearn.metrics import silhouette_score
from sklearn.metrics import silhouette_samples
from sklearn.ensemble import RandomForestClassifier
from mlxtend.feature_selection import SequentialFeatureSelector
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.tree import plot_tree
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
274/2:
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from kmodes.kmodes import KModes
from sklearn.metrics import silhouette_score
from sklearn.metrics import silhouette_samples
from sklearn.ensemble import RandomForestClassifier
from mlxtend.feature_selection import SequentialFeatureSelector
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.tree import plot_tree
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
275/1:
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from kmodes.kmodes import KModes
from sklearn.metrics import silhouette_score
from sklearn.metrics import silhouette_samples
from sklearn.ensemble import RandomForestClassifier
from mlxtend.feature_selection import SequentialFeatureSelector
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.tree import plot_tree
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
275/2:
data = pd.read_csv("05.Normalized Mental Health.csv").drop(['Unnamed: 0'], axis = 1)
data
275/3: data1 = data.copy()
275/4:
clusters = 15
inertias = []
for cluster in range(2, clusters+1):
    km = KMeans(n_clusters = cluster, random_state = 2)
    y = km.fit_predict(data1)
    inertias.append(km.inertia_)
275/5:
clusters = 15
inertias = []
for cluster in range(2, clusters+1):
    km = KMeans(n_clusters = cluster, random_state = 2)
    y = km.fit_predict(data1)
    inertias.append(km.inertia)
   1: %history
   2: %history -g -f 02.Libraries, Preview, Cleaning of Mental health Dataset.ipynb
